{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"tinygrad documentation","text":"<p>Welcome to the docs for tinygrad. This page is for users of the tinygrad library. tinygrad is not 1.0 yet, but it will be soon. The API has been pretty stable for a while.</p> <p>While you can <code>pip install tinygrad</code>, we encourage you to install from source:</p> <pre><code>git clone https://github.com/tinygrad/tinygrad.git\ncd tinygrad\npython3 -m pip install -e .\n</code></pre> <p>After you have installed tinygrad, try the MNIST tutorial.</p> <p>If you are new to tensor libraries, learn how to use them by solving puzzles from tinygrad-tensor-puzzles.</p> <p>We also have developer docs, and Di Zhu has created a bunch of tutorials to help understand how tinygrad works.</p>"},{"location":"#tinygrad-usage","title":"tinygrad Usage","text":"<p>The main class you will interact with is Tensor. It functions very similarly to PyTorch, but has a bit more of a functional style. tinygrad supports many datatypes.  All operations in tinygrad are lazy, meaning they won't do anything until you realize.</p> <ul> <li>tinygrad has a built in neural network library with some classes, optimizers, and load/save state management.</li> <li>tinygrad has a JIT to make things fast. Decorate your pure function with <code>TinyJit</code></li> <li>tinygrad has amazing support for multiple GPUs, allowing you to shard your Tensors with <code>Tensor.shard</code></li> </ul> <p>To understand what training looks like in tinygrad, you should read <code>beautiful_mnist.py</code></p> <p>We have a quickstart guide and a showcase</p>"},{"location":"#tinygrad-stack","title":"tinygrad Stack","text":""},{"location":"#differences-from-pytorch","title":"Differences from PyTorch","text":"<p>If you are migrating from PyTorch, welcome. Most of the API is the same. We hope you will find tinygrad both familiar and somehow more \"correct feeling\"</p>"},{"location":"#tinygrad-doesnt-have-nnmodule","title":"tinygrad doesn't have nn.Module","text":"<p>There's nothing special about a \"Module\" class in tinygrad, it's just a normal class. <code>nn.state.get_parameters</code> can be used to recursively search normal classes for valid tensors. Instead of the <code>forward</code> method in PyTorch, tinygrad just uses <code>__call__</code></p>"},{"location":"#tinygrad-is-functional","title":"tinygrad is functional","text":"<p>In tinygrad, you can do <code>x.conv2d(w, b)</code> or <code>x.sparse_categorical_cross_entropy(y)</code>. We do also have a <code>Conv2D</code> class like PyTorch if you want a place to keep the state, but all stateless operations don't have classes.</p>"},{"location":"#tinygrad-is-lazy","title":"tinygrad is lazy","text":"<p>When you do <code>a+b</code> in tinygrad, nothing happens. It's not until you <code>realize</code> the Tensor that the computation actually runs.</p>"},{"location":"#tinygrad-requires-tinyjit-to-be-fast","title":"tinygrad requires @TinyJit to be fast","text":"<p>PyTorch spends a lot of development effort to make dispatch very fast. tinygrad doesn't. We have a simple decorator that will replay the kernels used in the decorated function.</p>"},{"location":"dtypes/","title":"dtypes","text":""},{"location":"dtypes/#tinygrad.dtype.DType","title":"DType  <code>dataclass</code>","text":"<pre><code>DType(\n    priority: int,\n    itemsize: int,\n    name: str,\n    fmt: Optional[str],\n    count: int,\n    _scalar: Optional[DType],\n)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes","title":"dtypes","text":"<p>Methods:</p> <ul> <li> <code>is_float</code>             \u2013              </li> <li> <code>is_int</code>             \u2013              </li> <li> <code>is_unsigned</code>             \u2013              </li> <li> <code>from_py</code>             \u2013              </li> <li> <code>as_const</code>             \u2013              </li> <li> <code>min</code>             \u2013              </li> <li> <code>max</code>             \u2013              </li> <li> <code>finfo</code>             \u2013              <p>(exponent, mantissa)</p> </li> <li> <code>fields</code>             \u2013              </li> <li> <code>imageh</code>             \u2013              </li> <li> <code>imagef</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>void</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>bool</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int8</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint8</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>bfloat16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>half</code>           \u2013            </li> <li> <code>float</code>           \u2013            </li> <li> <code>double</code>           \u2013            </li> <li> <code>uchar</code>           \u2013            </li> <li> <code>ushort</code>           \u2013            </li> <li> <code>uint</code>           \u2013            </li> <li> <code>ulong</code>           \u2013            </li> <li> <code>char</code>           \u2013            </li> <li> <code>short</code>           \u2013            </li> <li> <code>int</code>           \u2013            </li> <li> <code>long</code>           \u2013            </li> <li> <code>default_float</code>               (<code>DType</code>)           \u2013            </li> <li> <code>default_int</code>               (<code>DType</code>)           \u2013            </li> <li> <code>floats</code>           \u2013            </li> <li> <code>uints</code>           \u2013            </li> <li> <code>sints</code>           \u2013            </li> <li> <code>ints</code>           \u2013            </li> </ul>"},{"location":"dtypes/#tinygrad.dtype.dtypes.void","title":"void","text":"<pre><code>void: Final[DType] = new(-1, 0, 'void', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.bool","title":"bool","text":"<pre><code>bool: Final[DType] = new(0, 1, 'bool', '?')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int8","title":"int8","text":"<pre><code>int8: Final[DType] = new(1, 1, 'char', 'b')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint8","title":"uint8","text":"<pre><code>uint8: Final[DType] = new(2, 1, 'unsigned char', 'B')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int16","title":"int16","text":"<pre><code>int16: Final[DType] = new(3, 2, 'short', 'h')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint16","title":"uint16","text":"<pre><code>uint16: Final[DType] = new(4, 2, 'unsigned short', 'H')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int32","title":"int32","text":"<pre><code>int32: Final[DType] = new(5, 4, 'int', 'i')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint32","title":"uint32","text":"<pre><code>uint32: Final[DType] = new(6, 4, 'unsigned int', 'I')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int64","title":"int64","text":"<pre><code>int64: Final[DType] = new(7, 8, 'long', 'q')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint64","title":"uint64","text":"<pre><code>uint64: Final[DType] = new(8, 8, 'unsigned long', 'Q')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float16","title":"float16","text":"<pre><code>float16: Final[DType] = new(9, 2, 'half', 'e')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.bfloat16","title":"bfloat16","text":"<pre><code>bfloat16: Final[DType] = new(10, 2, '__bf16', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float32","title":"float32","text":"<pre><code>float32: Final[DType] = new(11, 4, 'float', 'f')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float64","title":"float64","text":"<pre><code>float64: Final[DType] = new(12, 8, 'double', 'd')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.half","title":"half","text":"<pre><code>half = float16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float","title":"float","text":"<pre><code>float = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.double","title":"double","text":"<pre><code>double = float64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uchar","title":"uchar","text":"<pre><code>uchar = uint8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ushort","title":"ushort","text":"<pre><code>ushort = uint16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint","title":"uint","text":"<pre><code>uint = uint32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ulong","title":"ulong","text":"<pre><code>ulong = uint64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.char","title":"char","text":"<pre><code>char = int8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.short","title":"short","text":"<pre><code>short = int16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int","title":"int","text":"<pre><code>int = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.long","title":"long","text":"<pre><code>long = int64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.default_float","title":"default_float","text":"<pre><code>default_float: DType = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.default_int","title":"default_int","text":"<pre><code>default_int: DType = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.floats","title":"floats","text":"<pre><code>floats = (float16, bfloat16, float32, float64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uints","title":"uints","text":"<pre><code>uints = (uint8, uint16, uint32, uint64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.sints","title":"sints","text":"<pre><code>sints = (int8, int16, int32, int64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ints","title":"ints","text":"<pre><code>ints = uints + sints\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_float","title":"is_float","text":"<pre><code>is_float(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.lru_cache(None)\ndef is_float(x: DType) -&gt; bool: return x.scalar() in dtypes.floats or isinstance(x, ImageDType)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_int","title":"is_int","text":"<pre><code>is_int(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool\n@functools.lru_cache(None)\ndef is_int(x: DType) -&gt; bool: return x.scalar() in dtypes.ints\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_unsigned","title":"is_unsigned","text":"<pre><code>is_unsigned(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.lru_cache(None)\ndef is_unsigned(x: DType) -&gt; bool: return x.scalar() in dtypes.uints\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.from_py","title":"from_py","text":"<pre><code>from_py(x) -&gt; DType\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef from_py(x) -&gt; DType:\n  if x.__class__ is float: return dtypes.default_float\n  if x.__class__ is int: return dtypes.default_int\n  if x.__class__ is bool: return dtypes.bool\n  # put this in the last is faster because there are more items than lists/tuples to check\n  if x.__class__ is list or x.__class__ is tuple: return max(dtypes.from_py(xi) for xi in x) if x else dtypes.default_float\n  raise RuntimeError(f\"Could not infer dtype of {x} with type {type(x)}\")\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.as_const","title":"as_const","text":"<pre><code>as_const(\n    val: Tuple[ConstType, ...] | ConstType, dtype: DType\n)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef as_const(val: Tuple[ConstType, ...]|ConstType, dtype:DType):\n  if isinstance(val, tuple):\n    assert len(val) == dtype.count, f\"mismatch {val} {dtype}\"\n    return tuple(dtypes.as_const(x, dtype) for x in val)\n  # TODO: should truncate here\n  return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.min","title":"min","text":"<pre><code>min(dtype: DType)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.lru_cache(None)\ndef min(dtype:DType):\n  if dtypes.is_int(dtype): return 0 if dtypes.is_unsigned(dtype) else -2**(dtype.itemsize*8-1)\n  return -float(\"inf\") if dtypes.is_float(dtype) else False\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.max","title":"max","text":"<pre><code>max(dtype: DType)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.lru_cache(None)\ndef max(dtype:DType):\n  if dtypes.is_int(dtype): return (2**(dtype.itemsize*8-(0 if dtypes.is_unsigned(dtype) else 1)))-1\n  return float(\"inf\") if dtypes.is_float(dtype) else True\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.finfo","title":"finfo","text":"<pre><code>finfo(dtype: DType) -&gt; Tuple[int, int]\n</code></pre> <p>(exponent, mantissa)</p> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef finfo(dtype:DType) -&gt; Tuple[int, int]:\n  \"\"\"(exponent, mantissa)\"\"\"\n  if not dtypes.is_float(dtype): raise ValueError(f\"{dtype} is not a floating point type\")\n  return {dtypes.float16: (5, 10), dtypes.bfloat16: (8, 7), dtypes.float32: (8, 23), dtypes.float64: (11, 52)}[dtype]\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.fields","title":"fields","text":"<pre><code>fields() -&gt; Dict[str, DType]\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef fields() -&gt; Dict[str, DType]: return DTYPES_DICT\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.imageh","title":"imageh","text":"<pre><code>imageh(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imageh(shp): return ImageDType(100, 2, \"imageh\", 'e', 1, None, dtypes.float32, False, 1, shp)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.imagef","title":"imagef","text":"<pre><code>imagef(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imagef(shp): return ImageDType(100, 4, \"imagef\", 'f', 1, None, dtypes.float32, False, 1, shp)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.ConstType","title":"ConstType  <code>module-attribute</code>","text":"<pre><code>ConstType = Union[float, int, bool]\n</code></pre>"},{"location":"env_vars/","title":"List of environment variables that control tinygrad behavior.","text":"<p>This is a list of environment variable that control the runtime behavior of tinygrad and its examples. Most of these are self-explanatory, and are usually used to set an option at runtime.</p> <p>Example: <code>GPU=1 DEBUG=4 python3 -m pytest</code></p> <p>However you can also decorate a function to set a value only inside that function.</p> <pre><code># in tensor.py (probably only useful if you are a tinygrad developer)\n@Context(DEBUG=4)\ndef numpy(self) -&gt; ...\n</code></pre> <p>Or use contextmanager to temporarily set a value inside some scope:</p> <pre><code>with Context(DEBUG=0):\n  a = Tensor.ones(10, 10)\n  a *= 2\n</code></pre>"},{"location":"env_vars/#global-variables","title":"Global Variables","text":"<p>The columns of this list are are: Variable, Possible Value(s) and Description.</p> <ul> <li>A <code>#</code> means that the variable can take any integer value.</li> </ul> <p>These control the behavior of core tinygrad even when used as a library.</p> Variable Possible Value(s) Description DEBUG [1-6] enable debugging output, with 4 you get operations, timings, speed, generated code and more GPU [1] enable the GPU backend CUDA [1] enable CUDA backend AMD [1] enable AMD backend NV [1] enable NV backend METAL [1] enable Metal backend (for Mac M1 and after) METAL_XCODE [1] enable Metal using macOS Xcode SDK CLANG [1] enable Clang backend LLVM [1] enable LLVM backend BEAM [#] number of beams in kernel beam search DEFAULT_FLOAT [HALF, ...] specify the default float dtype (FLOAT32, HALF, BFLOAT16, FLOAT64, ...), default to FLOAT32 IMAGE [1-2] enable 2d specific optimizations FLOAT16 [1] use float16 for images instead of float32 PTX [1] enable the specialized PTX assembler for Nvidia GPUs. If not set, defaults to generic CUDA codegen backend. PROFILE [1] enable output of perfetto compatible profile. This feature is supported in NV and AMD backends. VISIBLE_DEVICES [list[int]] restricts the NV/AMD devices that are available. The format is a comma-separated list of identifiers (indexing starts with 0). JIT [0-2] 0=disabled, 1=jit enabled (default), 2=jit enabled, but graphs are disabled VIZ [1] 0=disabled, 1=viz enabled"},{"location":"mnist/","title":"MNIST Tutorial","text":"<p>After you have installed tinygrad, this is a great first tutorial.</p> <p>Start up a notebook locally, or use colab. tinygrad is very lightweight, so it's easy to install anywhere and doesn't need a special colab image, but for speed we recommend a T4 GPU image.</p>"},{"location":"mnist/#one-liner-to-install-tinygrad-in-colab","title":"One-liner to install tinygrad in colab","text":"<pre><code>!pip install git+https://github.com/tinygrad/tinygrad.git\n</code></pre>"},{"location":"mnist/#whats-the-default-device","title":"What's the default device?","text":"<pre><code>from tinygrad import Device\nprint(Device.DEFAULT)\n</code></pre> <p>You will see <code>CUDA</code> here on a GPU instance, or <code>CLANG</code> here on a CPU instance.</p>"},{"location":"mnist/#a-simple-model","title":"A simple model","text":"<p>We'll use the model from the Keras tutorial.</p> <pre><code>from tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n</code></pre> <p>Two key differences from PyTorch:</p> <ul> <li>Only the stateful layers are declared in <code>__init__</code></li> <li>There's no <code>nn.Module</code> class or <code>forward</code> function, just a normal class and <code>__call__</code></li> </ul>"},{"location":"mnist/#getting-the-dataset","title":"Getting the dataset","text":"<pre><code>from tinygrad.nn.datasets import mnist\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n</code></pre> <p>tinygrad includes MNIST, it only adds four lines. Feel free to read the function.</p>"},{"location":"mnist/#using-the-model","title":"Using the model","text":"<p>MNIST is small enough that the <code>mnist()</code> function copies the dataset to the default device.</p> <p>So creating the model and evaluating it is a matter of:</p> <pre><code>model = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\n# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n</code></pre>"},{"location":"mnist/#training-the-model","title":"Training the model","text":"<p>We'll use the Adam optimizer. The <code>nn.state.get_parameters</code> will walk the model class and pull out the parameters for the optimizer. Also, in tinygrad, it's typical to write a function to do the training step so it can be jitted.</p> <pre><code>optim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n  Tensor.training = True  # makes dropout work\n  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n  X, Y = X_train[samples], Y_train[samples]\n  optim.zero_grad()\n  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n  optim.step()\n  return loss\n</code></pre> <p>You can time a step with:</p> <pre><code>import timeit\ntimeit.repeat(step, repeat=5, number=1)\n#[0.08268719699981375,\n# 0.07478952900009972,\n# 0.07714716600003158,\n# 0.07785399599970333,\n# 0.07605237000007037]\n</code></pre> <p>So around 75 ms on T4 colab.</p> <p>If you want to see a breakdown of the time by kernel:</p> <pre><code>from tinygrad import GlobalCounters, Context\nGlobalCounters.reset()\nwith Context(DEBUG=2): step()\n</code></pre>"},{"location":"mnist/#why-so-slow","title":"Why so slow?","text":"<p>Unlike PyTorch, tinygrad isn't designed to be fast like that. While 75 ms for one step is plenty fast for debugging, it's not great for training. Here, we introduce the first quintessentially tinygrad concept, the <code>TinyJit</code>.</p> <pre><code>from tinygrad import TinyJit\njit_step = TinyJit(step)\n</code></pre> <p>Note</p> <p>It can also be used as a decorator <code>@TinyJit</code></p> <p>Now when we time it:</p> <pre><code>import timeit\ntimeit.repeat(jit_step, repeat=5, number=1)\n# [0.2596786549997887,\n#  0.08989566299987928,\n#  0.0012115650001760514,\n#  0.001010227999813651,\n#  0.0012164899999334011]\n</code></pre> <p>1.0 ms is 75x faster! Note that we aren't syncing the GPU, so GPU time may be slower.</p> <p>The slowness the first two times is the JIT capturing the kernels. And this JIT will not run any Python in the function, it will just replay the tinygrad kernels that were run, so be aware that non tinygrad Python operations won't work. Randomness functions work as expected.</p> <p>Unlike other JITs, we JIT everything, including the optimizer. Think of it as a dumb replay on different data.</p>"},{"location":"mnist/#putting-it-together","title":"Putting it together","text":"<p>Since we are just randomly sampling from the dataset, there's no real concept of an epoch. We have a batch size of 128, so the Keras example is taking about 7000 steps.</p> <pre><code>for step in range(7000):\n  loss = jit_step()\n  if step%100 == 0:\n    Tensor.training = False\n    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n    print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n</code></pre> <p>It doesn't take long to reach 98%, and it usually reaches 99%.</p> <pre><code>step    0, loss 4.03, acc 71.43%\nstep  100, loss 0.34, acc 93.86%\nstep  200, loss 0.23, acc 95.97%\nstep  300, loss 0.18, acc 96.32%\nstep  400, loss 0.18, acc 96.76%\nstep  500, loss 0.13, acc 97.46%\nstep  600, loss 0.14, acc 97.45%\nstep  700, loss 0.10, acc 97.27%\nstep  800, loss 0.23, acc 97.49%\nstep  900, loss 0.13, acc 97.51%\nstep 1000, loss 0.13, acc 97.88%\nstep 1100, loss 0.11, acc 97.72%\nstep 1200, loss 0.14, acc 97.65%\nstep 1300, loss 0.12, acc 98.04%\nstep 1400, loss 0.25, acc 98.17%\nstep 1500, loss 0.11, acc 97.86%\nstep 1600, loss 0.21, acc 98.21%\nstep 1700, loss 0.14, acc 98.34%\n...\n</code></pre>"},{"location":"mnist/#from-here","title":"From here?","text":"<p>tinygrad is yours to play with now. It's pure Python and short, so unlike PyTorch, fixing library bugs is well within your abilities.</p> <ul> <li>It's two lines to add multiGPU support to this example (can you find them?). You have to <code>.shard</code> the model to all GPUs, and <code>.shard</code> the dataset by batch.</li> <li><code>with Context(DEBUG=2)</code> shows the running kernels, <code>DEBUG=4</code> shows the code. All <code>Context</code> variables can also be environment variables.</li> <li><code>with Context(BEAM=2)</code> will do a BEAM search on the kernels, searching many possible implementations for what runs the fastest on your hardware. After this search, tinygrad is usually speed competitive with PyTorch, and the results are cached so you won't have to search next time.</li> </ul> <p>Join our Discord for help, and if you want to be a tinygrad developer. Please read the Discord rules when you get there.</p> <p>Follow us on Twitter to keep up with the project.</p>"},{"location":"nn/","title":"nn (Neural Networks)","text":""},{"location":"nn/#neural-network-classes","title":"Neural Network classes","text":""},{"location":"nn/#tinygrad.nn.BatchNorm","title":"BatchNorm","text":"<pre><code>BatchNorm(\n    sz: int,\n    eps=1e-05,\n    affine=True,\n    track_running_stats=True,\n    momentum=0.1,\n)\n</code></pre> <p>Applies Batch Normalization over a 2D or 3D input.</p> <ul> <li>Described: https://paperswithcode.com/method/batch-normalization</li> <li>Paper: https://arxiv.org/abs/1502.03167v3</li> </ul> <p>See: <code>Tensor.batchnorm</code></p> <p></p> <p><pre><code>norm = nn.BatchNorm(3)\nt = Tensor.rand(2, 3, 4, 4)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>0.5152959823608398 0.2827147841453552\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>0.5152933597564697 0.2827133536338806\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, sz:int, eps=1e-5, affine=True, track_running_stats=True, momentum=0.1):\n  self.eps, self.track_running_stats, self.momentum = eps, track_running_stats, momentum\n\n  self.weight: Optional[Tensor] = Tensor.ones(sz) if affine else None\n  self.bias: Optional[Tensor] = Tensor.zeros(sz) if affine else None\n\n  self.num_batches_tracked = Tensor.zeros(1, dtype='long' if is_dtype_supported(dtypes.long) else 'int', requires_grad=False)\n  if track_running_stats: self.running_mean, self.running_var = Tensor.zeros(sz, requires_grad=False), Tensor.ones(sz, requires_grad=False)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv1d","title":"Conv1d","text":"<pre><code>Conv1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride=1,\n    padding: Union[int, str] = 0,\n    dilation=1,\n    groups=1,\n    bias=True,\n) -&gt; Conv2d\n</code></pre> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d</p> <p><pre><code>conv = nn.Conv1d(1, 1, 3)\nt = Tensor.rand(1, 1, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[0.8461 0.8374 0.7859 0.1354]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[-0.8594 -0.6183]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def Conv1d(in_channels:int, out_channels:int, kernel_size:int, stride=1, padding:Union[int, str]=0, dilation=1, groups=1, bias=True) -&gt; Conv2d:\n  \"\"\"\n  Applies a 1D convolution over an input signal composed of several input planes.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  conv = nn.Conv1d(1, 1, 3)\n  t = Tensor.rand(1, 1, 4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = conv(t)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Conv2d(in_channels, out_channels, (kernel_size,), stride, padding, dilation, groups, bias)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv2d","title":"Conv2d","text":"<pre><code>Conv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: Union[int, Tuple[int, ...]],\n    stride=1,\n    padding: Union[int, Tuple[int, ...], str] = 0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d</p> <p><pre><code>conv = nn.Conv2d(1, 1, 3)\nt = Tensor.rand(1, 1, 4, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0.38   0.7223 0.717  0.0823]\n   [0.9943 0.7944 0.5043 0.0422]\n   [0.5077 0.8806 0.0848 0.1367]\n   [0.8226 0.2978 0.7187 0.627 ]]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[ 0.2286 -0.0301]\n   [ 0.0098 -0.4092]]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels:int, out_channels:int, kernel_size:Union[int, Tuple[int, ...]], stride=1, padding:Union[int, Tuple[int, ...], str]=0,\n             dilation=1, groups=1, bias=True):\n  self.kernel_size = make_tuple(kernel_size, 2)\n  if isinstance(padding, str):\n    if padding.lower() != 'same': raise ValueError(f\"Invalid padding string {padding!r}, only 'same' is supported\")\n    if stride != 1: raise ValueError(\"padding='same' is not supported for strided convolutions\")\n    pad = [(d*(k-1)//2, d*(k-1) - d*(k-1)//2) for d,k in zip(make_tuple(dilation, len(self.kernel_size)), self.kernel_size[::-1])]\n    padding = tuple(flatten(pad))\n  self.stride, self.dilation, self.groups, self.padding = stride, dilation, groups, padding\n  scale = 1 / math.sqrt(in_channels * prod(self.kernel_size))\n  self.weight = Tensor.uniform(out_channels, in_channels//groups, *self.kernel_size, low=-scale, high=scale)\n  self.bias: Optional[Tensor] = Tensor.uniform(out_channels, low=-scale, high=scale) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.ConvTranspose1d","title":"ConvTranspose1d","text":"<pre><code>ConvTranspose1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n) -&gt; ConvTranspose2d\n</code></pre> <p>Applies a 1D transposed convolution operator over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d</p> <p><pre><code>conv = nn.ConvTranspose1d(1, 1, 3)\nt = Tensor.rand(1, 1, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[0.9045 0.2462 0.0398 0.1696]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[-0.0689 -0.0574  0.262   0.1497  0.134   0.1851]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def ConvTranspose1d(in_channels:int, out_channels:int, kernel_size:int, stride=1, padding=0, output_padding=0, dilation=1,\n                      groups=1, bias=True) -&gt; ConvTranspose2d:\n  \"\"\"\n  Applies a 1D transposed convolution operator over an input signal composed of several input planes.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  conv = nn.ConvTranspose1d(1, 1, 3)\n  t = Tensor.rand(1, 1, 4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = conv(t)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return ConvTranspose2d(in_channels, out_channels, (kernel_size,), stride, padding, output_padding, dilation, groups, bias)\n</code></pre>"},{"location":"nn/#tinygrad.nn.ConvTranspose2d","title":"ConvTranspose2d","text":"<pre><code>ConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: Union[int, Tuple[int, ...]],\n    stride=1,\n    padding=0,\n    output_padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> <p>               Bases: <code>Conv2d</code></p> <p>Applies a 2D transposed convolution operator over an input image.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d</p> <p><pre><code>conv = nn.ConvTranspose2d(1, 1, 3)\nt = Tensor.rand(1, 1, 4, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0.1008 0.6509 0.7938 0.3   ]\n   [0.4199 0.9327 0.5151 0.5282]\n   [0.2193 0.2067 0.9991 0.8653]\n   [0.5417 0.5656 0.4337 0.9592]]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[ 0.0558 -0.1197 -0.249  -0.0158  0.2007  0.1513]\n   [-0.0095 -0.0725  0.2385  0.3196  0.1316  0.1809]\n   [ 0.0946  0.1598  0.0671  0.0484  0.455   0.2974]\n   [-0.1419 -0.2921  0.3089  0.4537  0.3593  0.3381]\n   [ 0.1504  0.3614  0.0988  0.3248  0.6634  0.1818]\n   [-0.0797 -0.0164  0.1303 -0.0413  0.2921  0.268 ]]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels:int, out_channels:int, kernel_size:Union[int, Tuple[int, ...]], stride=1, padding=0, output_padding=0,\n              dilation=1, groups=1, bias=True):\n  super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n  scale = 1 / math.sqrt(in_channels * prod(self.kernel_size))\n  self.weight = Tensor.uniform(in_channels, out_channels//groups, *self.kernel_size, low=-scale, high=scale)\n  self.output_padding = output_padding\n</code></pre>"},{"location":"nn/#tinygrad.nn.Linear","title":"Linear","text":"<pre><code>Linear(in_features: int, out_features: int, bias=True)\n</code></pre> <p>Applies a linear transformation to the incoming data.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Linear</p> <p><pre><code>lin = nn.Linear(3, 4)\nt = Tensor.rand(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0.016  0.6345 0.0614]\n [0.2393 0.7782 0.805 ]]\n</code></pre> <pre><code>t = lin(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.0119  0.4813  0.3627  0.3142]\n [ 0.0216  0.8669 -0.0788  0.3406]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_features:int, out_features:int, bias=True):\n  bound = 1 / math.sqrt(in_features)\n  self.weight = Tensor.uniform(out_features, in_features, low=-bound, high=bound)\n  self.bias = Tensor.uniform(out_features, low=-bound, high=bound) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.GroupNorm","title":"GroupNorm","text":"<pre><code>GroupNorm(\n    num_groups: int,\n    num_channels: int,\n    eps=1e-05,\n    affine=True,\n)\n</code></pre> <p>Applies Group Normalization over a mini-batch of inputs.</p> <ul> <li>Described: https://paperswithcode.com/method/group-normalization</li> <li>Paper: https://arxiv.org/abs/1803.08494v3</li> </ul> <p><pre><code>norm = nn.GroupNorm(2, 12)\nt = Tensor.rand(2, 12, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.9816570281982422 0.5622589588165283\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>3.553417471380271e-08 1.0012885332107544\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_groups:int, num_channels:int, eps=1e-5, affine=True):\n  self.num_groups, self.num_channels, self.eps = num_groups, num_channels, eps\n  self.weight: Optional[Tensor] = Tensor.ones(num_channels) if affine else None\n  self.bias: Optional[Tensor] = Tensor.zeros(num_channels) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.InstanceNorm","title":"InstanceNorm","text":"<pre><code>InstanceNorm(num_features: int, eps=1e-05, affine=True)\n</code></pre> <p>Applies Instance Normalization over a mini-batch of inputs.</p> <ul> <li>Described: https://paperswithcode.com/method/instance-normalization</li> <li>Paper: https://arxiv.org/abs/1607.08022v3</li> </ul> <p><pre><code>norm = nn.InstanceNorm(3)\nt = Tensor.rand(2, 3, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.926388144493103 0.5727556347846985\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.4178527685260178e-08 1.0052329301834106\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_features:int, eps=1e-5, affine=True):\n  self.num_features, self.eps = num_features, eps\n  self.weight: Optional[Tensor] = Tensor.ones(num_features) if affine else None\n  self.bias: Optional[Tensor] = Tensor.zeros(num_features) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(\n    normalized_shape: Union[int, Tuple[int, ...]],\n    eps=1e-05,\n    elementwise_affine=True,\n)\n</code></pre> <p>Applies Layer Normalization over a mini-batch of inputs.</p> <ul> <li>Described: https://paperswithcode.com/method/layer-normalization</li> <li>Paper: https://arxiv.org/abs/1607.06450v1</li> </ul> <p><pre><code>norm = nn.LayerNorm(3)\nt = Tensor.rand(2, 5, 3) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.9243342876434326 0.530549168586731\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-1.7950378605746664e-07 1.016965627670288\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:Union[int, Tuple[int, ...]], eps=1e-5, elementwise_affine=True):\n  self.normalized_shape: Tuple[int, ...] = (normalized_shape,) if isinstance(normalized_shape, int) else tuple(normalized_shape)\n  self.axis, self.eps, self.elementwise_affine = tuple(-1-i for i in range(len(self.normalized_shape))), eps, elementwise_affine\n  self.weight, self.bias = (Tensor.ones(*self.normalized_shape), Tensor.zeros(*self.normalized_shape)) if elementwise_affine else (None, None)\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm2d","title":"LayerNorm2d","text":"<pre><code>LayerNorm2d(\n    normalized_shape: Union[int, Tuple[int, ...]],\n    eps=1e-05,\n    elementwise_affine=True,\n)\n</code></pre> <p>               Bases: <code>LayerNorm</code></p> <p>Applies Layer Normalization over a mini-batch of 2D inputs.</p> <p>See: <code>LayerNorm</code></p> <p><pre><code>norm = nn.LayerNorm2d(3)\nt = Tensor.rand(2, 3, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>2.0069570541381836 0.5708586573600769\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-2.551869613398594e-07 1.0051771402359009\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:Union[int, Tuple[int, ...]], eps=1e-5, elementwise_affine=True):\n  self.normalized_shape: Tuple[int, ...] = (normalized_shape,) if isinstance(normalized_shape, int) else tuple(normalized_shape)\n  self.axis, self.eps, self.elementwise_affine = tuple(-1-i for i in range(len(self.normalized_shape))), eps, elementwise_affine\n  self.weight, self.bias = (Tensor.ones(*self.normalized_shape), Tensor.zeros(*self.normalized_shape)) if elementwise_affine else (None, None)\n</code></pre>"},{"location":"nn/#tinygrad.nn.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(dim: int, eps=1e-06)\n</code></pre> <p>Applies Root Mean Square Normalization to input.</p> <ul> <li>Described: https://paperswithcode.com/method/rmsnorm</li> <li>Paper: https://arxiv.org/abs/1910.07467</li> </ul> <p><pre><code>norm = nn.RMSNorm(4)\nt = Tensor.arange(12, dtype=dtypes.float).reshape(3, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.  1.  2.  3.]\n [ 4.  5.  6.  7.]\n [ 8.  9. 10. 11.]]\n</code></pre> <pre><code>print(norm(t).numpy())\n</code></pre> <pre><code>[[0.     0.5345 1.069  1.6036]\n [0.7127 0.8909 1.069  1.2472]\n [0.8363 0.9409 1.0454 1.15  ]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, dim:int, eps=1e-6): self.eps, self.weight = eps, Tensor.ones(dim)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Embedding","title":"Embedding","text":"<pre><code>Embedding(vocab_size: int, embed_size: int)\n</code></pre> <p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Embedding</p> <pre><code>emb = nn.Embedding(10, 3)\nprint(emb(Tensor([1, 2, 3, 1])).numpy())\n</code></pre> <pre><code>[[-0.2061  0.0563 -0.3588]\n [-0.0483 -0.344   0.0924]\n [-0.5069 -0.3611 -0.4899]\n [-0.2061  0.0563 -0.3588]]\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, vocab_size:int, embed_size:int):\n  self.vocab_sz, self.embed_sz, self.weight = vocab_size, embed_size, Tensor.glorot_uniform(vocab_size, embed_size)\n</code></pre>"},{"location":"nn/#tinygrad.nn.LSTMCell","title":"LSTMCell","text":"<pre><code>LSTMCell(\n    input_size: int, hidden_size: int, bias: bool = True\n)\n</code></pre> <p>A long short-term memory (LSTM) cell.</p> <p>Parameters:</p> <ul> <li> <code>input_size</code>               (<code>int</code>)           \u2013            <p>The number of expected features in the input <code>x</code></p> </li> <li> <code>hidden_size</code>               (<code>int</code>)           \u2013            <p>The number of features in the hidden state <code>h</code></p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code></p> </li> </ul> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, input_size:int, hidden_size:int, bias:bool=True):\n  stdv = 1.0 / math.sqrt(hidden_size)\n  self.weight_ih = Tensor.uniform(hidden_size*4, input_size, low=-stdv, high=stdv)\n  self.weight_hh = Tensor.uniform(hidden_size*4, hidden_size, low=-stdv, high=stdv)\n  self.bias_ih, self.bias_hh = (Tensor.zeros(hidden_size*4), Tensor.zeros(hidden_size*4)) if bias else (None, None)\n</code></pre>"},{"location":"nn/#optimizers","title":"Optimizers","text":""},{"location":"nn/#tinygrad.nn.optim.SGD","title":"SGD","text":"<pre><code>SGD(\n    params: List[Tensor],\n    lr=0.001,\n    momentum=0.0,\n    weight_decay=0.0,\n    nesterov=False,\n    classic=False,\n)\n</code></pre> <p>Stochastic Gradient Descent (SGD) optimizer with optional momentum and weight decay.</p> <p><code>classic</code> is a boolean flag that determines whether to use the popular momentum update rule or the classic momentum update rule.</p> <ul> <li>Described: https://paperswithcode.com/method/sgd</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def SGD(params: List[Tensor], lr=0.001, momentum=0.0, weight_decay=0.0, nesterov=False, classic=False):\n  \"\"\"\n  Stochastic Gradient Descent (SGD) optimizer with optional momentum and weight decay.\n\n  `classic` is a boolean flag that determines whether to use the popular momentum update rule or the classic momentum update rule.\n\n  - Described: https://paperswithcode.com/method/sgd\n  \"\"\"\n  return LARS(params, lr, momentum, weight_decay, nesterov, classic, tcoef=0.0)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LARS","title":"LARS","text":"<pre><code>LARS(\n    params: List[Tensor],\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=0.0001,\n    nesterov=False,\n    classic=True,\n    tcoef=0.001,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Layer-wise Adaptive Rate Scaling (LARS) optimizer with optional momentum and weight decay.</p> <ul> <li>Described: https://paperswithcode.com/method/lars</li> <li>Paper: https://arxiv.org/abs/1708.03888v3</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params:List[Tensor], lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=False, classic=True, tcoef=0.001):\n  super().__init__(params, lr)\n  self.momentum, self.wd, self.nesterov, self.classic, self.tcoef = momentum, weight_decay, nesterov, classic, tcoef\n  self.b = [Tensor.zeros(*t.shape, dtype=t.dtype, device=t.device, requires_grad=False) for t in self.params] if self.momentum else []\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.AdamW","title":"AdamW","text":"<pre><code>AdamW(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n    weight_decay=0.01,\n)\n</code></pre> <p>AdamW optimizer with optional weight decay.</p> <ul> <li>Described: https://paperswithcode.com/method/adamw</li> <li>Paper: https://arxiv.org/abs/1711.05101v3</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def AdamW(params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8, weight_decay=0.01):\n  \"\"\"\n  AdamW optimizer with optional weight decay.\n\n  - Described: https://paperswithcode.com/method/adamw\n  - Paper: https://arxiv.org/abs/1711.05101v3\n  \"\"\"\n  return LAMB(params, lr, b1, b2, eps, weight_decay, adam=True)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.Adam","title":"Adam","text":"<pre><code>Adam(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n)\n</code></pre> <p>Adam optimizer.</p> <ul> <li>Described: https://paperswithcode.com/method/adam</li> <li>Paper: https://arxiv.org/abs/1412.6980</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def Adam(params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8):\n  \"\"\"\n  Adam optimizer.\n\n  - Described: https://paperswithcode.com/method/adam\n  - Paper: https://arxiv.org/abs/1412.6980\n  \"\"\"\n  return LAMB(params, lr, b1, b2, eps, 0.0, adam=True)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LAMB","title":"LAMB","text":"<pre><code>LAMB(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-06,\n    weight_decay=0.0,\n    adam=False,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>LAMB optimizer with optional weight decay.</p> <ul> <li>Described: https://paperswithcode.com/method/lamb</li> <li>Paper: https://arxiv.org/abs/1904.00962</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-6, weight_decay=0.0, adam=False):\n  super().__init__(params, lr)\n  self.b1, self.b2, self.eps, self.wd, self.adam = b1, b2, eps, weight_decay, adam\n  self.b1_t, self.b2_t = (Tensor.ones((1,), dtype=dtypes.float32, device=self.device, requires_grad=False).contiguous() for _ in [b1, b2])\n  self.m = [Tensor.zeros(*t.shape, dtype=dtypes.float32, device=t.device, requires_grad=False).contiguous() for t in self.params]\n  self.v = [Tensor.zeros(*t.shape, dtype=dtypes.float32, device=t.device, requires_grad=False).contiguous() for t in self.params]\n</code></pre>"},{"location":"nn/#loadsave","title":"Load/Save","text":""},{"location":"nn/#tinygrad.nn.state.safe_load","title":"safe_load","text":"<pre><code>safe_load(fn: Union[Tensor, str]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Loads a .safetensor file from disk, returning the state_dict.</p> <pre><code>state_dict = nn.state.safe_load(\"test.safetensor\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_load(fn:Union[Tensor,str]) -&gt; Dict[str, Tensor]:\n  \"\"\"\n  Loads a .safetensor file from disk, returning the state_dict.\n\n  ```python\n  state_dict = nn.state.safe_load(\"test.safetensor\")\n  ```\n  \"\"\"\n  t, json_len, metadata = safe_load_metadata(fn)\n  ret = {}\n  for k,v in metadata.items():\n    if k == \"__metadata__\": continue\n    dtype = safe_dtypes[v['dtype']]\n    sz = (v['data_offsets'][1]-v['data_offsets'][0])\n    ret[k] = t[8+json_len+v['data_offsets'][0]:8+json_len+v['data_offsets'][0]+sz].bitcast(dtype).reshape(v['shape'])\n  return ret\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.safe_save","title":"safe_save","text":"<pre><code>safe_save(\n    tensors: Dict[str, Tensor],\n    fn: str,\n    metadata: Optional[Dict[str, Any]] = None,\n)\n</code></pre> <p>Saves a state_dict to disk in a .safetensor file with optional metadata.</p> <pre><code>t = Tensor([1, 2, 3])\nnn.state.safe_save({'t':t}, \"test.safetensor\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_save(tensors:Dict[str, Tensor], fn:str, metadata:Optional[Dict[str, Any]]=None):\n  \"\"\"\n  Saves a state_dict to disk in a .safetensor file with optional metadata.\n\n  ```python\n  t = Tensor([1, 2, 3])\n  nn.state.safe_save({'t':t}, \"test.safetensor\")\n  ```\n  \"\"\"\n  headers, offset = {}, 0\n  if metadata: headers['__metadata__'] = metadata\n  for k,v in tensors.items():\n    headers[k] = {'dtype': inverse_safe_dtypes[v.dtype], 'shape': list(v.shape), 'data_offsets':[offset, offset+v.nbytes()]}\n    offset += v.nbytes()\n  j = json.dumps(headers, separators=(',', ':'))\n  j += \"\\x20\"*((8-len(j)%8)%8)\n  pathlib.Path(fn).unlink(missing_ok=True)\n  t = Tensor.empty(8+len(j)+offset, dtype=dtypes.uint8, device=f\"disk:{fn}\")\n  t[0:8].bitcast(dtypes.int64).assign([len(j)])\n  t[8:8+len(j)].assign(list(j.encode('utf-8')))\n  for k,v in safe_load(t).items(): v.assign(tensors[k])\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_state_dict","title":"get_state_dict","text":"<pre><code>get_state_dict(\n    obj, prefix: str = \"\", tensor_type=Tensor\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Returns a state_dict of the object, with optional prefix.</p> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nprint(nn.state.get_state_dict(net).keys())\n</code></pre> <pre><code>dict_keys(['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias'])\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_state_dict(obj, prefix:str='', tensor_type=Tensor) -&gt; Dict[str, Tensor]:\n  \"\"\"\n  Returns a state_dict of the object, with optional prefix.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  print(nn.state.get_state_dict(net).keys())\n  ```\n  \"\"\"\n  if isinstance(obj, tensor_type): return {prefix.strip('.'):obj}\n  if hasattr(obj, '_asdict'): return get_state_dict(obj._asdict(), prefix, tensor_type)  # namedtuple\n  if isinstance(obj, OrderedDict): return get_state_dict(dict(obj), prefix, tensor_type)\n  if hasattr(obj, '__dict__'): return get_state_dict(obj.__dict__, prefix, tensor_type)\n  state_dict = {}\n  if isinstance(obj, (list, tuple)):\n    for i,x in enumerate(obj): state_dict.update(get_state_dict(x, f\"{prefix}{str(i)}.\", tensor_type))\n  elif isinstance(obj, dict):\n    for k,v in obj.items(): state_dict.update(get_state_dict(v, f\"{prefix}{str(k)}.\", tensor_type))\n  return state_dict\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters(obj) -&gt; List[Tensor]\n</code></pre> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nprint(len(nn.state.get_parameters(net)))\n</code></pre> <pre><code>4\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_parameters(obj) -&gt; List[Tensor]:\n  \"\"\"\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  print(len(nn.state.get_parameters(net)))\n  ```\n  \"\"\"\n  return list(get_state_dict(obj).values())\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(\n    model,\n    state_dict: Dict[str, Tensor],\n    strict=True,\n    verbose=True,\n    consume=False,\n) -&gt; None\n</code></pre> <p>Loads a state_dict into a model.</p> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nstate_dict = nn.state.get_state_dict(net)\nnn.state.load_state_dict(net, state_dict)\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def load_state_dict(model, state_dict:Dict[str, Tensor], strict=True, verbose=True, consume=False) -&gt; None:\n  \"\"\"\n  Loads a state_dict into a model.\n\n  ```python\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  state_dict = nn.state.get_state_dict(net)\n  nn.state.load_state_dict(net, state_dict)\n  ```\n  \"\"\"\n  start_mem_used = GlobalCounters.mem_used\n  with Timing(\"loaded weights in \", lambda et_ns: f\", {(GlobalCounters.mem_used-start_mem_used)/1e9:.2f} GB loaded at {(GlobalCounters.mem_used-start_mem_used)/et_ns:.2f} GB/s\"):  # noqa: E501\n    model_state_dict = get_state_dict(model)\n    if DEBUG &gt;= 1 and len(state_dict) &gt; len(model_state_dict):\n      print(\"WARNING: unused weights in state_dict\", sorted(list(state_dict.keys() - model_state_dict.keys())))\n    for k,v in (t := tqdm(model_state_dict.items(), disable=CI or not verbose)):\n      t.desc = f\"ram used: {GlobalCounters.mem_used/1e9:5.2f} GB, {k:50s}: \"\n      if k not in state_dict and not strict:\n        if DEBUG &gt;= 1: print(f\"WARNING: not loading {k}\")\n        continue\n      if isinstance((mlb:=v.lazydata), MultiLazyBuffer):\n        if isinstance(state_dict[k].lazydata, MultiLazyBuffer): v.replace(state_dict[k]).realize()\n        else: v.replace(state_dict[k].shard(mlb.device, mlb.axis)).realize()\n      else: v.replace(state_dict[k].to(v.device)).realize()\n      if consume: del state_dict[k]\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.torch_load","title":"torch_load","text":"<pre><code>torch_load(fn: str) -&gt; Dict[str, Tensor]\n</code></pre> <p>Loads a torch .pth file from disk.</p> <pre><code>state_dict = nn.state.torch_load(\"test.pth\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def torch_load(fn:str) -&gt; Dict[str, Tensor]:\n  \"\"\"\n  Loads a torch .pth file from disk.\n\n  ```python\n  state_dict = nn.state.torch_load(\"test.pth\")\n  ```\n  \"\"\"\n  t = Tensor.empty(os.stat(fn).st_size, dtype=dtypes.uint8, device=f\"disk:{fn}\")\n\n  offsets: Dict[Union[str, int], int] = {}\n  lens: Dict[Union[str, int], int] = {}\n  def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad=None, backward_hooks=None, metadata=None):\n    #print(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\n    lens[storage[2]] = storage[4] * storage[1].itemsize\n    if storage[2] not in offsets: return None\n    byte_offset = offsets[storage[2]]+storage_offset*storage[1].itemsize\n    ret = t[byte_offset:byte_offset+prod(size)*storage[1].itemsize].bitcast(storage[1])\n\n    # 7 lines to deal with permuted tensors. NOTE: this currently requires reading off the disk\n    shape_strides = [(s, st) for s,st in zip(size, stride) if s != 1]\n    permute_indexes = [len(shape_strides)-1-y for y in argsort([x[1] for x in shape_strides])]\n    if tuple(permute_indexes) != tuple(range(len(permute_indexes))):\n      intermediate_shape = tuple([shape_strides[x][0] for x in argsort(permute_indexes)])\n      assert tuple([shape_strides[i][1] for i in argsort(permute_indexes)]) == strides_for_shape(intermediate_shape), \"nonpermutable strides\"\n      if DEBUG &gt;= 3: print(f\"WARNING: this torch load is slow. CLANG to permute {intermediate_shape} with {permute_indexes}\")\n      assert storage[1] != dtypes.bfloat16, \"can't CLANG permute BF16\"\n      # TODO: find a nice way to support all shapetracker on disktensors\n      ret = ret.to(None).reshape(intermediate_shape).permute(permute_indexes)\n\n    return ret.reshape(size)\n\n  class Parameter:\n    def __setstate__(self, state): self.tensor = state[0]\n\n  deserialized_objects: Dict[str, Any] = {}\n  intercept = {\"HalfStorage\": dtypes.float16, \"FloatStorage\": dtypes.float32, \"BFloat16Storage\": dtypes.bfloat16,\n               \"IntStorage\": dtypes.int32, \"BoolStorage\": dtypes.bool,\n               \"LongStorage\": dtypes.int64, \"_rebuild_tensor_v2\": _rebuild_tensor_v2, \"FloatTensor\": None, \"Parameter\": Parameter}\n  whitelist = {\"torch\", \"collections\", \"numpy\", \"_codecs\"}  # NOTE: this is not for security, only speed\n  class Dummy: pass\n  class TorchPickle(pickle.Unpickler):\n    def find_class(self, module, name):\n      module_root = module.split(\".\")[0]\n      if module_root not in whitelist:\n        if DEBUG &gt;= 2: print(f\"WARNING: returning Dummy for {module} {name}\")\n        return Dummy\n      return intercept[name] if module_root == \"torch\" else super().find_class(module, name)\n    def persistent_load(self, pid): return deserialized_objects.get(pid, pid)\n\n  if zipfile.is_zipfile(fn):\n    myzip = zipfile.ZipFile(fn, 'r')\n    base_name = myzip.namelist()[0].split('/', 1)[0]\n    for n in myzip.namelist():\n      if n.startswith(f'{base_name}/data/'):\n        with myzip.open(n) as myfile:\n          offsets[n.split(\"/\")[-1]] = myfile._orig_compress_start # type: ignore\n    with myzip.open(f'{base_name}/data.pkl') as myfile:\n      return TorchPickle(myfile).load()\n  elif tarfile.is_tarfile(fn):\n    with tarfile.open(fn, \"r\") as tar:\n      storages_offset = tar.getmember('storages').offset_data\n      f = unwrap(tar.extractfile('storages'))\n      for i in range(TorchPickle(f).load()):  # num_storages\n        (key, _, storage_type), sz = TorchPickle(f).load(), struct.unpack('&lt;q', f.read(8))[0]\n        offsets[key] = storages_offset + f.tell()\n        f.seek(sz*storage_type.itemsize, 1)\n      f = unwrap(tar.extractfile('tensors'))\n      for _ in range(TorchPickle(f).load()):  # num_tensors\n        (key, storage_id, _), ndim, _ = TorchPickle(f).load(), struct.unpack('&lt;i', f.read(4))[0], f.read(4)\n        size, stride = struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim)), struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim))\n        storage_offset = struct.unpack('&lt;q', f.read(8))[0]\n        deserialized_objects[str(key)] = _rebuild_tensor_v2((None, storage_type, storage_id, None, -1), storage_offset, size, stride)\n      return {k:v.tensor if isinstance(v, Parameter) else v for k,v in TorchPickle(unwrap(tar.extractfile('pickle'))).load().items()}\n  else:\n    with open(fn, \"rb\") as f:\n      pkl = TorchPickle(f)\n      _, _, _, rwd, _, ids, base_offset = pkl.load(), pkl.load(), pkl.load(), f.tell(), pkl.load(), pkl.load(), f.tell()\n      for i in ids:\n        offsets[i] = base_offset + 8\n        base_offset += 8 + lens[i]\n      f.seek(rwd)\n      return TorchPickle(f).load()\n</code></pre>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>This guide assumes no prior knowledge of pytorch or any other deep learning framework, but does assume some basic knowledge of neural networks. It is intended to be a very quick overview of the high level API that tinygrad provides.</p> <p>This guide is also structured as a tutorial which at the end of it you will have a working model that can classify handwritten digits.</p> <p>We need some imports to get started:</p> <pre><code>import numpy as np\nfrom tinygrad.helpers import Timing\n</code></pre>"},{"location":"quickstart/#tensors","title":"Tensors","text":"<p>Tensors are the base data structure in tinygrad. They can be thought of as a multidimensional array of a specific data type. All high level operations in tinygrad operate on these tensors.</p> <p>The tensor class can be imported like so:</p> <pre><code>from tinygrad import Tensor\n</code></pre> <p>Tensors can be created from an existing data structure like a python list or numpy ndarray:</p> <pre><code>t1 = Tensor([1, 2, 3, 4, 5])\nna = np.array([1, 2, 3, 4, 5])\nt2 = Tensor(na)\n</code></pre> <p>Tensors can also be created using one of the many factory methods:</p> <pre><code>full = Tensor.full(shape=(2, 3), fill_value=5) # create a tensor of shape (2, 3) filled with 5\nzeros = Tensor.zeros(2, 3) # create a tensor of shape (2, 3) filled with 0\nones = Tensor.ones(2, 3) # create a tensor of shape (2, 3) filled with 1\n\nfull_like = Tensor.full_like(full, fill_value=2) # create a tensor of the same shape as `full` filled with 2\nzeros_like = Tensor.zeros_like(full) # create a tensor of the same shape as `full` filled with 0\nones_like = Tensor.ones_like(full) # create a tensor of the same shape as `full` filled with 1\n\neye = Tensor.eye(3) # create a 3x3 identity matrix\narange = Tensor.arange(start=0, stop=10, step=1) # create a tensor of shape (10,) filled with values from 0 to 9\n\nrand = Tensor.rand(2, 3) # create a tensor of shape (2, 3) filled with random values from a uniform distribution\nrandn = Tensor.randn(2, 3) # create a tensor of shape (2, 3) filled with random values from a standard normal distribution\nuniform = Tensor.uniform(2, 3, low=0, high=10) # create a tensor of shape (2, 3) filled with random values from a uniform distribution between 0 and 10\n</code></pre> <p>There are even more of these factory methods, you can find them in the Tensor Creation file.</p> <p>All the tensors creation methods can take a <code>dtype</code> argument to specify the data type of the tensor, find the supported <code>dtype</code> in dtypes.</p> <pre><code>from tinygrad import dtypes\n\nt3 = Tensor([1, 2, 3, 4, 5], dtype=dtypes.int32)\n</code></pre> <p>Tensors allow you to perform operations on them like so:</p> <pre><code>t4 = Tensor([1, 2, 3, 4, 5])\nt5 = (t4 + 1) * 2\nt6 = (t5 * t4).relu().log_softmax()\n</code></pre> <p>All of these operations are lazy and are only executed when you realize the tensor using <code>.realize()</code> or <code>.numpy()</code>.</p> <pre><code>print(t6.numpy())\n# [-56. -48. -36. -20.   0.]\n</code></pre> <p>There are a lot more operations that can be performed on tensors, you can find them in the Tensor Ops file. Additionally reading through abstractions2.py will help you understand how operations on these tensors make their way down to your hardware.</p>"},{"location":"quickstart/#models","title":"Models","text":"<p>Neural networks in tinygrad are really just represented by the operations performed on tensors. These operations are commonly grouped into the <code>__call__</code> method of a class which allows modularization and reuse of these groups of operations. These classes do not need to inherit from any base class, in fact if they don't need any trainable parameters they don't even need to be a class!</p> <p>An example of this would be the <code>nn.Linear</code> class which represents a linear layer in a neural network.</p> <pre><code>class Linear:\n  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n    self.bias = Tensor.zeros(out_features) if bias else None\n\n  def __call__(self, x):\n    return x.linear(self.weight.transpose(), self.bias)\n</code></pre> <p>There are more neural network modules already implemented in nn, and you can also implement your own.</p> <p>We will be implementing a simple neural network that can classify handwritten digits from the MNIST dataset. Our classifier will be a simple 2 layer neural network with a Leaky ReLU activation function. It will use a hidden layer size of 128 and an output layer size of 10 (one for each digit) with no bias on either Linear layer.</p> <pre><code>class TinyNet:\n  def __init__(self):\n    self.l1 = Linear(784, 128, bias=False)\n    self.l2 = Linear(128, 10, bias=False)\n\n  def __call__(self, x):\n    x = self.l1(x)\n    x = x.leakyrelu()\n    x = self.l2(x)\n    return x\n\nnet = TinyNet()\n</code></pre> <p>We can see that the forward pass of our neural network is just the sequence of operations performed on the input tensor <code>x</code>. We can also see that functional operations like <code>leakyrelu</code> are not defined as classes and instead are just methods we can just call. Finally, we just initialize an instance of our neural network, and we are ready to start training it.</p>"},{"location":"quickstart/#training","title":"Training","text":"<p>Now that we have our neural network defined we can start training it. Training neural networks in tinygrad is super simple. All we need to do is define our neural network, define our loss function, and then call <code>.backward()</code> on the loss function to compute the gradients. They can then be used to update the parameters of our neural network using one of the many Optimizers.</p> <p>For our loss function we will be using sparse categorical cross entropy loss. The implementation below is taken from tensor.py, it's copied below to highlight an important detail of tinygrad.</p> <pre><code>def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -&gt; Tensor:\n    loss_mask = Y != ignore_index\n    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n</code></pre> <p>As we can see in this implementation of cross entropy loss, there are certain operations that tinygrad does not support natively. Load/store ops are not supported in tinygrad natively because they add complexity when trying to port to different backends, 90% of the models out there don't use/need them, and they can be implemented like it's done above with an <code>arange</code> mask.</p> <p>For our optimizer we will be using the traditional stochastic gradient descent optimizer with a learning rate of 3e-4.</p> <pre><code>from tinygrad.nn.optim import SGD\n\nopt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)\n</code></pre> <p>We can see that we are passing in the parameters of our neural network to the optimizer. This is due to the fact that the optimizer needs to know which parameters to update. There is a simpler way to do this just by using <code>get_parameters(net)</code> from <code>tinygrad.nn.state</code> which will return a list of all the parameters in the neural network. The parameters are just listed out explicitly here for clarity.</p> <p>Now that we have our network, loss function, and optimizer defined all we are missing is the data to train on! There are a couple of dataset loaders in tinygrad located in /extra/datasets. We will be using the MNIST dataset loader.</p> <pre><code>from extra.datasets import fetch_mnist\n</code></pre> <p>Now we have everything we need to start training our neural network. We will be training for 1000 steps with a batch size of 64.</p> <p>We use <code>with Tensor.train()</code> to set the internal flag <code>Tensor.training</code> to <code>True</code> during training. Upon exit, the flag is restored to its previous value by the context manager.</p> <pre><code>X_train, Y_train, X_test, Y_test = fetch_mnist()\n\nwith Tensor.train():\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_train.shape[0], size=(64))\n    batch = Tensor(X_train[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Tensor(Y_train[samp])\n\n    # forward pass\n    out = net(batch)\n\n    # compute loss\n    loss = sparse_categorical_crossentropy(out, labels)\n\n    # zero gradients\n    opt.zero_grad()\n\n    # backward pass\n    loss.backward()\n\n    # update parameters\n    opt.step()\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1)\n    acc = (pred == labels).mean()\n\n    if step % 100 == 0:\n      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")\n</code></pre>"},{"location":"quickstart/#evaluation","title":"Evaluation","text":"<p>Now that we have trained our neural network we can evaluate it on the test set. We will be using the same batch size of 64 and will be evaluating for 1000 of those batches.</p> <pre><code>with Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass\n    out = net(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre>"},{"location":"quickstart/#and-thats-it","title":"And that's it","text":"<p>Highly recommend you check out the examples/ folder for more examples of using tinygrad. Reading the source code of tinygrad is also a great way to learn how it works. Specifically the tests in test/ are a great place to see how to use and the semantics of the different operations. There are also a bunch of models implemented in models/ that you can use as a reference.</p> <p>Additionally, feel free to ask questions in the <code>#learn-tinygrad</code> channel on the discord. Don't ask to ask, just ask!</p>"},{"location":"quickstart/#extras","title":"Extras","text":""},{"location":"quickstart/#jit","title":"JIT","text":"<p>Additionally, it is possible to speed up the computation of certain neural networks by using the JIT. Currently, this does not support models with varying input sizes and non tinygrad operations.</p> <p>To use the JIT we just need to add a function decorator to the forward pass of our neural network and ensure that the input and output are realized tensors. Or in this case we will create a wrapper function and decorate the wrapper function to speed up the evaluation of our neural network.</p> <pre><code>from tinygrad import TinyJit\n\n@TinyJit\ndef jit(x):\n  return net(x).realize()\n\nwith Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass with jit\n    out = jit(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre> <p>You will find that the evaluation time is much faster than before and that your accelerator utilization is much higher.</p>"},{"location":"quickstart/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>The standard weight format for tinygrad is safetensors. This means that you can load the weights of any model also using safetensors into tinygrad. There are functions in state.py to save and load models to and from this format.</p> <pre><code>from tinygrad.nn.state import safe_save, safe_load, get_state_dict, load_state_dict\n\n# first we need the state dict of our model\nstate_dict = get_state_dict(net)\n\n# then we can just save it to a file\nsafe_save(state_dict, \"model.safetensors\")\n\n# and load it back in\nstate_dict = safe_load(\"model.safetensors\")\nload_state_dict(net, state_dict)\n</code></pre> <p>Many of the models in the models/ folder have a <code>load_from_pretrained</code> method that will download and load the weights for you. These usually are pytorch weights meaning that you would need pytorch installed to load them.</p>"},{"location":"quickstart/#environment-variables","title":"Environment Variables","text":"<p>There exist a bunch of environment variables that control the runtime behavior of tinygrad. Some of the commons ones are <code>DEBUG</code> and the different backend enablement variables.</p> <p>You can find a full list and their descriptions in env_vars.md.</p>"},{"location":"quickstart/#visualizing-the-computation-graph","title":"Visualizing the Computation Graph","text":"<p>It is possible to visualize the computation graph of a neural network using VIZ=1.</p>"},{"location":"runtime/","title":"Runtimes","text":"<p>tinygrad supports various runtimes, enabling your code to scale across a wide range of devices. The default runtime can be automatically selected based on the available hardware, or you can force a specific runtime to be default using environment variables (e.g., <code>CLANG=1</code>).</p> Runtime Description Requirements NV Provides acceleration for NVIDIA GPUs Ampere/Ada series GPUs AMD Provides acceleration for AMD GPUs RDNA2/RDNA3 series GPUs QCOM Provides acceleration for QCOM GPUs 6xx series GPUs METAL Utilizes Metal for acceleration on Apple devices M1+ Macs; Metal 3.0+ for <code>bfloat</code> support CUDA Utilizes CUDA for acceleration on NVIDIA GPUs NVIDIA GPU with CUDA support GPU (OpenCL) Accelerates computations using OpenCL on GPUs OpenCL 2.0 compatible device CLANG (C Code) Runs on CPU using the clang compiler <code>clang</code> compiler in system <code>PATH</code> LLVM Runs on CPU using the LLVM compiler infrastructure <code>llvmlite</code> package installed"},{"location":"showcase/","title":"Showcase","text":"<p>Despite being a tiny library, tinygrad is capable of doing a lot of things. From state-of-the-art vision to state-of-the-art language models.</p>"},{"location":"showcase/#vision","title":"Vision","text":""},{"location":"showcase/#efficientnet","title":"EfficientNet","text":"<p>You can either pass in the URL of a picture to discover what it is: <pre><code>python3 examples/efficientnet.py ./test/models/efficientnet/Chicken.jpg\n</code></pre> Or, if you have a camera and OpenCV installed, you can detect what is in front of you: <pre><code>python3 examples/efficientnet.py webcam\n</code></pre></p>"},{"location":"showcase/#yolov8","title":"YOLOv8","text":"<p>Take a look at yolov8.py.</p> <p></p>"},{"location":"showcase/#audio","title":"Audio","text":""},{"location":"showcase/#whisper","title":"Whisper","text":"<p>Take a look at whisper.py. You need pyaudio and torchaudio installed.</p> <pre><code>SMALL=1 python3 examples/whisper.py\n</code></pre>"},{"location":"showcase/#generative","title":"Generative","text":""},{"location":"showcase/#stable-diffusion","title":"Stable Diffusion","text":"<pre><code>python3 examples/stable_diffusion.py\n</code></pre> <p>\"a horse sized cat eating a bagel\"</p>"},{"location":"showcase/#llama","title":"LLaMA","text":"<p>You will need to download and put the weights into the <code>weights/LLaMA</code> directory, which may need to be created.</p> <p>Then you can have a chat with Stacy: <pre><code>python3 examples/llama.py\n</code></pre></p>"},{"location":"showcase/#conversation","title":"Conversation","text":"<p>Make sure you have espeak installed and <code>PHONEMIZER_ESPEAK_LIBRARY</code> set.</p> <p>Then you can talk to Stacy: <pre><code>python3 examples/conversation.py\n</code></pre></p>"},{"location":"tinybox/","title":"tinybox","text":"<p>Although these docs live in tinygrad, they pertain to deep learning hardware sold by the tiny corp. tinyboxes are used heavily in tinygrad's CI, and are the best tested platform to use tinygrad with. They appeared running tinygrad on MLPerf Training 4.0</p> <p>If you don't have a tinybox and you want one, see tinygrad.org. If you don't want one, that's okay too.</p>"},{"location":"tinybox/#welcome","title":"Welcome","text":"<p>Welcome to your tinybox! The tinybox is the universal system purpose-built for all AI infrastructure and workloads, from training to inference. The red box includes six 7900XTX GPUs, and the green box includes six 4090 GPUs. Whether you bought a red one or a green one, we want you to love it.</p> <p>We don't have a stupid cloud service, you don't have to create a tiny account to set it up, and we aren't tracking how you use the box. We're just happy you bought one. This petaflop is your petaflop.</p>"},{"location":"tinybox/#plugging-it-in","title":"Plugging it in","text":"<p>tinybox has two 1600W PSUs, which together exceed the capacity of most 120V household circuits. Fortunately, it comes with two plugs. You'll want to plug each plug into a different circuit. You can verify that they are different circuits by flipping the breaker and seeing what turns off. If you have at least a 120V 30A or 220V 20A circuit, you are welcome to use only that one.</p> <p>You'll also want to connect the Ethernet port without a rubber stopper to your home network.</p> <p>While it's designed primarily for the home or office, the tinybox is 12U rack mountable using these rails.</p>"},{"location":"tinybox/#power-limiting-the-box","title":"Power limiting the box","text":"<p>While a tinybox should ideally be run without power limits, there are cases where you might want to run the box off of a single outlet.</p> <p>In such cases, it is possible to power limit the box using the provided <code>power-limit</code> script, which will power limit all of the GPUs to a specified wattage.</p> <p><code>sudo power-limit 150</code> should be good to run off of a single 120V 15A outlet.</p>"},{"location":"tinybox/#connecting-to-the-box","title":"Connecting to the box","text":"<p>tinybox ships with a relatively basic install of Ubuntu 22.04. To do initial setup, you can either plug in a VGA monitor and keyboard, or you can connect remotely to the machine using the BMC. The BMC IP and password are displayed on the screen.</p> <p><code>ipmitool -H &lt;BMC IP&gt; -U admin -P &lt;BMC PW&gt; -I lanplus sol activate</code></p> <p>The default username is <code>tiny</code> and the default password is <code>tiny</code>. Once you are logged in, you can add an SSH key to authorized keys to connect over SSH (on the normal IP). Exit <code>ipmitool</code> with <code>~.</code> after a newline.</p> <p>The BMC also has a web interface you can use if you find that easier.</p>"},{"location":"tinybox/#changing-the-bmc-password","title":"Changing the BMC password","text":"<p>It is recommended that you change the BMC password after setting up the box, as the password on the screen is only the initial password.</p> <p>If you do decide to change the BMC password and no longer want the initial password to be displayed, remove the <code>/root/.bmc_password</code> file. Reboot after making these changes or restart the <code>displayservice.service</code> service.</p>"},{"location":"tinybox/#what-do-i-use-it-for","title":"What do I use it for?","text":"<p>The default tinybox image ships with tinygrad and PyTorch. While we develop tinygrad, the box is universal hardware. Use whatever framework you desire, run notebooks, download demos, install more things, train, inference, live, laugh, love, you aren't paying per hour for this box so the only limit is your imagination.</p>"},{"location":"tinybox/#tinychat","title":"tinychat","text":"<p>Since LLMs are so popular, we ship with a built in tinygrad based chatbot using a LLaMA-3 finetune. Visit the IP (not the BMC IP) of your tinybox in a web browser on your computer or phone, and you'll find a friendly looking chat interface. This chatbot also provides an OpenAI compatible LLM API on that port, so you can script it.</p> <p>The conversations you have with this chatbot are between you and your tinybox. Also, the history in the web app is saved on the client, not the tinybox.</p>"},{"location":"developer/developer/","title":"Intro","text":"<p>The tinygrad framework has four pieces</p> <ul> <li>a PyTorch like frontend.</li> <li>a scheduler which breaks the compute into kernels.</li> <li>a lowering engine which converts ASTs into code that can run on the accelerator.</li> <li>an execution engine which can run that code.</li> </ul> <p>There is a good bunch of tutorials by Di Zhu that go over tinygrad internals.</p>"},{"location":"developer/developer/#frontend","title":"Frontend","text":"<p>Everything in Tensor is syntactic sugar around function.py, where the forwards and backwards passes are implemented for the different functions. There's about 25 of them, implemented using about 20 basic ops. Those basic ops go on to construct a graph of:</p> <p>The <code>LazyBuffer</code> graph specifies the compute in terms of low level tinygrad ops. Not all LazyBuffers will actually become realized. There's two types of LazyBuffers, base and view. base contains compute into a contiguous buffer, and view is a view (specified by a ShapeTracker). Inputs to a base can be either base or view, inputs to a view can only be a single base.</p>"},{"location":"developer/developer/#tinygrad.engine.lazy.LazyBuffer","title":"LazyBuffer","text":"<pre><code>LazyBuffer(\n    device: str,\n    st: ShapeTracker,\n    dtype: DType,\n    op: Optional[Ops] = None,\n    arg: Any = None,\n    srcs: Tuple[LazyBuffer, ...] = (),\n    base: Optional[LazyBuffer] = None,\n    metadata: Optional[Metadata] = None,\n)\n</code></pre> <p>               Bases: <code>MathTrait</code></p>"},{"location":"developer/developer/#scheduling","title":"Scheduling","text":"<p>The scheduler converts the graph of LazyBuffers into a list of <code>ScheduleItem</code>. One <code>ScheduleItem</code> is one kernel on the GPU, and the scheduler is responsible for breaking the large compute graph into subgraphs that can fit in a kernel. <code>ast</code> specifies what compute to run, and <code>bufs</code> specifies what buffers to run it on.</p>"},{"location":"developer/developer/#tinygrad.engine.schedule.ScheduleItem","title":"ScheduleItem  <code>dataclass</code>","text":"<pre><code>ScheduleItem(\n    ast: UOp,\n    bufs: Tuple[Buffer, ...],\n    metadata: Tuple[Metadata, ...],\n    assign_preloads: Tuple[UOp, ...],\n)\n</code></pre> <p>Attributes:</p> <ul> <li> <code>inputs</code>               (<code>Tuple[Buffer, ...]</code>)           \u2013            <p>Read only buffers in the schedule.</p> </li> <li> <code>outputs</code>               (<code>Tuple[Buffer, ...]</code>)           \u2013            <p>Read/write or write only buffers in the schedule.</p> </li> </ul>"},{"location":"developer/developer/#tinygrad.engine.schedule.ScheduleItem.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: Tuple[Buffer, ...]\n</code></pre> <p>Read only buffers in the schedule.</p>"},{"location":"developer/developer/#tinygrad.engine.schedule.ScheduleItem.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Tuple[Buffer, ...]\n</code></pre> <p>Read/write or write only buffers in the schedule.</p>"},{"location":"developer/developer/#lowering","title":"Lowering","text":"<p>The code in realize lowers <code>ScheduleItem</code> to <code>ExecItem</code> with</p> <p>There's a ton of complexity hidden behind this, see the <code>codegen/</code> directory.</p> <p>First we lower the AST to UOps, which is a linear list of the compute to be run. This is where the BEAM search happens.</p> <p>Then we render the UOps into code with a <code>Renderer</code>, then we compile the code to binary with a <code>Compiler</code>.</p>"},{"location":"developer/developer/#tinygrad.engine.realize.lower_schedule","title":"lower_schedule","text":"<pre><code>lower_schedule(\n    schedule: List[ScheduleItem],\n) -&gt; Generator[ExecItem, None, None]\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def lower_schedule(schedule:List[ScheduleItem]) -&gt; Generator[ExecItem, None, None]:\n  while len(schedule):\n    si = schedule.pop(0)\n    try: yield lower_schedule_item(si)\n    except Exception as e:\n      if DEBUG &gt;= 2:\n        print(f\"error lowering {si.ast.op}\")\n        print(\"tensor operations:\")\n        pprint.pprint(si.metadata, indent=2)\n      raise e\n</code></pre>"},{"location":"developer/developer/#execution","title":"Execution","text":"<p>Creating <code>ExecItem</code>, which has a run method</p> <p>Lists of <code>ExecItem</code> can be condensed into a single ExecItem with the Graph API (rename to Queue?)</p>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem","title":"ExecItem  <code>dataclass</code>","text":"<pre><code>ExecItem(\n    prg: Runner,\n    bufs: List[Optional[Buffer]],\n    metadata: Optional[Tuple[Metadata, ...]] = None,\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>bufs</code>               (<code>List[Optional[Buffer]]</code>)           \u2013            </li> <li> <code>metadata</code>               (<code>Optional[Tuple[Metadata, ...]]</code>)           \u2013            </li> <li> <code>prg</code>               (<code>Runner</code>)           \u2013            </li> </ul>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.bufs","title":"bufs  <code>instance-attribute</code>","text":"<pre><code>bufs: List[Optional[Buffer]]\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[Tuple[Metadata, ...]] = None\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.prg","title":"prg  <code>instance-attribute</code>","text":"<pre><code>prg: Runner\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.run","title":"run","text":"<pre><code>run(\n    _var_vals: Optional[Dict[Variable, int]] = None,\n    wait=False,\n    jit=False,\n    do_update_stats=True,\n) -&gt; Optional[float]\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def run(self, _var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -&gt; Optional[float]:\n  var_vals = {} if _var_vals is None else _var_vals\n  bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]\n  et = self.prg(bufs, var_vals, wait=wait or DEBUG &gt;= 2)\n  if do_update_stats:\n    GlobalCounters.kernel_count += 1\n    GlobalCounters.global_ops += (op_est:=sym_infer(self.prg.op_estimate, var_vals))\n    GlobalCounters.global_mem += (mem_est:=sym_infer(self.prg.mem_estimate, var_vals))\n    if et is not None: GlobalCounters.time_sum_s += et\n    if DEBUG &gt;= 2:\n      lds_est = sym_infer(self.prg.lds_estimate, var_vals)\n      mem_est = min(mem_est, lds_est)   # there can't be more memory accessed than loads/stores. remove this when symbolic is fixed\n      ptm = (colored(f\"{et*1e3:9.2f}ms\", \"yellow\") if et &gt; 0.01 else f\"{et*1e6:9.2f}us\") if et is not None else \"\"\n      print(f\"{colored(f'*** {self.prg.dname[:7]:7s} {GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} {self.prg.display_name+' '*(41-ansilen(self.prg.display_name))} arg {len(bufs):2d} mem {GlobalCounters.mem_used/1e9:5.2f} GB \" +  # noqa: E501\n            (str() if et is None else f\"tm {ptm}/{GlobalCounters.time_sum_s*1e3:9.2f}ms ({op_est/((et or 1e-20)*1e9):9.2f} GFLOPS {mem_est/((et or 1e-20)*1e9):6.1f}|{lds_est/((et or 1e-20)*1e9):&lt;7.1f} GB/s)\" +  # noqa: E501\n             f\" {[repr(m) if TRACEMETA &gt;= 2 else str(m) for m in self.metadata] if self.metadata else ''}\"))\n    self.prg.first_run = False\n  return et\n</code></pre>"},{"location":"developer/developer/#runtime","title":"Runtime","text":"<p>Runtimes are responsible for device-specific interactions. They handle tasks such as initializing devices, allocating memory, loading/launching programs, and more. You can find more information about the runtimes API on the runtime overview page.</p> <p>All runtime implementations can be found in the runtime directory.</p>"},{"location":"developer/developer/#hcq-compatible-runtimes","title":"HCQ Compatible Runtimes","text":"<p>HCQ API is a lower-level API for defining runtimes. Interaction with HCQ-compatible devices occurs at a lower level, with commands issued directly to hardware queues. Some examples of such backends are NV and AMD, which are userspace drivers for NVIDIA and AMD devices respectively. You can find more information about the API on HCQ overview page</p>"},{"location":"developer/function/","title":"Function (autodiff)","text":""},{"location":"developer/function/#tinygrad.function","title":"function","text":"<p>This is where the forwards and backwards passes live.</p> <p>Classes:</p> <ul> <li> <code>Contiguous</code>           \u2013            </li> <li> <code>ContiguousBackward</code>           \u2013            </li> <li> <code>Cast</code>           \u2013            </li> <li> <code>Reciprocal</code>           \u2013            </li> <li> <code>Sin</code>           \u2013            </li> <li> <code>Relu</code>           \u2013            </li> <li> <code>Log</code>           \u2013            </li> <li> <code>Exp</code>           \u2013            </li> <li> <code>Sqrt</code>           \u2013            </li> <li> <code>Sigmoid</code>           \u2013            </li> <li> <code>Sign</code>           \u2013            </li> <li> <code>Less</code>           \u2013            </li> <li> <code>Xor</code>           \u2013            </li> <li> <code>Add</code>           \u2013            </li> <li> <code>Mul</code>           \u2013            </li> <li> <code>Where</code>           \u2013            </li> <li> <code>Sum</code>           \u2013            </li> <li> <code>Max</code>           \u2013            </li> <li> <code>Expand</code>           \u2013            </li> <li> <code>Reshape</code>           \u2013            </li> <li> <code>Permute</code>           \u2013            </li> <li> <code>Pad</code>           \u2013            </li> <li> <code>Shrink</code>           \u2013            </li> <li> <code>Flip</code>           \u2013            </li> </ul>"},{"location":"developer/function/#tinygrad.function.Contiguous","title":"Contiguous","text":"<pre><code>Contiguous(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.ContiguousBackward","title":"ContiguousBackward","text":"<pre><code>ContiguousBackward(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Cast","title":"Cast","text":"<pre><code>Cast(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Reciprocal","title":"Reciprocal","text":"<pre><code>Reciprocal(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Sin","title":"Sin","text":"<pre><code>Sin(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Relu","title":"Relu","text":"<pre><code>Relu(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Log","title":"Log","text":"<pre><code>Log(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Exp","title":"Exp","text":"<pre><code>Exp(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Sqrt","title":"Sqrt","text":"<pre><code>Sqrt(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Sigmoid","title":"Sigmoid","text":"<pre><code>Sigmoid(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Sign","title":"Sign","text":"<pre><code>Sign(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Less","title":"Less","text":"<pre><code>Less(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Xor","title":"Xor","text":"<pre><code>Xor(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Add","title":"Add","text":"<pre><code>Add(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Mul","title":"Mul","text":"<pre><code>Mul(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Where","title":"Where","text":"<pre><code>Where(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Sum","title":"Sum","text":"<pre><code>Sum(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Max","title":"Max","text":"<pre><code>Max(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Expand","title":"Expand","text":"<pre><code>Expand(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Reshape","title":"Reshape","text":"<pre><code>Reshape(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Permute","title":"Permute","text":"<pre><code>Permute(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Pad","title":"Pad","text":"<pre><code>Pad(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Shrink","title":"Shrink","text":"<pre><code>Shrink(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/function/#tinygrad.function.Flip","title":"Flip","text":"<pre><code>Flip(\n    device: Union[str, Tuple[str, ...]],\n    *tensors: Tensor,\n    metadata: Optional[Metadata] = None\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"developer/hcq/","title":"HCQ Compatible Runtime","text":""},{"location":"developer/hcq/#overview","title":"Overview","text":"<p>The main aspect of HCQ-compatible runtimes is how they interact with devices. In HCQ, all interactions with devices occur in a hardware-friendly manner using command queues. This approach allows commands to be issued directly to devices, bypassing runtime overhead such as HIP or CUDA. Additionally, by using the HCQ API, these runtimes can benefit from various optimizations and features, including HCQGraph and built-in profiling capabilities.</p>"},{"location":"developer/hcq/#command-queues","title":"Command Queues","text":"<p>To interact with devices, there are 2 types of queues: <code>HWComputeQueue</code> and <code>HWCopyQueue</code>. Commands which are defined in a base <code>HWCommandQueue</code> class should be supported by both queues. These methods are timestamp and synchronization methods like signal and wait.</p> <p>For example, the following Python code enqueues a wait, execute, and signal command on the HCQ-compatible device: <pre><code>HWComputeQueue().wait(signal_to_wait, value_to_wait) \\\n                .exec(program, args_state, global_dims, local_dims) \\\n                .signal(signal_to_fire, value_to_fire) \\\n                .submit(your_device)\n</code></pre></p> <p>Each runtime should implement the required functions that are defined in the <code>HWCommandQueue</code>, <code>HWComputeQueue</code>, and <code>HWCopyQueue</code> classes.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue","title":"HWCommandQueue","text":"<pre><code>HWCommandQueue()\n</code></pre> <p>A base class for hardware command queues in the HCQ (Hardware Command Queue) API. Both compute and copy queues should have the following commands implemented.</p> <p>Methods:</p> <ul> <li> <code>signal</code>             \u2013              <p>Enqueues a signal command which sets the signal to the given value, ensuring all previous operations are completed.</p> </li> <li> <code>wait</code>             \u2013              <p>Enqueues a wait command which halts execution until the signal is greater than or equal to a specific value.</p> </li> <li> <code>timestamp</code>             \u2013              <p>Enqueues a timestamp command which records the current time in a signal after all previously enqueued commands are completed.</p> </li> <li> <code>update_signal</code>             \u2013              <p>Updates a previously queued signal command.</p> </li> <li> <code>update_wait</code>             \u2013              <p>Updates a previously queued wait command.</p> </li> <li> <code>bind</code>             \u2013              <p>Associates the queue with a specific device for optimized execution.</p> </li> <li> <code>submit</code>             \u2013              <p>Submits the command queue to a specific device for execution.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.signal","title":"signal","text":"<pre><code>signal(signal: HCQSignal, value: int)\n</code></pre> <p>Enqueues a signal command which sets the signal to the given value, ensuring all previous operations are completed.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>HCQSignal</code>)           \u2013            <p>The signal to set</p> </li> <li> <code>value</code>               (<code>int</code>)           \u2013            <p>The value to set the signal to</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.wait","title":"wait","text":"<pre><code>wait(signal: HCQSignal, value: int)\n</code></pre> <p>Enqueues a wait command which halts execution until the signal is greater than or equal to a specific value.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>HCQSignal</code>)           \u2013            <p>The signal to wait on</p> </li> <li> <code>value</code>               (<code>int</code>)           \u2013            <p>The value to wait for</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.timestamp","title":"timestamp","text":"<pre><code>timestamp(signal: HCQSignal)\n</code></pre> <p>Enqueues a timestamp command which records the current time in a signal after all previously enqueued commands are completed.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>HCQSignal</code>)           \u2013            <p>The signal to store the timestamp</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.update_signal","title":"update_signal","text":"<pre><code>update_signal(\n    cmd_idx: int,\n    signal: Optional[Any] = None,\n    value: Optional[int] = None,\n)\n</code></pre> <p>Updates a previously queued signal command.</p> <p>Parameters:</p> <ul> <li> <code>cmd_idx</code>               (<code>int</code>)           \u2013            <p>Index of the signal command to update</p> </li> <li> <code>signal</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>New signal to set (if None, keeps the original)</p> </li> <li> <code>value</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>New value to set (if None, keeps the original)</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.update_wait","title":"update_wait","text":"<pre><code>update_wait(\n    cmd_idx: int,\n    signal: Optional[Any] = None,\n    value: Optional[int] = None,\n)\n</code></pre> <p>Updates a previously queued wait command.</p> <p>Parameters:</p> <ul> <li> <code>cmd_idx</code>               (<code>int</code>)           \u2013            <p>Index of the wait command to update</p> </li> <li> <code>signal</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>New signal to wait on (if None, keeps the original)</p> </li> <li> <code>value</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>New value to wait for (if None, keeps the original)</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.bind","title":"bind","text":"<pre><code>bind(device: HCQCompiled)\n</code></pre> <p>Associates the queue with a specific device for optimized execution.</p> <p>This optional method allows backend implementations to tailor the queue for efficient use on the given device. When implemented, it can eliminate the need to copy queues into the device, thereby enhancing performance.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>HCQCompiled</code>)           \u2013            <p>The target device for queue optimization.</p> </li> </ul> Note <p>Implementing this method is optional but recommended for performance gains.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCommandQueue.submit","title":"submit","text":"<pre><code>submit(device: HCQCompiled)\n</code></pre> <p>Submits the command queue to a specific device for execution.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>HCQCompiled</code>)           \u2013            <p>The device to submit the queue to</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWComputeQueue","title":"HWComputeQueue","text":"<pre><code>HWComputeQueue()\n</code></pre> <p>               Bases: <code>HWCommandQueue</code></p> <p>Methods:</p> <ul> <li> <code>memory_barrier</code>             \u2013              <p>Enqueues a memory barrier command to ensure memory coherence between agents.</p> </li> <li> <code>exec</code>             \u2013              <p>Enqueues an execution command for a kernel program.</p> </li> <li> <code>update_exec</code>             \u2013              <p>Updates a previously queued execution command.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWComputeQueue.memory_barrier","title":"memory_barrier","text":"<pre><code>memory_barrier()\n</code></pre> <p>Enqueues a memory barrier command to ensure memory coherence between agents.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWComputeQueue.exec","title":"exec","text":"<pre><code>exec(\n    prg: HCQProgram,\n    args_state: HCQArgsState,\n    global_size: Tuple[int, int, int],\n    local_size: Tuple[int, int, int],\n)\n</code></pre> <p>Enqueues an execution command for a kernel program.</p> <p>Parameters:</p> <ul> <li> <code>prg</code>               (<code>HCQProgram</code>)           \u2013            <p>The program to execute</p> </li> <li> <code>args_state</code>               (<code>HCQArgsState</code>)           \u2013            <p>The args state to execute program with</p> </li> <li> <code>global_size</code>               (<code>Tuple[int, int, int]</code>)           \u2013            <p>The global work size</p> </li> <li> <code>local_size</code>               (<code>Tuple[int, int, int]</code>)           \u2013            <p>The local work size</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWComputeQueue.update_exec","title":"update_exec","text":"<pre><code>update_exec(\n    cmd_idx: int,\n    global_size: Optional[Tuple[int, int, int]] = None,\n    local_size: Optional[Tuple[int, int, int]] = None,\n)\n</code></pre> <p>Updates a previously queued execution command.</p> <p>Parameters:</p> <ul> <li> <code>cmd_idx</code>               (<code>int</code>)           \u2013            <p>Index of the execution command to update</p> </li> <li> <code>global_size</code>               (<code>Optional[Tuple[int, int, int]]</code>, default:                   <code>None</code> )           \u2013            <p>New global work size (if None, keeps the original)</p> </li> <li> <code>local_size</code>               (<code>Optional[Tuple[int, int, int]]</code>, default:                   <code>None</code> )           \u2013            <p>New local work size (if None, keeps the original)</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCopyQueue","title":"HWCopyQueue","text":"<pre><code>HWCopyQueue()\n</code></pre> <p>               Bases: <code>HWCommandQueue</code></p> <p>Methods:</p> <ul> <li> <code>copy</code>             \u2013              <p>Enqueues a copy command to transfer data.</p> </li> <li> <code>update_copy</code>             \u2013              <p>Updates a previously queued copy command.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCopyQueue.copy","title":"copy","text":"<pre><code>copy(dest: HCQBuffer, src: HCQBuffer, copy_size: int)\n</code></pre> <p>Enqueues a copy command to transfer data.</p> <p>Parameters:</p> <ul> <li> <code>dest</code>               (<code>HCQBuffer</code>)           \u2013            <p>The destination of the copy</p> </li> <li> <code>src</code>               (<code>HCQBuffer</code>)           \u2013            <p>The source of the copy</p> </li> <li> <code>copy_size</code>               (<code>int</code>)           \u2013            <p>The size of data to copy</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWCopyQueue.update_copy","title":"update_copy","text":"<pre><code>update_copy(\n    cmd_idx: int,\n    dest: Optional[HCQBuffer] = None,\n    src: Optional[HCQBuffer] = None,\n)\n</code></pre> <p>Updates a previously queued copy command.</p> <p>Parameters:</p> <ul> <li> <code>cmd_idx</code>               (<code>int</code>)           \u2013            <p>Index of the copy command to update</p> </li> <li> <code>dest</code>               (<code>Optional[HCQBuffer]</code>, default:                   <code>None</code> )           \u2013            <p>New destination of the copy (if None, keeps the original)</p> </li> <li> <code>src</code>               (<code>Optional[HCQBuffer]</code>, default:                   <code>None</code> )           \u2013            <p>New source of the copy (if None, keeps the original)</p> </li> </ul>"},{"location":"developer/hcq/#implementing-custom-commands","title":"Implementing custom commands","text":"<p>To implement custom commands in the queue, use the @hcq_command decorator for your command implementations.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.hcq_command","title":"hcq_command","text":"<pre><code>hcq_command(func)\n</code></pre> <p>Decorator for HWCommandQueue commands. Enables command indexing and stores metadata for command updates.</p> For example <pre><code>  @hcq_command\n  def command_method(self, ...): ...\n</code></pre>"},{"location":"developer/hcq/#hcq-compatible-device","title":"HCQ Compatible Device","text":"<p>The <code>HCQCompiled</code> class defines the API for HCQ-compatible devices. This class serves as an abstract base class that device-specific implementations should inherit from and implement.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQCompiled","title":"HCQCompiled","text":"<pre><code>HCQCompiled(\n    device: str,\n    allocator: Allocator,\n    renderer: Renderer,\n    compiler: Compiler,\n    runtime,\n    signal_t: Type[HCQSignal],\n    comp_queue_t: Type[HWComputeQueue],\n    copy_queue_t: Optional[Type[HWCopyQueue]],\n)\n</code></pre> <p>               Bases: <code>Compiled</code></p> <p>A base class for devices compatible with the HCQ (Hardware Command Queue) API.</p>"},{"location":"developer/hcq/#signals","title":"Signals","text":"<p>Signals are device-dependent structures used for synchronization and timing in HCQ-compatible devices. They should be designed to record both a <code>value</code> and a <code>timestamp</code> within the same signal. HCQ-compatible backend implementations should use <code>HCQSignal</code> as a base class.</p> <p>The following Python code demonstrates the usage of signals:</p> <pre><code>signal = your_device.signal_t()\n\nHWComputeQueue().timestamp(signal) \\\n                .signal(signal, value_to_fire) \\\n                .submit(your_device)\n\nsignal.wait(value_to_fire)\nsignaled_value = signal.value # should be the same as `value_to_fire`\ntimestamp = signal.timestamp\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal","title":"HCQSignal","text":"<pre><code>HCQSignal(value: int = 0, is_timeline: bool = False)\n</code></pre> <p>Methods:</p> <ul> <li> <code>wait</code>             \u2013              <p>Waits the signal is greater than or equal to a specific value.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>value</code>               (<code>int</code>)           \u2013            </li> <li> <code>timestamp</code>               (<code>Decimal</code>)           \u2013            <p>Get the timestamp field of the signal.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.value","title":"value  <code>property</code> <code>writable</code>","text":"<pre><code>value: int\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.timestamp","title":"timestamp  <code>property</code>","text":"<pre><code>timestamp: Decimal\n</code></pre> <p>Get the timestamp field of the signal.</p> <p>This property provides read-only access to the signal's timestamp.</p> <p>Returns:</p> <ul> <li> <code>Decimal</code>           \u2013            <p>The timestamp in microseconds.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.wait","title":"wait","text":"<pre><code>wait(\n    value: int,\n    timeout: int = getenv(\"HCQDEV_WAIT_TIMEOUT_MS\", 30000),\n)\n</code></pre> <p>Waits the signal is greater than or equal to a specific value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>int</code>)           \u2013            <p>The value to wait for.</p> </li> <li> <code>timeout</code>               (<code>int</code>, default:                   <code>getenv('HCQDEV_WAIT_TIMEOUT_MS', 30000)</code> )           \u2013            <p>Maximum time to wait in milliseconds. Defaults to 10s.</p> </li> </ul>"},{"location":"developer/hcq/#synchronization-signals","title":"Synchronization signals","text":"<p>Each HCQ-compatible device must allocate two signals for global synchronization purposes. These signals are passed to the <code>HCQCompiled</code> base class during initialization: an active timeline signal <code>self.timeline_signal</code> and a shadow timeline signal <code>self._shadow_timeline_signal</code> which helps to handle signal value overflow issues. You can find more about synchronization in the synchronization section</p>"},{"location":"developer/hcq/#hcq-compatible-allocator","title":"HCQ Compatible Allocator","text":"<p>The <code>HCQAllocator</code> base class simplifies allocator logic by leveraging command queues abstractions. This class efficiently handles copy and transfer operations, leaving only the alloc and free functions to be implemented by individual backends.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQAllocator","title":"HCQAllocator","text":"<pre><code>HCQAllocator(\n    device: HCQCompiled,\n    batch_size: int = 2 &lt;&lt; 20,\n    batch_cnt: int = 32,\n)\n</code></pre> <p>               Bases: <code>LRUAllocator</code></p> <p>A base allocator class compatible with the HCQ (Hardware Command Queue) API.</p> <p>This class implements basic copy operations following the HCQ API, utilizing both <code>HWComputeQueue</code> and <code>HWCopyQueue</code>.</p> <p>Methods:</p> <ul> <li> <code>_alloc</code>             \u2013              </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQAllocator._alloc","title":"_alloc","text":"<pre><code>_alloc(size: int, options: BufferOptions) -&gt; HCQBuffer\n</code></pre>"},{"location":"developer/hcq/#hcq-allocator-result-protocol","title":"HCQ Allocator Result Protocol","text":"<p>Backends must adhere to the <code>HCQBuffer</code> protocol when returning allocation results.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer","title":"HCQBuffer","text":"<p>               Bases: <code>Protocol</code></p> <p>Attributes:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            </li> <li> <code>va_addr</code>               (<code>int</code>)           \u2013            </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.va_addr","title":"va_addr  <code>instance-attribute</code>","text":"<pre><code>va_addr: int\n</code></pre>"},{"location":"developer/hcq/#hcq-compatible-program","title":"HCQ Compatible Program","text":"<p><code>HCQProgram</code> is a base class for defining programs compatible with HCQ-enabled devices. It provides a flexible framework for handling different argument layouts (see <code>HCQArgsState</code>).</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram","title":"HCQProgram","text":"<pre><code>HCQProgram(\n    args_state_t: Type[HCQArgsState],\n    device: HCQCompiled,\n    name: str,\n    kernargs_alloc_size: int,\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Enqueues the program for execution with the given arguments and dimensions.</p> </li> <li> <code>fill_kernargs</code>             \u2013              <p>Fills arguments for the kernel, optionally allocating space from the device if <code>kernargs_ptr</code> is not provided.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram.__call__","title":"__call__","text":"<pre><code>__call__(\n    *bufs: HCQBuffer,\n    global_size: Tuple[int, int, int] = (1, 1, 1),\n    local_size: Tuple[int, int, int] = (1, 1, 1),\n    vals: Tuple[int, ...] = (),\n    wait: bool = False\n) -&gt; Optional[float]\n</code></pre> <p>Enqueues the program for execution with the given arguments and dimensions.</p> <p>Parameters:</p> <ul> <li> <code>bufs</code>               (<code>HCQBuffer</code>, default:                   <code>()</code> )           \u2013            <p>Buffer arguments to execute the kernel with.</p> </li> <li> <code>global_size</code>               (<code>Tuple[int, int, int]</code>, default:                   <code>(1, 1, 1)</code> )           \u2013            <p>Specifies the global work size for kernel execution (equivalent to CUDA's grid size).</p> </li> <li> <code>local_size</code>               (<code>Tuple[int, int, int]</code>, default:                   <code>(1, 1, 1)</code> )           \u2013            <p>Specifies the local work size for kernel execution (equivalent to CUDA's block size).</p> </li> <li> <code>vals</code>               (<code>Tuple[int, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Value arguments to execute the kernel with.</p> </li> <li> <code>wait</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, waits for the kernel to complete execution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[float]</code>           \u2013            <p>Execution time of the kernel if 'wait' is True, otherwise None.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram.fill_kernargs","title":"fill_kernargs","text":"<pre><code>fill_kernargs(\n    bufs: Tuple[HCQBuffer, ...],\n    vals: Tuple[int, ...] = (),\n    kernargs_ptr: Optional[int] = None,\n) -&gt; HCQArgsState\n</code></pre> <p>Fills arguments for the kernel, optionally allocating space from the device if <code>kernargs_ptr</code> is not provided. Args:   bufs: Buffers to be written to kernel arguments.   vals: Values to be written to kernel arguments.   kernargs_ptr: Optional pointer to pre-allocated kernel arguments memory. Returns:   Arguments state with the given buffers and values set for the program.</p>"},{"location":"developer/hcq/#arguments-state","title":"Arguments State","text":"<p><code>HCQArgsState</code> is a base class for managing the argument state for HCQ programs. Backend implementations should create a subclass of <code>HCQArgsState</code> to manage arguments for the given program.</p> <p>Lifetime: The <code>HCQArgsState</code> is passed to <code>HWComputeQueue.exec</code> and is guaranteed not to be freed until <code>HWComputeQueue.submit</code> for the same queue is called.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState","title":"HCQArgsState","text":"<pre><code>HCQArgsState(\n    ptr: int,\n    prg: HCQProgram,\n    bufs: Tuple[HCQBuffer, ...],\n    vals: Tuple[int, ...] = (),\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>update_buffer</code>             \u2013              </li> <li> <code>update_var</code>             \u2013              </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState.update_buffer","title":"update_buffer","text":"<pre><code>update_buffer(index: int, buf: HCQBuffer)\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState.update_var","title":"update_var","text":"<pre><code>update_var(index: int, val: int)\n</code></pre>"},{"location":"developer/hcq/#synchronization","title":"Synchronization","text":"<p>HCQ-compatible devices use a global timeline signal for synchronizing all operations. This mechanism ensures proper ordering and completion of tasks across the device. By convention, <code>self.timeline_value</code> points to the next value to signal. So, to wait for all previous operations on the device to complete, wait for <code>self.timeline_value - 1</code> value. The following Python code demonstrates the typical usage of signals to synchronize execution to other operations on the device:</p> <pre><code>HWComputeQueue().wait(your_device.timeline_signal, your_device.timeline_value - 1) \\\n                .exec(...)\n                .signal(your_device.timeline_signal, your_device.timeline_value) \\\n                .submit(your_device)\nyour_device.timeline_value += 1\n\n# Optionally wait for execution\nyour_device.timeline_signal.wait(your_device.timeline_value - 1)\n</code></pre>"},{"location":"developer/hcq/#hcqgraph","title":"HCQGraph","text":"<p>HCQGraph is a core feature that implements <code>GraphRunner</code> for HCQ-compatible devices. <code>HCQGraph</code> builds a static <code>HWComputeQueue</code> and <code>HWCopyQueue</code> for all operations per device. To optimize enqueue time, only the necessary parts of the queues are updated for each run using the update APIs of the queues, avoiding a complete rebuild. Optionally, queues can implement a <code>bind</code> API, which allows further optimization by eliminating the need to copy the queues into the device ring.</p>"},{"location":"developer/runtime/","title":"Runtime Overview","text":""},{"location":"developer/runtime/#overview","title":"Overview","text":"<p>A typical runtime consists of the following parts:</p> <ul> <li>Compiled</li> <li>Allocator</li> <li>Program</li> <li>Compiler</li> </ul>"},{"location":"developer/runtime/#compiled","title":"Compiled","text":"<p>The <code>Compiled</code> class is responsible for initializing and managing a device.</p>"},{"location":"developer/runtime/#tinygrad.device.Compiled","title":"Compiled","text":"<pre><code>Compiled(\n    device: str,\n    allocator: Allocator,\n    renderer: Optional[Renderer],\n    compiler: Optional[Compiler],\n    runtime,\n    graph=None,\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>synchronize</code>             \u2013              <p>Synchronize all pending operations on the device.</p> </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.Compiled.synchronize","title":"synchronize","text":"<pre><code>synchronize()\n</code></pre> <p>Synchronize all pending operations on the device.</p> <p>This method ensures that all previously queued operations on the device have been completed before proceeding.</p>"},{"location":"developer/runtime/#allocator","title":"Allocator","text":"<p>The <code>Allocator</code> class is responsible for managing memory on the device. There is also a version called the <code>LRUAllocator</code>, which caches allocated buffers to optimize performance.</p>"},{"location":"developer/runtime/#tinygrad.device.Allocator","title":"Allocator","text":"<p>Methods:</p> <ul> <li> <code>_alloc</code>             \u2013              </li> <li> <code>_free</code>             \u2013              </li> <li> <code>alloc</code>             \u2013              </li> <li> <code>copyin</code>             \u2013              </li> <li> <code>copyout</code>             \u2013              </li> <li> <code>free</code>             \u2013              </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.Allocator._alloc","title":"_alloc","text":"<pre><code>_alloc(size: int, options: BufferOptions)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator._free","title":"_free","text":"<pre><code>_free(opaque, options: BufferOptions)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.alloc","title":"alloc","text":"<pre><code>alloc(size: int, options: Optional[BufferOptions] = None)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.copyin","title":"copyin","text":"<pre><code>copyin(dest, src: memoryview)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.copyout","title":"copyout","text":"<pre><code>copyout(dest: memoryview, src)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.free","title":"free","text":"<pre><code>free(\n    opaque,\n    size: int,\n    options: Optional[BufferOptions] = None,\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator","title":"LRUAllocator","text":"<pre><code>LRUAllocator()\n</code></pre> <p>               Bases: <code>Allocator</code></p> <p>The LRU Allocator is responsible for caching buffers. It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.</p> <p>Methods:</p> <ul> <li> <code>alloc</code>             \u2013              </li> <li> <code>free</code>             \u2013              </li> <li> <code>free_cache</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>cache</code>               (<code>Dict[Tuple[int, Optional[BufferOptions]], Any]</code>)           \u2013            </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.cache","title":"cache  <code>instance-attribute</code>","text":"<pre><code>cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = (\n    defaultdict(list)\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.alloc","title":"alloc","text":"<pre><code>alloc(size: int, options: Optional[BufferOptions] = None)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.free","title":"free","text":"<pre><code>free(\n    opaque: Any,\n    size: int,\n    options: Optional[BufferOptions] = None,\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.free_cache","title":"free_cache","text":"<pre><code>free_cache()\n</code></pre>"},{"location":"developer/runtime/#program","title":"Program","text":"<p>The <code>Program</code> class is created for each loaded program. It is responsible for compiling and executing the program on the device. As an example, here is a <code>ClangProgram</code> implementation which loads program and runs it.</p>"},{"location":"developer/runtime/#tinygrad.runtime.ops_clang.ClangProgram","title":"ClangProgram","text":"<pre><code>ClangProgram(name: str, lib: bytes)\n</code></pre> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>fxn</code>           \u2013            </li> </ul> Source code in <code>tinygrad/runtime/ops_clang.py</code> <pre><code>def __init__(self, name:str, lib:bytes):\n  if DEBUG &gt;= 6: cpu_objdump(lib)\n  self.name, self.lib = name, lib\n  # write to disk so we can load it\n  with tempfile.NamedTemporaryFile(delete=True) as cached_file_path:\n    pathlib.Path(cached_file_path.name).write_bytes(lib)\n    self.fxn = ctypes.CDLL(str(cached_file_path.name))[name]\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_clang.ClangProgram.fxn","title":"fxn  <code>instance-attribute</code>","text":"<pre><code>fxn = CDLL(str(name))[name]\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_clang.ClangProgram.__call__","title":"__call__","text":"<pre><code>__call__(*bufs, vals=(), wait=False)\n</code></pre> Source code in <code>tinygrad/runtime/ops_clang.py</code> <pre><code>def __call__(self, *bufs, vals=(), wait=False): return cpu_time_execution(lambda: self.fxn(*bufs, *vals), enable=wait)\n</code></pre>"},{"location":"developer/runtime/#compiler","title":"Compiler","text":"<p>The <code>Compiler</code> class compiles the output from the <code>Renderer</code> and produces it in a device-specific format.</p>"},{"location":"developer/runtime/#tinygrad.device.Compiler","title":"Compiler","text":"<pre><code>Compiler(cachekey: Optional[str] = None)\n</code></pre> <p>Methods:</p> <ul> <li> <code>compile</code>             \u2013              </li> <li> <code>compile_cached</code>             \u2013              </li> <li> <code>disassemble</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>cachekey</code>           \u2013            </li> </ul> Source code in <code>tinygrad/device.py</code> <pre><code>def __init__(self, cachekey:Optional[str]=None): self.cachekey = None if getenv(\"DISABLE_COMPILER_CACHE\") else cachekey\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.cachekey","title":"cachekey  <code>instance-attribute</code>","text":"<pre><code>cachekey = (\n    None if getenv(\"DISABLE_COMPILER_CACHE\") else cachekey\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.compile","title":"compile","text":"<pre><code>compile(src: str) -&gt; bytes\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def compile(self, src:str) -&gt; bytes: return src.encode()   # NOTE: empty compiler is the default\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.compile_cached","title":"compile_cached","text":"<pre><code>compile_cached(src: str) -&gt; bytes\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def compile_cached(self, src:str) -&gt; bytes:\n  if self.cachekey is None or (lib := diskcache_get(self.cachekey, src)) is None:\n    assert not getenv(\"ASSERT_COMPILE\"), f\"tried to compile with ASSERT_COMPILE set\\n{src}\"\n    lib = self.compile(src)\n    if self.cachekey is not None: diskcache_put(self.cachekey, src, lib)\n  return lib\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.disassemble","title":"disassemble","text":"<pre><code>disassemble(lib: bytes)\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def disassemble(self, lib:bytes): pass\n</code></pre>"},{"location":"developer/uop/","title":"UOp","text":""},{"location":"developer/uop/#tinygrad.ops.UOp","title":"UOp","text":"<pre><code>UOp(\n    op: Ops,\n    dtype: DType = void,\n    src: Tuple[UOp, ...] = tuple(),\n    arg: Any = None,\n)\n</code></pre> <p>               Bases: <code>MathTrait</code></p> Source code in <code>tinygrad/ops.py</code> <pre><code>def __init__(self, op:Ops, dtype:DType=dtypes.void, src: Tuple[UOp,...]=tuple(), arg:Any=None):\n  # TODO: instant check rules here make debugging easier\n  self.op, self.dtype, self.src, self.arg = op, dtype, src, arg\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops","title":"Ops","text":"<p>               Bases: <code>FastEnum</code></p> <p>Attributes:</p> <ul> <li> <code>SINK</code>           \u2013            </li> <li> <code>CONTIGUOUS</code>           \u2013            </li> <li> <code>PRELOAD</code>           \u2013            </li> <li> <code>COPY</code>           \u2013            </li> <li> <code>EMPTY</code>           \u2013            </li> <li> <code>BUFFER_VIEW</code>           \u2013            </li> <li> <code>EXPAND</code>           \u2013            </li> <li> <code>CONTRACT</code>           \u2013            </li> <li> <code>VIEW</code>           \u2013            </li> <li> <code>DEFINE_GLOBAL</code>           \u2013            </li> <li> <code>BUFFER</code>           \u2013            </li> <li> <code>DEFINE_VAR</code>           \u2013            </li> <li> <code>DEFINE_LOCAL</code>           \u2013            </li> <li> <code>DEFINE_ACC</code>           \u2013            </li> <li> <code>VALID</code>           \u2013            </li> <li> <code>SPECIAL</code>           \u2013            </li> <li> <code>NOOP</code>           \u2013            </li> <li> <code>REDUCE_AXIS</code>           \u2013            </li> <li> <code>SUM</code>           \u2013            </li> <li> <code>PROD</code>           \u2013            </li> <li> <code>REDUCE_MAX</code>           \u2013            </li> <li> <code>GEP</code>           \u2013            </li> <li> <code>VECTORIZE</code>           \u2013            </li> <li> <code>CAST</code>           \u2013            </li> <li> <code>BITCAST</code>           \u2013            </li> <li> <code>EXP2</code>           \u2013            </li> <li> <code>LOG2</code>           \u2013            </li> <li> <code>SIN</code>           \u2013            </li> <li> <code>SQRT</code>           \u2013            </li> <li> <code>RECIP</code>           \u2013            </li> <li> <code>NEG</code>           \u2013            </li> <li> <code>LOAD</code>           \u2013            </li> <li> <code>WMMA</code>           \u2013            </li> <li> <code>ADD</code>           \u2013            </li> <li> <code>MUL</code>           \u2013            </li> <li> <code>IDIV</code>           \u2013            </li> <li> <code>MAX</code>           \u2013            </li> <li> <code>MOD</code>           \u2013            </li> <li> <code>CMPLT</code>           \u2013            </li> <li> <code>CMPNE</code>           \u2013            </li> <li> <code>XOR</code>           \u2013            </li> <li> <code>SHL</code>           \u2013            </li> <li> <code>SHR</code>           \u2013            </li> <li> <code>OR</code>           \u2013            </li> <li> <code>AND</code>           \u2013            </li> <li> <code>THREEFRY</code>           \u2013            </li> <li> <code>SUB</code>           \u2013            </li> <li> <code>FDIV</code>           \u2013            </li> <li> <code>WHERE</code>           \u2013            </li> <li> <code>MULACC</code>           \u2013            </li> <li> <code>STORE</code>           \u2013            </li> <li> <code>ASSIGN</code>           \u2013            </li> <li> <code>BIND</code>           \u2013            </li> <li> <code>INDEX</code>           \u2013            </li> <li> <code>BARRIER</code>           \u2013            </li> <li> <code>IF</code>           \u2013            </li> <li> <code>RANGE</code>           \u2013            </li> <li> <code>ENDRANGE</code>           \u2013            </li> <li> <code>ENDIF</code>           \u2013            </li> <li> <code>VCONST</code>           \u2013            </li> <li> <code>CONST</code>           \u2013            </li> </ul>"},{"location":"developer/uop/#tinygrad.ops.Ops.SINK","title":"SINK","text":"<pre><code>SINK = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CONTIGUOUS","title":"CONTIGUOUS","text":"<pre><code>CONTIGUOUS = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.PRELOAD","title":"PRELOAD","text":"<pre><code>PRELOAD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.COPY","title":"COPY","text":"<pre><code>COPY = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.EMPTY","title":"EMPTY","text":"<pre><code>EMPTY = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.BUFFER_VIEW","title":"BUFFER_VIEW","text":"<pre><code>BUFFER_VIEW = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.EXPAND","title":"EXPAND","text":"<pre><code>EXPAND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CONTRACT","title":"CONTRACT","text":"<pre><code>CONTRACT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.VIEW","title":"VIEW","text":"<pre><code>VIEW = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.DEFINE_GLOBAL","title":"DEFINE_GLOBAL","text":"<pre><code>DEFINE_GLOBAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.BUFFER","title":"BUFFER","text":"<pre><code>BUFFER = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.DEFINE_VAR","title":"DEFINE_VAR","text":"<pre><code>DEFINE_VAR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.DEFINE_LOCAL","title":"DEFINE_LOCAL","text":"<pre><code>DEFINE_LOCAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.DEFINE_ACC","title":"DEFINE_ACC","text":"<pre><code>DEFINE_ACC = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.VALID","title":"VALID","text":"<pre><code>VALID = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SPECIAL","title":"SPECIAL","text":"<pre><code>SPECIAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.NOOP","title":"NOOP","text":"<pre><code>NOOP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.REDUCE_AXIS","title":"REDUCE_AXIS","text":"<pre><code>REDUCE_AXIS = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SUM","title":"SUM","text":"<pre><code>SUM = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.PROD","title":"PROD","text":"<pre><code>PROD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.REDUCE_MAX","title":"REDUCE_MAX","text":"<pre><code>REDUCE_MAX = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.GEP","title":"GEP","text":"<pre><code>GEP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.VECTORIZE","title":"VECTORIZE","text":"<pre><code>VECTORIZE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CAST","title":"CAST","text":"<pre><code>CAST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.BITCAST","title":"BITCAST","text":"<pre><code>BITCAST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.EXP2","title":"EXP2","text":"<pre><code>EXP2 = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.LOG2","title":"LOG2","text":"<pre><code>LOG2 = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SIN","title":"SIN","text":"<pre><code>SIN = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SQRT","title":"SQRT","text":"<pre><code>SQRT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.RECIP","title":"RECIP","text":"<pre><code>RECIP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.NEG","title":"NEG","text":"<pre><code>NEG = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.LOAD","title":"LOAD","text":"<pre><code>LOAD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.WMMA","title":"WMMA","text":"<pre><code>WMMA = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.ADD","title":"ADD","text":"<pre><code>ADD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.MUL","title":"MUL","text":"<pre><code>MUL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.IDIV","title":"IDIV","text":"<pre><code>IDIV = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.MAX","title":"MAX","text":"<pre><code>MAX = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.MOD","title":"MOD","text":"<pre><code>MOD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CMPLT","title":"CMPLT","text":"<pre><code>CMPLT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CMPNE","title":"CMPNE","text":"<pre><code>CMPNE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.XOR","title":"XOR","text":"<pre><code>XOR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SHL","title":"SHL","text":"<pre><code>SHL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SHR","title":"SHR","text":"<pre><code>SHR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.OR","title":"OR","text":"<pre><code>OR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.AND","title":"AND","text":"<pre><code>AND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.THREEFRY","title":"THREEFRY","text":"<pre><code>THREEFRY = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.SUB","title":"SUB","text":"<pre><code>SUB = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.FDIV","title":"FDIV","text":"<pre><code>FDIV = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.WHERE","title":"WHERE","text":"<pre><code>WHERE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.MULACC","title":"MULACC","text":"<pre><code>MULACC = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.STORE","title":"STORE","text":"<pre><code>STORE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.ASSIGN","title":"ASSIGN","text":"<pre><code>ASSIGN = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.BIND","title":"BIND","text":"<pre><code>BIND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.INDEX","title":"INDEX","text":"<pre><code>INDEX = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.BARRIER","title":"BARRIER","text":"<pre><code>BARRIER = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.IF","title":"IF","text":"<pre><code>IF = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.RANGE","title":"RANGE","text":"<pre><code>RANGE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.ENDRANGE","title":"ENDRANGE","text":"<pre><code>ENDRANGE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.ENDIF","title":"ENDIF","text":"<pre><code>ENDIF = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.VCONST","title":"VCONST","text":"<pre><code>VCONST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.ops.Ops.CONST","title":"CONST","text":"<pre><code>CONST = auto()\n</code></pre>"},{"location":"tensor/","title":"Tensor","text":""},{"location":"tensor/#tinygrad.Tensor","title":"Tensor","text":"<pre><code>Tensor(\n    data: Union[\n        None,\n        ConstType,\n        List,\n        Tuple,\n        LazyBuffer,\n        \"np.ndarray\",\n        bytes,\n        MultiLazyBuffer,\n        UOp,\n        Path,\n    ],\n    device: Optional[Union[str, tuple, list]] = None,\n    dtype: Optional[DTypeLike] = None,\n    requires_grad: Optional[bool] = None,\n)\n</code></pre> <p>               Bases: <code>SimpleMathTrait</code></p> <p>A <code>Tensor</code> is a multi-dimensional matrix containing elements of a single data type.</p> <p></p>"},{"location":"tensor/creation/","title":"Creation","text":""},{"location":"tensor/creation/#creation-basic","title":"Creation (basic)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.empty","title":"empty  <code>staticmethod</code>","text":"<pre><code>empty(*shape, **kwargs)\n</code></pre> <p>Creates an empty tensor with the given shape.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.empty(2, 3)\nprint(t.shape)\n</code></pre> <pre><code>(2, 3)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef empty(*shape, **kwargs):\n  \"\"\"\n  Creates an empty tensor with the given shape.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 3)\n  print(t.shape)\n  ```\n  \"\"\"\n  return Tensor._metaop(MetaOps.EMPTY, argfix(*shape), **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.zeros","title":"zeros  <code>staticmethod</code>","text":"<pre><code>zeros(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with zeros.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.zeros(2, 3).numpy())\n</code></pre> <pre><code>[[0. 0. 0.]\n [0. 0. 0.]]\n</code></pre> <pre><code>print(Tensor.zeros(2, 3, dtype=dtypes.int32).numpy())\n</code></pre> <pre><code>[[0 0 0]\n [0 0 0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef zeros(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with zeros.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.zeros(2, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.zeros(2, 3, dtype=dtypes.int32).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 0.0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.ones","title":"ones  <code>staticmethod</code>","text":"<pre><code>ones(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with ones.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.ones(2, 3).numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <pre><code>print(Tensor.ones(2, 3, dtype=dtypes.int32).numpy())\n</code></pre> <pre><code>[[1 1 1]\n [1 1 1]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef ones(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with ones.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(2, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(2, 3, dtype=dtypes.int32).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 1.0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.full","title":"full  <code>staticmethod</code>","text":"<pre><code>full(\n    shape: Tuple[sint, ...], fill_value: ConstType, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with the given value.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.full((2, 3), 42).numpy())\n</code></pre> <pre><code>[[42 42 42]\n [42 42 42]]\n</code></pre> <pre><code>print(Tensor.full((2, 3), False).numpy())\n</code></pre> <pre><code>[[False False False]\n [False False False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef full(shape:Tuple[sint, ...], fill_value:ConstType, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with the given value.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 3), 42).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 3), False).numpy())\n  ```\n  \"\"\"\n  return Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.arange","title":"arange  <code>staticmethod</code>","text":"<pre><code>arange(start, stop=None, step=1, **kwargs) -&gt; Tensor\n</code></pre> <p>Returns a 1-D tensor of size <code>ceil((stop - start) / step)</code> with values from <code>[start, stop)</code>, with spacing between values given by <code>step</code>.</p> <p>If <code>stop</code> is not specified, values are generated from <code>[0, start)</code> with the given <code>step</code>.</p> <p>If <code>stop</code> is specified, values are generated from <code>[start, stop)</code> with the given <code>step</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.arange(5).numpy())\n</code></pre> <pre><code>[0 1 2 3 4]\n</code></pre> <pre><code>print(Tensor.arange(5, 10).numpy())\n</code></pre> <pre><code>[5 6 7 8 9]\n</code></pre> <pre><code>print(Tensor.arange(5, 10, 2).numpy())\n</code></pre> <pre><code>[5 7 9]\n</code></pre> <pre><code>print(Tensor.arange(5.5, 10, 2).numpy())\n</code></pre> <pre><code>[5.5 7.5 9.5]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef arange(start, stop=None, step=1, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 1-D tensor of size `ceil((stop - start) / step)` with values from `[start, stop)`, with spacing between values given by `step`.\n\n  If `stop` is not specified, values are generated from `[0, start)` with the given `step`.\n\n  If `stop` is specified, values are generated from `[start, stop)` with the given `step`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5, 10).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5, 10, 2).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5.5, 10, 2).numpy())\n  ```\n  \"\"\"\n  if stop is None: stop, start = start, 0\n  dtype = kwargs.pop(\"dtype\", dtypes.default_float if any(isinstance(x, float) for x in (start, stop, step)) else dtypes.default_int)\n  # NOTE: this matches numpy, torch raises RuntimeError if stop-start and step have different signs\n  if (output_len:=ceildiv(stop-start, step)) &lt;= 0: return Tensor([], dtype=dtype, **kwargs)\n  return (Tensor.full((output_len,), step, dtype=dtype, **kwargs)._cumsum() + (start - step)).cast(dtype)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.linspace","title":"linspace  <code>staticmethod</code>","text":"<pre><code>linspace(\n    start: Union[int, float],\n    stop: Union[int, float],\n    steps: int,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Returns a 1-D tensor of <code>steps</code> evenly spaced values from <code>start</code> to <code>stop</code>, inclusive.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.linspace(0, 10, 5).numpy())\n</code></pre> <pre><code>[ 0.   2.5  5.   7.5 10. ]\n</code></pre> <pre><code>print(Tensor.linspace(-1, 1, 5).numpy())\n</code></pre> <pre><code>[-1.  -0.5  0.   0.5  1. ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef linspace(start:Union[int, float], stop:Union[int, float], steps:int, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 1-D tensor of `steps` evenly spaced values from `start` to `stop`, inclusive.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.linspace(0, 10, 5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.linspace(-1, 1, 5).numpy())\n  ```\n  \"\"\"\n  if steps &lt; 0: raise ValueError(\"number of steps must be non-negative\")\n  if (dtype := to_dtype(kwargs.pop(\"dtype\", dtypes.default_float))) == dtypes.bool: raise ValueError(\"linspace with bool dtype is not supported\")\n  if steps == 1: return Tensor([start], dtype=dtype, **kwargs)\n  return (start + Tensor.arange(steps, **kwargs) * ((stop - start) / (steps - 1))).cast(dtype)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.eye","title":"eye  <code>staticmethod</code>","text":"<pre><code>eye(n: int, m: Optional[int] = None, **kwargs) -&gt; Tensor\n</code></pre> <p>Returns a 2-D tensor with <code>n</code> rows and <code>m</code> columns, with ones on the diagonal and zeros elsewhere.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>print(Tensor.eye(3).numpy())\n</code></pre> <pre><code>[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n</code></pre> <pre><code>print(Tensor.eye(2, 4).numpy())\n</code></pre> <pre><code>[[1. 0. 0. 0.]\n [0. 1. 0. 0.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef eye(n:int, m:Optional[int]=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 2-D tensor with `n` rows and `m` columns, with ones on the diagonal and zeros elsewhere.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.eye(3).numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.eye(2, 4).numpy())\n  ```\n  \"\"\"\n  if n &lt; 0 or (m is not None and m &lt; 0): raise ValueError(f\"cannot have negative {n=}, {m=}\")\n  x = Tensor.ones((n,1),**kwargs).pad((None,(0,n))).flatten().shrink(((0,n*n),)).reshape(n,n)\n  return x if m is None else x.pad((None, (0, m-n))) if m &gt; n else x.shrink((None, (0, m)))\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.full_like","title":"full_like","text":"<pre><code>full_like(fill_value: ConstType, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with the given value. If <code>dtype</code> is not specified, the dtype of <code>self</code> is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.full_like(t, 42).numpy())\n</code></pre> <pre><code>[[42. 42. 42.]\n [42. 42. 42.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def full_like(self, fill_value:ConstType, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with the given value.\n  If `dtype` is not specified, the dtype of `self` is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.full_like(t, 42).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(self.shape, fill_value, dtype=kwargs.pop(\"dtype\", self.dtype), device=kwargs.pop(\"device\", self.device), **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.zeros_like","title":"zeros_like","text":"<pre><code>zeros_like(**kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with zeros.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.zeros_like(t).numpy())\n</code></pre> <pre><code>[[0. 0. 0.]\n [0. 0. 0.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def zeros_like(self, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with zeros.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.zeros_like(t).numpy())\n  ```\n  \"\"\"\n  return self.full_like(0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.ones_like","title":"ones_like","text":"<pre><code>ones_like(**kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with ones.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.zeros(2, 3)\nprint(Tensor.ones_like(t).numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def ones_like(self, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with ones.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.zeros(2, 3)\n  print(Tensor.ones_like(t).numpy())\n  ```\n  \"\"\"\n  return self.full_like(1, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#creation-external","title":"Creation (external)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.from_blob","title":"from_blob  <code>staticmethod</code>","text":"<pre><code>from_blob(\n    ptr: int, shape: Tuple[int, ...], **kwargs\n) -&gt; Tensor\n</code></pre> <p>Exposes the pointer as a Tensor without taking ownership of the original data. The pointer must remain valid for the entire lifetime of the created Tensor.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef from_blob(ptr:int, shape:Tuple[int, ...], **kwargs) -&gt; Tensor:\n  \"\"\"\n  Exposes the pointer as a Tensor without taking ownership of the original data.\n  The pointer must remain valid for the entire lifetime of the created Tensor.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n  \"\"\"\n\n  r = Tensor._metaop(MetaOps.EMPTY, shape, **kwargs)\n  r.lazydata.buffer.allocate(external_ptr=ptr)\n  del r.lazydata.srcs # fake realize\n  return r\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.from_url","title":"from_url  <code>staticmethod</code>","text":"<pre><code>from_url(\n    url: str, gunzip: bool = False, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Create a Tensor from a URL.</p> <p>This is the preferred way to access Internet resources. It currently returns a DISK Tensor, but in the future it may return an HTTP Tensor. This also will soon become lazy (when possible) and not print progress without DEBUG.</p> <p>THe <code>gunzip</code> flag will gzip extract the resource and return an extracted Tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef from_url(url:str, gunzip:bool=False, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Create a Tensor from a URL.\n\n  This is the preferred way to access Internet resources.\n  It currently returns a DISK Tensor, but in the future it may return an HTTP Tensor.\n  This also will soon become lazy (when possible) and not print progress without DEBUG.\n\n  THe `gunzip` flag will gzip extract the resource and return an extracted Tensor.\n  \"\"\"\n  return Tensor(fetch(url, gunzip=gunzip), **kwargs)\n</code></pre>"},{"location":"tensor/creation/#creation-random","title":"Creation (random)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.manual_seed","title":"manual_seed  <code>staticmethod</code>","text":"<pre><code>manual_seed(seed=0)\n</code></pre> <p>Sets the seed for random operations.</p> <p><pre><code>Tensor.manual_seed(42)\nprint(Tensor.rand(5).numpy())\nprint(Tensor.rand(5).numpy())\n</code></pre> <pre><code>[0.997  0.5899 0.2225 0.7551 0.9057]\n[0.6162 0.6213 0.9791 0.7851 0.4178]\n</code></pre> <pre><code>Tensor.manual_seed(42)  # reset to the same seed\nprint(Tensor.rand(5).numpy())\nprint(Tensor.rand(5).numpy())\n</code></pre> <pre><code>[0.997  0.5899 0.2225 0.7551 0.9057]\n[0.6162 0.6213 0.9791 0.7851 0.4178]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef manual_seed(seed=0):\n  \"\"\"\n  Sets the seed for random operations.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.rand(5).numpy())\n  print(Tensor.rand(5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)  # reset to the same seed\n  print(Tensor.rand(5).numpy())\n  print(Tensor.rand(5).numpy())\n  ```\n  \"\"\"\n  Tensor._seed, Tensor._device_seeds, Tensor._device_rng_counters = seed, {}, {}\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.rand","title":"rand  <code>staticmethod</code>","text":"<pre><code>rand(\n    *shape,\n    device: Optional[str] = None,\n    dtype: Optional[DTypeLike] = None,\n    contiguous: bool = True,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[0, 1)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.rand(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0.997  0.5899 0.2225]\n [0.7551 0.9057 0.8649]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef rand(*shape, device:Optional[str]=None, dtype:Optional[DTypeLike]=None, contiguous:bool=True, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[0, 1)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.rand(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  if not dtypes.is_float(dtype := to_dtype(dtype or dtypes.default_float)): raise ValueError(f\"rand only supports float dtypes, got {dtype}\")\n  if not all_int(shape:=argfix(*shape)) or not all(s &gt;= 0 for s in shape): raise ValueError(f\"invalid input {shape=}\")\n  if device is not None and not isinstance(device, str): raise ValueError(f\"rand only supports single device, got {device=}\")\n  _device = device = Device.canonicalize(device)\n\n  # when using MOCKGPU and NV generate rand on CLANG\n  if getenv(\"MOCKGPU\") and device.startswith(\"NV\"): device = \"CLANG\"\n\n  # generate per device seeds and rng counter if we haven't seen this device yet\n  if device not in Tensor._device_seeds:\n    Tensor._device_seeds[device] = Tensor(\n      [int.from_bytes(hashlib.sha256(len(Tensor._device_seeds).to_bytes(4, \"big\")).digest(), \"big\"), Tensor._seed],\n      device=device, dtype=dtypes.uint32, requires_grad=False)\n    Tensor._device_rng_counters[device] = Tensor([0], device=device, dtype=dtypes.uint32, requires_grad=False)\n    had_counter = False\n  else: had_counter = True\n\n  # if shape has 0, return zero tensor\n  if (num := ceildiv(((num_ := prod(shape)) * dtype.itemsize), 4)) == 0: return Tensor.zeros(shape, device=_device, dtype=dtype, **kwargs)\n\n  # increment rng counter for devices\n  if had_counter: Tensor._device_rng_counters[device].assign(Tensor._device_rng_counters[device] + num).contiguous()\n\n  # threefry random bits\n  counts0 = (Tensor.arange(ceildiv(num, 2), device=device, dtype=dtypes.uint32, requires_grad=False)+Tensor._device_rng_counters[device])\n  counts1 = counts0 + ceildiv(num, 2)\n  bits = Tensor._threefry_random_bits(Tensor._device_seeds[device], counts0, counts1)[:num]\n\n  # bitcast to uint with same number of bits\n  _, nmant = dtypes.finfo(dtype)\n  uint_dtype = {1: dtypes.uint8, 2: dtypes.uint16, 4: dtypes.uint32, 8: dtypes.uint64}[dtype.itemsize]\n  bits = bits.bitcast(uint_dtype)\n  # only randomize the mantissa bits and set the exponent to 1\n  one = Tensor.ones_like(bits, device=bits.device, dtype=dtype).bitcast(uint_dtype)\n  bits = bits.rshift((dtype.itemsize * 8) - nmant).bitwise_or(one)\n  # bitcast back to the original dtype and reshape\n  out = bits.bitcast(dtype)[:num_].sub(1).reshape(shape)\n\n  # move back to the original device if we were using MOCKGPU\n  if getenv(\"MOCKGPU\") and _device: out = out.to(_device)\n\n  out.requires_grad = kwargs.get(\"requires_grad\")\n  return out.contiguous() if contiguous else out\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randn","title":"randn  <code>staticmethod</code>","text":"<pre><code>randn(\n    *shape, dtype: Optional[DTypeLike] = None, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with mean <code>0</code> and standard deviation <code>1</code>. If <code>dtype</code> is not specified, the default type is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.randn(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randn(*shape, dtype:Optional[DTypeLike]=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with mean `0` and standard deviation `1`.\n  If `dtype` is not specified, the default type is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.randn(2, 3).numpy())\n  ```\n  \"\"\"\n  # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform\n  src = Tensor.rand((2, *argfix(*shape)), **{**kwargs, \"dtype\": dtypes.float32})\n  return src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(dtype or dtypes.default_float)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randint","title":"randint  <code>staticmethod</code>","text":"<pre><code>randint(*shape, low=0, high=10, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random integer values generated uniformly from the interval <code>[low, high)</code>. If <code>dtype</code> is not specified, the default type is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.randint(2, 3, low=5, high=10).numpy())\n</code></pre> <pre><code>[[9 7 6]\n [8 9 9]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randint(*shape, low=0, high=10, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random integer values generated uniformly from the interval `[low, high)`.\n  If `dtype` is not specified, the default type is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.randint(2, 3, low=5, high=10).numpy())\n  ```\n  \"\"\"\n  if not isinstance(low, int) or not isinstance(high, int): raise TypeError(f\"{low=} and {high=} must be integers\")\n  dtype = to_dtype(kwargs.pop(\"dtype\", dtypes.int32))\n  if not dtypes.is_int(dtype): raise TypeError(f\"{dtype=} must be int\")\n  return Tensor.uniform(*shape, low=low, high=high, dtype=dtype, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.normal","title":"normal  <code>staticmethod</code>","text":"<pre><code>normal(*shape, mean=0.0, std=1.0, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with the given <code>mean</code> and standard deviation <code>std</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.normal(2, 3, mean=10, std=2).numpy())\n</code></pre> <pre><code>[[11.9557 10.9356 11.1053]\n [ 9.3423  8.289  10.5505]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef normal(*shape, mean=0.0, std=1.0, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with the given `mean` and standard deviation `std`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.normal(2, 3, mean=10, std=2).numpy())\n  ```\n  \"\"\"\n  return (std * Tensor.randn(*shape, **kwargs)) + mean\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.uniform","title":"uniform  <code>staticmethod</code>","text":"<pre><code>uniform(*shape, low=0.0, high=1.0, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[low, high)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.uniform(2, 3, low=2, high=10).numpy())\n</code></pre> <pre><code>[[9.9763 6.7193 3.7804]\n [8.0404 9.2452 8.9191]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef uniform(*shape, low=0.0, high=1.0, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[low, high)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.uniform(2, 3, low=2, high=10).numpy())\n  ```\n  \"\"\"\n  dtype = kwargs.pop(\"dtype\", dtypes.default_float)\n  return ((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype) + low\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.scaled_uniform","title":"scaled_uniform  <code>staticmethod</code>","text":"<pre><code>scaled_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[-prod(shape)**-0.5, prod(shape)**-0.5)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.scaled_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.4058  0.0734 -0.2265]\n [ 0.2082  0.3312  0.2979]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef scaled_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution\n  over the interval `[-prod(shape)**-0.5, prod(shape)**-0.5)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.scaled_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul(prod(argfix(*shape))**-0.5)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.glorot_uniform","title":"glorot_uniform  <code>staticmethod</code>","text":"<pre><code>glorot_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.glorot_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 1.0889  0.197  -0.6079]\n [ 0.5588  0.8887  0.7994]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef glorot_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.glorot_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul((6/(argfix(*shape)[0]+prod(argfix(*shape)[1:])))**0.5)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.kaiming_uniform","title":"kaiming_uniform  <code>staticmethod</code>","text":"<pre><code>kaiming_uniform(\n    *shape, a: float = 0.01, **kwargs\n) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.kaiming_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 1.4058  0.2543 -0.7847]\n [ 0.7214  1.1473  1.032 ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_uniform(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.kaiming_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.kaiming_normal","title":"kaiming_normal  <code>staticmethod</code>","text":"<pre><code>kaiming_normal(*shape, a: float = 0.01, **kwargs) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.kaiming_normal(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.7984  0.3819  0.4512]\n [-0.2685 -0.6985  0.2247]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_normal(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.kaiming_normal(2, 3).numpy())\n  ```\n  \"\"\"\n  std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)\n</code></pre>"},{"location":"tensor/elementwise/","title":"Elementwise","text":"<p>Elementwise ops operate on a per element basis. They don't change the shape of the tensor.</p>"},{"location":"tensor/elementwise/#unary-ops-math","title":"Unary Ops (math)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.logical_not","title":"logical_not","text":"<pre><code>logical_not()\n</code></pre> <p>Computes the logical NOT of the tensor element-wise.</p> <pre><code>print(Tensor([False, True]).logical_not().numpy())\n</code></pre> <pre><code>[ True False]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logical_not(self):\n  \"\"\"\n  Computes the logical NOT of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([False, True]).logical_not().numpy())\n  ```\n  \"\"\"\n  return F.Neq.apply(*self.cast(dtypes.bool)._broadcasted(True))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.neg","title":"neg","text":"<pre><code>neg()\n</code></pre> <p>Negates the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).neg().numpy())\n</code></pre> <pre><code>[ 3.  2.  1. -0. -1. -2. -3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def neg(self):\n  \"\"\"\n  Negates the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).neg().numpy())\n  ```\n  \"\"\"\n  return self*-1 if self.dtype != dtypes.bool else self.logical_not()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.log","title":"log","text":"<pre><code>log()\n</code></pre> <p>Computes the natural logarithm element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Logarithm</p> <pre><code>print(Tensor([1., 2., 4., 8.]).log().numpy())\n</code></pre> <pre><code>[0.     0.6931 1.3863 2.0794]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log(self):\n  \"\"\"\n  Computes the natural logarithm element-wise.\n\n  See: https://en.wikipedia.org/wiki/Logarithm\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 4., 8.]).log().numpy())\n  ```\n  \"\"\"\n  return F.Log.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.log2","title":"log2","text":"<pre><code>log2()\n</code></pre> <p>Computes the base-2 logarithm element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Logarithm</p> <pre><code>print(Tensor([1., 2., 4., 8.]).log2().numpy())\n</code></pre> <pre><code>[0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log2(self):\n  \"\"\"\n  Computes the base-2 logarithm element-wise.\n\n  See: https://en.wikipedia.org/wiki/Logarithm\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 4., 8.]).log2().numpy())\n  ```\n  \"\"\"\n  return self.log()/math.log(2)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.exp","title":"exp","text":"<pre><code>exp()\n</code></pre> <p>Computes the exponential function element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Exponential_function</p> <pre><code>print(Tensor([0., 1., 2., 3.]).exp().numpy())\n</code></pre> <pre><code>[ 1.      2.7183  7.3891 20.0855]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp(self):\n  \"\"\"\n  Computes the exponential function element-wise.\n\n  See: https://en.wikipedia.org/wiki/Exponential_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., 1., 2., 3.]).exp().numpy())\n  ```\n  \"\"\"\n  return F.Exp.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.exp2","title":"exp2","text":"<pre><code>exp2()\n</code></pre> <p>Computes the base-2 exponential function element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Exponential_function</p> <pre><code>print(Tensor([0., 1., 2., 3.]).exp2().numpy())\n</code></pre> <pre><code>[1. 2. 4. 8.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp2(self):\n  \"\"\"\n  Computes the base-2 exponential function element-wise.\n\n  See: https://en.wikipedia.org/wiki/Exponential_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., 1., 2., 3.]).exp2().numpy())\n  ```\n  \"\"\"\n  return F.Exp.apply(self*math.log(2))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sqrt","title":"sqrt","text":"<pre><code>sqrt()\n</code></pre> <p>Computes the square root of the tensor element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).sqrt().numpy())\n</code></pre> <pre><code>[1.     1.4142 1.7321 2.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sqrt(self):\n  \"\"\"\n  Computes the square root of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).sqrt().numpy())\n  ```\n  \"\"\"\n  return F.Sqrt.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.rsqrt","title":"rsqrt","text":"<pre><code>rsqrt()\n</code></pre> <p>Computes the reciprocal of the square root of the tensor element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).rsqrt().numpy())\n</code></pre> <pre><code>[1.     0.7071 0.5774 0.5   ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rsqrt(self):\n  \"\"\"\n  Computes the reciprocal of the square root of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).rsqrt().numpy())\n  ```\n  \"\"\"\n  return self.reciprocal().sqrt()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sin","title":"sin","text":"<pre><code>sin()\n</code></pre> <p>Computes the sine of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).sin().numpy())\n</code></pre> <pre><code>[ 0.  1. -0. -1.  0.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sin(self):\n  \"\"\"\n  Computes the sine of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).sin().numpy())\n  ```\n  \"\"\"\n  return F.Sin.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.cos","title":"cos","text":"<pre><code>cos()\n</code></pre> <p>Computes the cosine of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).cos().numpy())\n</code></pre> <pre><code>[ 1.0000e+00  0.0000e+00 -1.0000e+00 -2.3842e-07  1.0000e+00]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cos(self):\n  \"\"\"\n  Computes the cosine of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).cos().numpy())\n  ```\n  \"\"\"\n  return ((math.pi/2)-self).sin()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.tan","title":"tan","text":"<pre><code>tan()\n</code></pre> <p>Computes the tangent of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/4, math.pi/2, 3*math.pi/4, math.pi]).tan().numpy())\n</code></pre> <pre><code>[ 0.  1. inf -1.  0.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tan(self):\n  \"\"\"\n  Computes the tangent of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/4, math.pi/2, 3*math.pi/4, math.pi]).tan().numpy())\n  ```\n  \"\"\"\n  return self.sin() / self.cos()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.trunc","title":"trunc","text":"<pre><code>trunc() -&gt; Tensor\n</code></pre> <p>Truncates the tensor element-wise.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).trunc().numpy())\n</code></pre> <pre><code>[-3. -2. -1.  0.  0.  1.  2.  3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def trunc(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Truncates the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).trunc().numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.int32).cast(self.dtype)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.ceil","title":"ceil","text":"<pre><code>ceil() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise towards positive infinity.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).ceil().numpy())\n</code></pre> <pre><code>[-3. -2. -1.  0.  1.  2.  3.  4.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def ceil(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise towards positive infinity.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).ceil().numpy())\n  ```\n  \"\"\"\n  return (self &gt; (b := self.trunc())).where(b+1, b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.floor","title":"floor","text":"<pre><code>floor() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise towards negative infinity.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).floor().numpy())\n</code></pre> <pre><code>[-4. -3. -2. -1.  0.  1.  2.  3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def floor(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise towards negative infinity.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).floor().numpy())\n  ```\n  \"\"\"\n  return (self &lt; (b := self.trunc())).where(b-1, b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.round","title":"round","text":"<pre><code>round() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise with rounding half to even.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).round().numpy())\n</code></pre> <pre><code>[-4. -2. -2.  0.  0.  2.  2.  4.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def round(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise with rounding half to even.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).round().numpy())\n  ```\n  \"\"\"\n  return ((self &gt; 0) == ((b := self.cast(dtypes.int32) / 2.0).cast(dtypes.int32) == b)).where((self - 0.5).ceil(), (self + 0.5).floor())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.isinf","title":"isinf","text":"<pre><code>isinf(\n    detect_positive: bool = True,\n    detect_negative: bool = True,\n)\n</code></pre> <p>Checks the tensor element-wise to return True where the element is infinity, otherwise returns False</p> <pre><code>print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isinf().numpy())\n</code></pre> <pre><code>[False  True False  True False]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isinf(self:Tensor, detect_positive:bool=True, detect_negative:bool=True):\n  \"\"\"\n  Checks the tensor element-wise to return True where the element is infinity, otherwise returns False\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isinf().numpy())\n  ```\n  \"\"\"\n  return (self == float(\"inf\")) * detect_positive + (self == float(\"-inf\")) * detect_negative\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.isnan","title":"isnan","text":"<pre><code>isnan()\n</code></pre> <p>Checks the tensor element-wise to return True where the element is NaN, otherwise returns False</p> <pre><code>print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isnan().numpy())\n</code></pre> <pre><code>[False False False False  True]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isnan(self:Tensor):\n  \"\"\"\n  Checks the tensor element-wise to return True where the element is NaN, otherwise returns False\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isnan().numpy())\n  ```\n  \"\"\"\n  return self != self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.lerp","title":"lerp","text":"<pre><code>lerp(end: Tensor, weight: Union[Tensor, float]) -&gt; Tensor\n</code></pre> <p>Linearly interpolates between <code>self</code> and <code>end</code> by <code>weight</code>.</p> <pre><code>print(Tensor([1., 2., 3.]).lerp(Tensor([4., 5., 6.]), 0.5).numpy())\n</code></pre> <pre><code>[2.5 3.5 4.5]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def lerp(self, end: Tensor, weight: Union[Tensor, float]) -&gt; Tensor:\n  \"\"\"\n  Linearly interpolates between `self` and `end` by `weight`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3.]).lerp(Tensor([4., 5., 6.]), 0.5).numpy())\n  ```\n  \"\"\"\n  if self.dtype == dtypes.uint8 and isinstance(weight, Tensor):\n    w_i = (weight * (1&lt;&lt;(W_PREC:=7)) + 0.5).cast(dtypes.int16)\n    return (self+(((end - self).cast(dtypes.int8) * w_i + (1&lt;&lt;W_PREC-1)).cast(dtypes.uint16) &gt;&gt; W_PREC)).cast(dtypes.uint8)\n  return self + (end - self) * weight\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.square","title":"square","text":"<pre><code>square()\n</code></pre> <p>Squares the tensor element-wise. Equivalent to <code>self*self</code>.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).square().numpy())\n</code></pre> <pre><code>[9. 4. 1. 0. 1. 4. 9.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def square(self):\n  \"\"\"\n  Squares the tensor element-wise.\n  Equivalent to `self*self`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).square().numpy())\n  ```\n  \"\"\"\n  return self*self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.clamp","title":"clamp","text":"<pre><code>clamp(min_=None, max_=None)\n</code></pre> <p>Clips (clamps) the values in the tensor between <code>min_</code> and <code>max_</code> element-wise. If <code>min_</code> is <code>None</code>, there is no lower bound. If <code>max_</code> is None, there is no upper bound.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).clip(-1, 1).numpy())\n</code></pre> <pre><code>[-1. -1. -1.  0.  1.  1.  1.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clamp(self, min_=None, max_=None):\n  \"\"\"\n  Clips (clamps) the values in the tensor between `min_` and `max_` element-wise.\n  If `min_` is `None`, there is no lower bound. If `max_` is None, there is no upper bound.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).clip(-1, 1).numpy())\n  ```\n  \"\"\"\n  if min_ is None and max_ is None: raise RuntimeError(\"at least one of 'min_' or 'max_' must not be None\")\n  ret = self.maximum(min_) if min_ is not None else self\n  return ret.minimum(max_) if max_ is not None else ret\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.clip","title":"clip","text":"<pre><code>clip(min_=None, max_=None)\n</code></pre> <p>Alias for <code>Tensor.clamp</code>.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clip(self, min_=None, max_=None):\n  \"\"\"\n  Alias for `Tensor.clamp`.\n  \"\"\"\n  return self.clamp(min_, max_)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sign","title":"sign","text":"<pre><code>sign()\n</code></pre> <p>Returns the sign of the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sign().numpy())\n</code></pre> <pre><code>[-1. -1. -1.  0.  1.  1.  1.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sign(self):\n  \"\"\"\n  Returns the sign of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sign().numpy())\n  ```\n  \"\"\"\n  return F.Sign.apply(self)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.abs","title":"abs","text":"<pre><code>abs()\n</code></pre> <p>Computes the absolute value of the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).abs().numpy())\n</code></pre> <pre><code>[3. 2. 1. 0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def abs(self):\n  \"\"\"\n  Computes the absolute value of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).abs().numpy())\n  ```\n  \"\"\"\n  return self * self.sign()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.reciprocal","title":"reciprocal","text":"<pre><code>reciprocal()\n</code></pre> <p>Compute <code>1/x</code> element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).reciprocal().numpy())\n</code></pre> <pre><code>[1.     0.5    0.3333 0.25  ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reciprocal(self):\n  \"\"\"\n  Compute `1/x` element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).reciprocal().numpy())\n  ```\n  \"\"\"\n  return F.Reciprocal.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#unary-ops-activation","title":"Unary Ops (activation)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.relu","title":"relu","text":"<pre><code>relu()\n</code></pre> <p>Applies the Rectified Linear Unit (ReLU) function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/relu</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).relu().numpy())\n</code></pre> <pre><code>[0. 0. 0. 0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu(self):\n  \"\"\"\n  Applies the Rectified Linear Unit (ReLU) function element-wise.\n\n  - Described: https://paperswithcode.com/method/relu\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).relu().numpy())\n  ```\n  \"\"\"\n  return F.Relu.apply(self)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sigmoid","title":"sigmoid","text":"<pre><code>sigmoid()\n</code></pre> <p>Applies the Sigmoid function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Sigmoid_function</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sigmoid().numpy())\n</code></pre> <pre><code>[0.0474 0.1192 0.2689 0.5    0.7311 0.8808 0.9526]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sigmoid(self):\n  \"\"\"\n  Applies the Sigmoid function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Sigmoid_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sigmoid().numpy())\n  ```\n  \"\"\"\n  return F.Sigmoid.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardsigmoid","title":"hardsigmoid","text":"<pre><code>hardsigmoid(alpha: float = 1 / 6, beta: float = 0.5)\n</code></pre> <p>Applies the Hardsigmoid function element-wise. NOTE: default <code>alpha</code> and <code>beta</code> values is taken from torch</p> <ul> <li>Described: https://paperswithcode.com/method/hard-sigmoid</li> <li>See: https://pytorch.org/docs/stable/generated/torch.nn.functional.hardsigmoid.html</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardsigmoid().numpy())\n</code></pre> <pre><code>[0.     0.1667 0.3333 0.5    0.6667 0.8333 1.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardsigmoid(self, alpha:float=1/6, beta:float=0.5):\n  \"\"\"\n  Applies the Hardsigmoid function element-wise.\n  NOTE: default `alpha` and `beta` values is taken from torch\n\n  - Described: https://paperswithcode.com/method/hard-sigmoid\n  - See: https://pytorch.org/docs/stable/generated/torch.nn.functional.hardsigmoid.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardsigmoid().numpy())\n  ```\n  \"\"\"\n  return (alpha * self + beta).relu() - (alpha * self + beta - 1).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.elu","title":"elu","text":"<pre><code>elu(alpha=1.0)\n</code></pre> <p>Applies the Exponential Linear Unit (ELU) function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/elu</li> <li>Paper: https://arxiv.org/abs/1511.07289v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).elu().numpy())\n</code></pre> <pre><code>[-0.9502 -0.8647 -0.6321  0.      1.      2.      3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def elu(self, alpha=1.0):\n  \"\"\"\n  Applies the Exponential Linear Unit (ELU) function element-wise.\n\n  - Described: https://paperswithcode.com/method/elu\n  - Paper: https://arxiv.org/abs/1511.07289v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).elu().numpy())\n  ```\n  \"\"\"\n  return self.relu() - alpha*(1-self.exp()).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.celu","title":"celu","text":"<pre><code>celu(alpha=1.0)\n</code></pre> <p>Applies the Continuously differentiable Exponential Linear Unit (CELU) function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/celu</li> <li>Paper: https://arxiv.org/abs/1704.07483</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).celu().numpy())\n</code></pre> <pre><code>[-0.9502 -0.8647 -0.6321  0.      1.      2.      3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def celu(self, alpha=1.0):\n  \"\"\"\n  Applies the Continuously differentiable Exponential Linear Unit (CELU) function element-wise.\n\n  - Described: https://paperswithcode.com/method/celu\n  - Paper: https://arxiv.org/abs/1704.07483\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).celu().numpy())\n  ```\n  \"\"\"\n  return self.maximum(0) + (alpha * ((self / alpha).exp() - 1)).minimum(0)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.swish","title":"swish","text":"<pre><code>swish()\n</code></pre> <p>See <code>.silu()</code></p> <ul> <li>Paper: https://arxiv.org/abs/1710.05941v1</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).swish().numpy())\n</code></pre> <pre><code>[-0.1423 -0.2384 -0.2689  0.      0.7311  1.7616  2.8577]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def swish(self):\n  \"\"\"\n  See `.silu()`\n\n  - Paper: https://arxiv.org/abs/1710.05941v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).swish().numpy())\n  ```\n  \"\"\"\n  return self * self.sigmoid()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.silu","title":"silu","text":"<pre><code>silu()\n</code></pre> <p>Applies the Sigmoid Linear Unit (SiLU) function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/silu</li> <li>Paper: https://arxiv.org/abs/1606.08415</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).silu().numpy())\n</code></pre> <pre><code>[-0.1423 -0.2384 -0.2689  0.      0.7311  1.7616  2.8577]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def silu(self):\n  \"\"\"\n  Applies the Sigmoid Linear Unit (SiLU) function element-wise.\n\n  - Described: https://paperswithcode.com/method/silu\n  - Paper: https://arxiv.org/abs/1606.08415\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).silu().numpy())\n  ```\n  \"\"\"\n  return self.swish()   # The SiLU function is also known as the swish function.\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.relu6","title":"relu6","text":"<pre><code>relu6()\n</code></pre> <p>Applies the ReLU6 function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/relu6</li> <li>Paper: https://arxiv.org/abs/1704.04861v1</li> </ul> <pre><code>print(Tensor([-9., -6., -3., 0., 3., 6., 9.]).relu6().numpy())\n</code></pre> <pre><code>[0. 0. 0. 0. 3. 6. 6.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu6(self):\n  \"\"\"\n  Applies the ReLU6 function element-wise.\n\n  - Described: https://paperswithcode.com/method/relu6\n  - Paper: https://arxiv.org/abs/1704.04861v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-9., -6., -3., 0., 3., 6., 9.]).relu6().numpy())\n  ```\n  \"\"\"\n  return self.relu() - (self-6).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardswish","title":"hardswish","text":"<pre><code>hardswish()\n</code></pre> <p>Applies the Hardswish function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/hard-swish</li> <li>Paper: https://arxiv.org/abs/1905.02244v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardswish().numpy())\n</code></pre> <pre><code>[-0.     -0.3333 -0.3333  0.      0.6667  1.6667  3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardswish(self):\n  \"\"\"\n  Applies the Hardswish function element-wise.\n\n  - Described: https://paperswithcode.com/method/hard-swish\n  - Paper: https://arxiv.org/abs/1905.02244v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardswish().numpy())\n  ```\n  \"\"\"\n  return self * (self+3).relu6() * (1/6)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.tanh","title":"tanh","text":"<pre><code>tanh()\n</code></pre> <p>Applies the Hyperbolic Tangent (tanh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).tanh().numpy())\n</code></pre> <pre><code>[-0.9951 -0.964  -0.7616  0.      0.7616  0.964   0.9951]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tanh(self):\n  \"\"\"\n  Applies the Hyperbolic Tangent (tanh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).tanh().numpy())\n  ```\n  \"\"\"\n  return 2.0 * ((2.0 * self).sigmoid()) - 1.0\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sinh","title":"sinh","text":"<pre><code>sinh()\n</code></pre> <p>Applies the Hyperbolic Sine (sinh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Sinh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sinh().numpy())\n</code></pre> <pre><code>[-10.0179  -3.6269  -1.1752   0.       1.1752   3.6269  10.0179]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sinh(self):\n  \"\"\"\n  Applies the Hyperbolic Sine (sinh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Sinh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sinh().numpy())\n  ```\n  \"\"\"\n  return (self.exp() - self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.cosh","title":"cosh","text":"<pre><code>cosh()\n</code></pre> <p>Applies the Hyperbolic Cosine (cosh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Cosh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).cosh().numpy())\n</code></pre> <pre><code>[10.0677  3.7622  1.5431  1.      1.5431  3.7622 10.0677]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cosh(self):\n  \"\"\"\n  Applies the Hyperbolic Cosine (cosh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Cosh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).cosh().numpy())\n  ```\n  \"\"\"\n  return (self.exp() + self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.atanh","title":"atanh","text":"<pre><code>atanh()\n</code></pre> <p>Applies the Inverse Hyperbolic Tangent (atanh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#atanh</li> </ul> <pre><code>print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).atanh().numpy())\n</code></pre> <pre><code>[-1.4722 -0.6931 -0.3095  0.      0.3095  0.6931  1.4722]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def atanh(self):\n  \"\"\"\n  Applies the Inverse Hyperbolic Tangent (atanh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#atanh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).atanh().numpy())\n  ```\n  \"\"\"\n  return ((1 + self)/(1 - self)).log() / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.asinh","title":"asinh","text":"<pre><code>asinh()\n</code></pre> <p>Applies the Inverse Hyperbolic Sine (asinh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#asinh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).asinh().numpy())\n</code></pre> <pre><code>[-1.8184 -1.4436 -0.8814  0.      0.8814  1.4436  1.8184]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def asinh(self):\n  \"\"\"\n  Applies the Inverse Hyperbolic Sine (asinh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#asinh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).asinh().numpy())\n  ```\n  \"\"\"\n  return (self + (self.square() + 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.acosh","title":"acosh","text":"<pre><code>acosh()\n</code></pre> <p>Applies the Inverse Hyperbolic Cosine (acosh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#acosh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).acosh().numpy())\n</code></pre> <pre><code>[   nan    nan    nan    nan 0.     1.317  1.7627]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def acosh(self):\n  \"\"\"\n  Applies the Inverse Hyperbolic Cosine (acosh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#acosh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).acosh().numpy())\n  ```\n  \"\"\"\n  return (self + (self.square() - 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardtanh","title":"hardtanh","text":"<pre><code>hardtanh(min_val=-1, max_val=1)\n</code></pre> <p>Applies the Hardtanh function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/hardtanh-activation</li> </ul> <pre><code>print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).hardtanh().numpy())\n</code></pre> <pre><code>[-1.  -1.  -0.5  0.   0.5  1.   1. ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardtanh(self, min_val=-1, max_val=1):\n  \"\"\"\n  Applies the Hardtanh function element-wise.\n\n  - Described: https://paperswithcode.com/method/hardtanh-activation\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).hardtanh().numpy())\n  ```\n  \"\"\"\n  return self.clip(min_val, max_val)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.erf","title":"erf","text":"<pre><code>erf()\n</code></pre> <p>Applies error function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Error_function</li> </ul> <pre><code>print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).erf().numpy())\n</code></pre> <pre><code>[-0.9661 -0.8427 -0.5205  0.      0.5205  0.8427  0.9661]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def erf(self):\n  \"\"\"\n  Applies error function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Error_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).erf().numpy())\n  ```\n  \"\"\"\n  # https://personal.math.ubc.ca/~cbm/aands/page_299.htm 7.1.26\n  t = 1.0 / (1.0 + 0.3275911 * self.abs())\n  return self.sign() * (1.0 - t * polyN(t, [1.061405429, -1.453152027, 1.421413741, -0.284496736, 0.254829592]) * (-self.square()).exp())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.gelu","title":"gelu","text":"<pre><code>gelu()\n</code></pre> <p>Applies the Gaussian Error Linear Unit (GELU) function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/gelu</li> <li>Paper: https://arxiv.org/abs/1606.08415v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).gelu().numpy())\n</code></pre> <pre><code>[-0.0036 -0.0454 -0.1588  0.      0.8412  1.9546  2.9964]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gelu(self):\n  \"\"\"\n  Applies the Gaussian Error Linear Unit (GELU) function element-wise.\n\n  - Described: https://paperswithcode.com/method/gelu\n  - Paper: https://arxiv.org/abs/1606.08415v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).gelu().numpy())\n  ```\n  \"\"\"\n  return 0.5 * self * (1 + (math.sqrt(2 / math.pi) * (self + 0.044715 * self ** 3)).tanh())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.quick_gelu","title":"quick_gelu","text":"<pre><code>quick_gelu()\n</code></pre> <p>Applies the Sigmoid GELU approximation element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/gelu</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).quick_gelu().numpy())\n</code></pre> <pre><code>[-0.0181 -0.0643 -0.1542  0.      0.8458  1.9357  2.9819]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def quick_gelu(self):\n  \"\"\"\n  Applies the Sigmoid GELU approximation element-wise.\n\n  - Described: https://paperswithcode.com/method/gelu\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).quick_gelu().numpy())\n  ```\n  \"\"\"\n  return self * (self * 1.702).sigmoid()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.leakyrelu","title":"leakyrelu","text":"<pre><code>leakyrelu(neg_slope=0.01)\n</code></pre> <p>Applies the Leaky ReLU function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/leaky-relu</li> </ul> <p><pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leakyrelu().numpy())\n</code></pre> <pre><code>[-0.03 -0.02 -0.01  0.    1.    2.    3.  ]\n</code></pre> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leakyrelu(neg_slope=0.42).numpy())\n</code></pre> <pre><code>[-1.26 -0.84 -0.42  0.    1.    2.    3.  ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def leakyrelu(self, neg_slope=0.01):\n  \"\"\"\n  Applies the Leaky ReLU function element-wise.\n\n  - Described: https://paperswithcode.com/method/leaky-relu\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leakyrelu().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leakyrelu(neg_slope=0.42).numpy())\n  ```\n  \"\"\"\n  return self.relu() - (-neg_slope*self).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.mish","title":"mish","text":"<pre><code>mish()\n</code></pre> <p>Applies the Mish function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/mish</li> <li>Paper: https://arxiv.org/abs/1908.08681v3</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).mish().numpy())\n</code></pre> <pre><code>[-0.1456 -0.2525 -0.3034  0.      0.8651  1.944   2.9865]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mish(self):\n  \"\"\"\n  Applies the Mish function element-wise.\n\n  - Described: https://paperswithcode.com/method/mish\n  - Paper: https://arxiv.org/abs/1908.08681v3\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).mish().numpy())\n  ```\n  \"\"\"\n  return self * self.softplus().tanh()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.softplus","title":"softplus","text":"<pre><code>softplus(beta=1)\n</code></pre> <p>Applies the Softplus function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/softplus</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softplus().numpy())\n</code></pre> <pre><code>[0.0486 0.1269 0.3133 0.6931 1.3133 2.1269 3.0486]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softplus(self, beta=1):\n  \"\"\"\n  Applies the Softplus function element-wise.\n\n  - Described: https://paperswithcode.com/method/softplus\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softplus().numpy())\n  ```\n  \"\"\"\n  return (1/beta) * (1 + (self*beta).exp()).log()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.softsign","title":"softsign","text":"<pre><code>softsign()\n</code></pre> <p>Applies the Softsign function element-wise.</p> <ul> <li>Described: https://paperswithcode.com/method/softsign</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softsign().numpy())\n</code></pre> <pre><code>[-0.75   -0.6667 -0.5     0.      0.5     0.6667  0.75  ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softsign(self):\n  \"\"\"\n  Applies the Softsign function element-wise.\n\n  - Described: https://paperswithcode.com/method/softsign\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softsign().numpy())\n  ```\n  \"\"\"\n  return self / (1 + self.abs())\n</code></pre>"},{"location":"tensor/elementwise/#elementwise-ops-broadcasted","title":"Elementwise Ops (broadcasted)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.add","title":"add","text":"<pre><code>add(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Adds <code>self</code> and <code>x</code>. Equivalent to <code>self + x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.add(20).numpy())\n</code></pre> <pre><code>[19.4856 21.085  20.9089 19.9159]\n</code></pre> <pre><code>print(t.add(Tensor([[2.0], [3.5]])).numpy())\n</code></pre> <pre><code>[[1.4856 3.085  2.9089 1.9159]\n [2.9856 4.585  4.4089 3.4159]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def add(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Adds `self` and `x`.\n  Equivalent to `self + x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.add(20).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.add(Tensor([[2.0], [3.5]])).numpy())\n  ```\n  \"\"\"\n  return F.Add.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sub","title":"sub","text":"<pre><code>sub(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Subtracts <code>x</code> from <code>self</code>. Equivalent to <code>self - x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.sub(20).numpy())\n</code></pre> <pre><code>[-20.5144 -18.915  -19.0911 -20.0841]\n</code></pre> <pre><code>print(t.sub(Tensor([[2.0], [3.5]])).numpy())\n</code></pre> <pre><code>[[-2.5144 -0.915  -1.0911 -2.0841]\n [-4.0144 -2.415  -2.5911 -3.5841]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sub(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Subtracts `x` from `self`.\n  Equivalent to `self - x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sub(20).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sub(Tensor([[2.0], [3.5]])).numpy())\n  ```\n  \"\"\"\n  a, b = self._broadcasted(x, reverse)\n  return a + (-b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.mul","title":"mul","text":"<pre><code>mul(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Multiplies <code>self</code> and <code>x</code>. Equivalent to <code>self * x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.mul(3).numpy())\n</code></pre> <pre><code>[-1.5431  3.2549  2.7267 -0.2523]\n</code></pre> <pre><code>print(t.mul(Tensor([[-1.0], [2.0]])).numpy())\n</code></pre> <pre><code>[[ 0.5144 -1.085  -0.9089  0.0841]\n [-1.0287  2.17    1.8178 -0.1682]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mul(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Multiplies `self` and `x`.\n  Equivalent to `self * x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mul(3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mul(Tensor([[-1.0], [2.0]])).numpy())\n  ```\n  \"\"\"\n  return F.Mul.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.div","title":"div","text":"<pre><code>div(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Divides <code>self</code> by <code>x</code>. Equivalent to <code>self / x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs. <code>div</code> performs true division.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.div(3).numpy())\n</code></pre> <pre><code>[-0.1715  0.3617  0.303  -0.028 ]\n</code></pre> <pre><code>print(Tensor([1, 4, 10]).div(Tensor([2, 3, 4])).numpy())\n</code></pre> <pre><code>[0.5    1.3333 2.5   ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def div(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Divides `self` by `x`.\n  Equivalent to `self / x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n  `div` performs true division.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.div(3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, 4, 10]).div(Tensor([2, 3, 4])).numpy())\n  ```\n  \"\"\"\n  numerator, denominator = self._broadcasted(x, reverse)\n  return numerator.cast(least_upper_float(numerator.dtype)) * denominator.cast(least_upper_float(denominator.dtype)).reciprocal()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.xor","title":"xor","text":"<pre><code>xor(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Computes bitwise xor of <code>self</code> and <code>x</code>. Equivalent to <code>self ^ x</code>. Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.</p> <p><pre><code>print(Tensor([-1, -2, 3]).xor(Tensor([1, 0, 3])).numpy())\n</code></pre> <pre><code>[-2 -2  0]\n</code></pre> <pre><code>print(Tensor([True, True, False, False]).xor(Tensor([True, False, True, False])).numpy())\n</code></pre> <pre><code>[False  True  True False]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def xor(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Computes bitwise xor of `self` and `x`.\n  Equivalent to `self ^ x`.\n  Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, -2, 3]).xor(Tensor([1, 0, 3])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([True, True, False, False]).xor(Tensor([True, False, True, False])).numpy())\n  ```\n  \"\"\"\n  return F.Xor.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.lshift","title":"lshift","text":"<pre><code>lshift(x: int)\n</code></pre> <p>Computes left arithmetic shift of <code>self</code> by <code>x</code> bits. <code>self</code> must have unsigned dtype. Equivalent to <code>self &lt;&lt; x</code>.</p> <pre><code>print(Tensor([1, 3, 31], dtype=dtypes.uint8).lshift(2).numpy())\n</code></pre> <pre><code>[  4  12 124]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def lshift(self, x:int):\n  \"\"\"\n  Computes left arithmetic shift of `self` by `x` bits. `self` must have unsigned dtype.\n  Equivalent to `self &lt;&lt; x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, 3, 31], dtype=dtypes.uint8).lshift(2).numpy())\n  ```\n  \"\"\"\n  assert dtypes.is_unsigned(self.dtype) and isinstance(x, int) and x &gt;= 0, f\"not supported {self.dtype=} {x=}\"\n  return self.mul(2 ** x)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.rshift","title":"rshift","text":"<pre><code>rshift(x: int)\n</code></pre> <p>Computes right arithmetic shift of <code>self</code> by <code>x</code> bits. <code>self</code> must have unsigned dtype. Equivalent to <code>self &gt;&gt; x</code>.</p> <pre><code>print(Tensor([4, 13, 125], dtype=dtypes.uint8).rshift(2).numpy())\n</code></pre> <pre><code>[ 1  3 31]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rshift(self, x:int):\n  \"\"\"\n  Computes right arithmetic shift of `self` by `x` bits. `self` must have unsigned dtype.\n  Equivalent to `self &gt;&gt; x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([4, 13, 125], dtype=dtypes.uint8).rshift(2).numpy())\n  ```\n  \"\"\"\n  assert dtypes.is_unsigned(self.dtype) and isinstance(x, int) and x &gt;= 0, f\"not supported {self.dtype=} {x=}\"\n  return self.idiv(2 ** x)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.pow","title":"pow","text":"<pre><code>pow(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> <p>Computes power of <code>self</code> with <code>x</code>. Equivalent to <code>self ** x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).pow(2).numpy())\n</code></pre> <pre><code>[1 4 9]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).pow(Tensor([-1.5, 0.5, 1.5])).numpy())\n</code></pre> <pre><code>[   nan 1.4142 5.1962]\n</code></pre> <pre><code>print((2 ** Tensor([-1, 2, 3])).numpy())\n</code></pre> <pre><code>[0.5 4.  8. ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pow(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  \"\"\"\n  Computes power of `self` with `x`.\n  Equivalent to `self ** x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).pow(2).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).pow(Tensor([-1.5, 0.5, 1.5])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print((2 ** Tensor([-1, 2, 3])).numpy())\n  ```\n  \"\"\"\n  x = self._to_const_val(x)\n  if not isinstance(x, Tensor) and not reverse:\n    # simple pow identities\n    if x &lt; 0: return self.reciprocal().pow(-x)\n    if x == 0: return 1 + self * 0\n    if int(x - 0.5) + 0.5 == x: return self.pow(int(x - 0.5)) * self.sqrt()\n    if int(x) == x: return self.pow(x // 2).square() * (1 if x % 2 == 0 else self)\n\n  # positive const ** self\n  if not isinstance(x, Tensor) and reverse and x &gt; 0: return self.mul(math.log(x)).exp()\n\n  base, exponent = self._broadcasted(x, reverse=reverse)\n  # start with b ** e = exp(e * log(b))\n  ret = base.abs().log().mul(exponent).exp()\n  # correct sign of negative base with odd exponent (cos has a period of 2pi so we use it here to get the oddness of the exponent)\n  negative_base = (base &lt; 0).detach().where(1, 0)\n  # 1 for non-negative base or negative even exponent, -1 for negative odd exponent, don't care about non-integer exponent\n  correct_sign = 1 + negative_base * ((exponent * math.pi).cos() - 1)\n  # inject nan for negative base and non-integer exponent\n  inject_nan = (negative_base * (exponent != exponent.trunc())).detach().where(math.nan, 1)\n  # apply correct_sign inject_nan, and fix 0 ** 0 = 1\n  return ((base == 0) * (exponent == 0)).detach().where(1, ret * correct_sign * inject_nan)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.maximum","title":"maximum","text":"<pre><code>maximum(x: Union[Tensor, ConstType]) -&gt; Tensor\n</code></pre> <p>Computes element-wise maximum of <code>self</code> and <code>x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).maximum(1).numpy())\n</code></pre> <pre><code>[1 2 3]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).maximum(Tensor([-4, -2, 9])).numpy())\n</code></pre> <pre><code>[-1  2  9]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def maximum(self, x:Union[Tensor, ConstType]) -&gt; Tensor:\n  \"\"\"\n  Computes element-wise maximum of `self` and `x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).maximum(1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).maximum(Tensor([-4, -2, 9])).numpy())\n  ```\n  \"\"\"\n  return (self&lt;x).detach().where(x, (self==x).detach().where(((self * 0.5 + x * 0.5).cast(self.dtype)), self))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.minimum","title":"minimum","text":"<pre><code>minimum(x: Union[Tensor, ConstType]) -&gt; Tensor\n</code></pre> <p>Computes element-wise minimum of <code>self</code> and <code>x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).minimum(1).numpy())\n</code></pre> <pre><code>[-1  1  1]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).minimum(Tensor([-4, -2, 9])).numpy())\n</code></pre> <pre><code>[-4 -2  3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def minimum(self, x:Union[Tensor, ConstType]) -&gt; Tensor:\n  \"\"\"\n  Computes element-wise minimum of `self` and `x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).minimum(1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).minimum(Tensor([-4, -2, 9])).numpy())\n  ```\n  \"\"\"\n  return -((-self).maximum(-x))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.where","title":"where","text":"<pre><code>where(\n    x: Union[Tensor, ConstType], y: Union[Tensor, ConstType]\n)\n</code></pre> <p>Return a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>self</code>. <code>output_i = x_i if self_i else y_i</code>.</p> <p><pre><code>cond = Tensor([[True, True, False], [True, False, False]])\nprint(cond.where(1, 3).numpy())\n</code></pre> <pre><code>[[1 1 3]\n [1 3 3]]\n</code></pre> <pre><code>Tensor.manual_seed(42)\ncond = Tensor.randn(2, 3)\nprint(cond.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print((cond &gt; 0).where(cond, -float(\"inf\")).numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [  -inf   -inf 0.2753]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def where(self:Tensor, x:Union[Tensor, ConstType], y:Union[Tensor, ConstType]):\n  \"\"\"\n  Return a tensor of elements selected from either `x` or `y`, depending on `self`.\n  `output_i = x_i if self_i else y_i`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  cond = Tensor([[True, True, False], [True, False, False]])\n  print(cond.where(1, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  cond = Tensor.randn(2, 3)\n  print(cond.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print((cond &gt; 0).where(cond, -float(\"inf\")).numpy())\n  ```\n  \"\"\"\n  if isinstance(x, Tensor): x, y = x._broadcasted(y)\n  elif isinstance(y, Tensor): y, x = y._broadcasted(x)\n  cond, x = self._broadcasted(x, match_dtype=False)\n  cond, y = cond._broadcasted(y, match_dtype=False)\n  return F.Where.apply(cond.cast(dtypes.bool), *x._broadcasted(y))\n</code></pre>"},{"location":"tensor/elementwise/#casting-ops","title":"Casting Ops","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.cast","title":"cast","text":"<pre><code>cast(dtype: DTypeLike) -&gt; Tensor\n</code></pre> <p>Casts <code>self</code> to the given <code>dtype</code>.</p> <p><pre><code>t = Tensor([-1, 2.5, 3], dtype=dtypes.float)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.   2.5  3. ]\n</code></pre> <pre><code>t = t.cast(dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cast(self, dtype:DTypeLike) -&gt; Tensor:\n  \"\"\"\n  Casts `self` to the given `dtype`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2.5, 3], dtype=dtypes.float)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.cast(dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self if self.dtype == (dt:=to_dtype(dtype)) else F.Cast.apply(self, dtype=dt)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitcast","title":"bitcast","text":"<pre><code>bitcast(dtype: DTypeLike) -&gt; Tensor\n</code></pre> <p>Bitcasts <code>self</code> to the given <code>dtype</code> of the same itemsize.</p> <p><code>self</code> must not require a gradient.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.bitcast(dtypes.uint32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.uint [4294967295          2          3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bitcast(self, dtype:DTypeLike) -&gt; Tensor:\n  \"\"\"\n  Bitcasts `self` to the given `dtype` of the same itemsize.\n\n  `self` must not require a gradient.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.bitcast(dtypes.uint32)\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  if self.requires_grad: raise RuntimeError(\"can't backprop through bitcast\")\n  dt = to_dtype(dtype)\n  if (not isinstance(self.device, str) or not self.device.startswith(\"DISK\")) and (ns:=dt.itemsize) != (os:=self.dtype.itemsize):\n    if (self.shape[-1]*os) % ns != 0: raise RuntimeError(\"unsupported size in bitcast\")\n    new_uint, old_uint = to_dtype(f\"uint{8*ns}\"), to_dtype(f\"uint{8*os}\")\n    tmp = self.bitcast(old_uint)\n    if ns &gt; os: return functools.reduce(Tensor.add, (tmp[..., i::ns//os].cast(new_uint) &lt;&lt; 8*i*os for i in range(ns//os))).bitcast(dtype)\n    return Tensor.stack(*(tmp&gt;&gt;8*i*ns for i in range(os//ns)), dim=-1).flatten(-2).cast(new_uint).bitcast(dtype)\n  return F.Cast.apply(self, dtype=dt, bitcast=True) if self.dtype != dt else self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.float","title":"float","text":"<pre><code>float() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>float32</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.float()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.  2.  3.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def float(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `float32` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.float()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.float32)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.half","title":"half","text":"<pre><code>half() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>float16</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.half()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.half [-1.  2.  3.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def half(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `float16` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.half()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.float16)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.int","title":"int","text":"<pre><code>int() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>int32</code> Tensor.</p> <p><pre><code>t = Tensor([-1.5, -0.5, 0.0, 0.5, 1.5])\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.5 -0.5  0.   0.5  1.5]\n</code></pre> <pre><code>t = t.int()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  0  0  0  1]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def int(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `int32` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1.5, -0.5, 0.0, 0.5, 1.5])\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.int()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bool","title":"bool","text":"<pre><code>bool() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>bool</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 0, 1])\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  0  1]\n</code></pre> <pre><code>t = t.bool()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.bool [ True False  True]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bool(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `bool` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 0, 1])\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.bool()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.bool)\n</code></pre>"},{"location":"tensor/movement/","title":"Movement","text":""},{"location":"tensor/movement/#movement-low-level","title":"Movement (low level)","text":""},{"location":"tensor/movement/#tinygrad.Tensor.view","title":"view","text":"<pre><code>view(*shape) -&gt; Tensor\n</code></pre> <p><code>.view</code> is an alias for <code>.reshape</code>.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def view(self, *shape) -&gt; Tensor:\n  \"\"\"`.view` is an alias for `.reshape`.\"\"\"\n  return self.reshape(shape)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.reshape","title":"reshape","text":"<pre><code>reshape(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor with the same data as the original tensor but with a different shape. <code>shape</code> can be passed as a tuple or as separate arguments.</p> <pre><code>t = Tensor.arange(6)\nprint(t.reshape(2, 3).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reshape(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with the same data as the original tensor but with a different shape.\n  `shape` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6)\n  print(t.reshape(2, 3).numpy())\n  ```\n  \"\"\"\n  # resolve None and args\n  new_shape = tuple([s if s is not None else self.shape[i] for i,s in enumerate(argfix(shape, *args))])\n  # resolve -1\n  if (c := new_shape.count(-1)) &gt; 1: raise RuntimeError(f\"only one dimension can be inferred using -1, getting {new_shape}\")\n  if c: new_shape = tuple([-prod(self.shape) // prod(new_shape) if s == -1 else s for s in new_shape])\n  return F.Reshape.apply(self, shape=new_shape) if new_shape != self.shape else self\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.expand","title":"expand","text":"<pre><code>expand(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is expanded to the shape that is specified. Expand can also increase the number of dimensions that a tensor has.</p> <p>Passing a <code>-1</code> or <code>None</code> to a dimension means that its size will not be changed.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.expand(4, -1).numpy())\n</code></pre> <pre><code>[[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def expand(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is expanded to the shape that is specified.\n  Expand can also increase the number of dimensions that a tensor has.\n\n  Passing a `-1` or `None` to a dimension means that its size will not be changed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.expand(4, -1).numpy())\n  ```\n  \"\"\"\n  return self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.permute","title":"permute","text":"<pre><code>permute(order, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a permutation of the original tensor. The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified. <code>order</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.permute(1, 0).numpy())\n</code></pre> <pre><code>[[0 3]\n [1 4]\n [2 5]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def permute(self, order, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a permutation of the original tensor.\n  The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified.\n  `order` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.permute(1, 0).numpy())\n  ```\n  \"\"\"\n  order_arg = tuple(self._resolve_dim(x) for x in argfix(order, *args))\n  if sorted(order_arg) != list(range(self.ndim)): raise RuntimeError(f\"order is not a valid permutation, getting {order_arg}\")\n  return F.Permute.apply(self, order=order_arg)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.flip","title":"flip","text":"<pre><code>flip(axis, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that reverses the order of the original tensor along given <code>axis</code>. <code>axis</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.flip(0).numpy())\n</code></pre> <pre><code>[[3 4 5]\n [0 1 2]]\n</code></pre> <pre><code>print(t.flip((0, 1)).numpy())\n</code></pre> <pre><code>[[5 4 3]\n [2 1 0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flip(self, axis, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that reverses the order of the original tensor along given `axis`.\n  `axis` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flip(0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flip((0, 1)).numpy())\n  ```\n  \"\"\"\n  axis_arg = tuple(self._resolve_dim(x) for x in argfix(axis, *args))\n  if len(axis_arg) != len(dedup(axis_arg)): raise RuntimeError(f\"dim can appear at least once, getting {axis_arg}\")\n  return F.Flip.apply(self, axis=axis_arg)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.shrink","title":"shrink","text":"<pre><code>shrink(\n    arg: Tuple[Optional[Tuple[sint, sint]], ...]\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor that shrinks the each axis based on input arg. <code>arg</code> must have the same length as <code>self.ndim</code>. For each axis, it can be <code>None</code>, which means no shrink, or a tuple <code>(start, end)</code> that works the same as Python slice.</p> <p><pre><code>t = Tensor.arange(9).reshape(3, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]\n [6 7 8]]\n</code></pre> <pre><code>print(t.shrink(((None, (1, 3)))).numpy())\n</code></pre> <pre><code>[[1 2]\n [4 5]\n [7 8]]\n</code></pre> <pre><code>print(t.shrink((((0, 2), (0, 2)))).numpy())\n</code></pre> <pre><code>[[0 1]\n [3 4]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shrink(self, arg:Tuple[Optional[Tuple[sint, sint]], ...]) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that shrinks the each axis based on input arg.\n  `arg` must have the same length as `self.ndim`.\n  For each axis, it can be `None`, which means no shrink, or a tuple `(start, end)` that works the same as Python slice.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(3, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.shrink(((None, (1, 3)))).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.shrink((((0, 2), (0, 2)))).numpy())\n  ```\n  \"\"\"\n  if all(x is None or x == (0,s) for x,s in zip(arg, self.shape)): return self\n  return F.Shrink.apply(self, arg=tuple(x if x is not None else (0,s) for x,s in zip(arg, self.shape)))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.pad","title":"pad","text":"<pre><code>pad(\n    arg: Tuple[Optional[Tuple[sint, sint]], ...],\n    value: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor that pads the each axis based on input arg. <code>arg</code> must have the same length as <code>self.ndim</code>. For each axis, it can be <code>None</code>, which means no pad, or a tuple <code>(pad_before, pad_after)</code>. If <code>value</code> is specified, the tensor is padded with <code>value</code> instead of <code>0.0</code>.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.pad(((None, (1, 2)))).numpy())\n</code></pre> <pre><code>[[0 0 1 2 0 0]\n [0 3 4 5 0 0]]\n</code></pre> <pre><code>print(t.pad(((None, (1, 2))), -2).numpy())\n</code></pre> <pre><code>[[-2  0  1  2 -2 -2]\n [-2  3  4  5 -2 -2]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pad(self, arg:Tuple[Optional[Tuple[sint, sint]], ...], value:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that pads the each axis based on input arg.\n  `arg` must have the same length as `self.ndim`.\n  For each axis, it can be `None`, which means no pad, or a tuple `(pad_before, pad_after)`.\n  If `value` is specified, the tensor is padded with `value` instead of `0.0`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad(((None, (1, 2)))).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad(((None, (1, 2))), -2).numpy())\n  ```\n  \"\"\"\n  if all(x is None or x == (0,0) for x in arg): return self\n  ret = F.Pad.apply(self, arg=(narg:=tuple(x if x is not None else (0,0) for x in arg)))\n  return ret if 0 == value else ret + F.Pad.apply(Tensor.ones_like(self), arg=narg).where(0, value)\n</code></pre>"},{"location":"tensor/movement/#movement-high-level","title":"Movement (high level)","text":""},{"location":"tensor/movement/#tinygrad.Tensor.gather","title":"gather","text":"<pre><code>gather(dim: int, index: Tensor) -&gt; Tensor\n</code></pre> <p>Gathers values along an axis specified by <code>dim</code>.</p> <p><pre><code>t = Tensor([[1, 2], [3, 4]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]]\n</code></pre> <pre><code>print(t.gather(1, Tensor([[0, 0], [1, 0]])).numpy())\n</code></pre> <pre><code>[[1 1]\n [4 3]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gather(self:Tensor, dim:int, index:Tensor) -&gt; Tensor:\n  \"\"\"\n  Gathers values along an axis specified by `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2], [3, 4]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.gather(1, Tensor([[0, 0], [1, 0]])).numpy())\n  ```\n  \"\"\"\n  assert index.ndim == self.ndim, f\"self.ndim must equal index.ndim, {self.ndim=}, {index.ndim=}\"\n  dim = self._resolve_dim(dim)\n  assert all(s &gt;= i for d,(s,i) in enumerate(zip(self.shape, index.shape)) if d != dim), \"requires self.shape[d] &gt;= index.shape[d] for all d != dim\"\n  index = index.to(self.device)\n  x = self.shrink(tuple((0, i) if d != dim else None for d,i in enumerate(index.shape))).unsqueeze(-1).transpose(-1, dim)\n  return ((index.unsqueeze(-1) == Tensor.arange(self.shape[dim], requires_grad=False, device=self.device)) * x).sum(-1, acc_dtype=self.dtype)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.cat","title":"cat","text":"<pre><code>cat(*args: Tensor, dim: int = 0) -&gt; Tensor\n</code></pre> <p>Concatenates self with other <code>Tensor</code> in <code>args</code> along an axis specified by <code>dim</code>. All tensors must have the same shape except in the concatenating dimension.</p> <p><pre><code>t0, t1, t2 = Tensor([[1, 2]]), Tensor([[3, 4]]), Tensor([[5, 6]])\nprint(t0.cat(t1, t2, dim=0).numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]\n [5 6]]\n</code></pre> <pre><code>print(t0.cat(t1, t2, dim=1).numpy())\n</code></pre> <pre><code>[[1 2 3 4 5 6]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cat(self:Tensor, *args:Tensor, dim:int=0) -&gt; Tensor:\n  \"\"\"\n  Concatenates self with other `Tensor` in `args` along an axis specified by `dim`.\n  All tensors must have the same shape except in the concatenating dimension.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t0, t1, t2 = Tensor([[1, 2]]), Tensor([[3, 4]]), Tensor([[5, 6]])\n  print(t0.cat(t1, t2, dim=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t0.cat(t1, t2, dim=1).numpy())\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim)\n  assert all(len(y.shape) == len(self.shape) and all(y.shape[i] == s for i,s in enumerate(self.shape) if i != dim) for y in args)\n  catargs = [self, *args]\n  cat_dims = [s.shape[dim] for s in catargs]\n  cat_dim_cumsum = [0, *itertools.accumulate(cat_dims)]\n  slc:List[List[Optional[Tuple[sint, sint]]]] = [[None for _ in self.shape] for _ in catargs]\n  for d,k,s in zip(cat_dims, cat_dim_cumsum[:-1], slc): s[dim] = (k, cat_dim_cumsum[-1] - k - d)\n  return functools.reduce(Tensor.add, [arg.pad(tuple(s)) for arg,s in zip(catargs, slc)])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.stack","title":"stack","text":"<pre><code>stack(*args: Tensor, dim: int = 0) -&gt; Tensor\n</code></pre> <p>Concatenates self with other <code>Tensor</code> in <code>args</code> along a new dimension specified by <code>dim</code>.</p> <p><pre><code>t0, t1, t2 = Tensor([1, 2]), Tensor([3, 4]), Tensor([5, 6])\nprint(t0.stack(t1, t2, dim=0).numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]\n [5 6]]\n</code></pre> <pre><code>print(t0.stack(t1, t2, dim=1).numpy())\n</code></pre> <pre><code>[[1 3 5]\n [2 4 6]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def stack(self:Tensor, *args:Tensor, dim:int=0) -&gt; Tensor:\n  \"\"\"\n  Concatenates self with other `Tensor` in `args` along a new dimension specified by `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t0, t1, t2 = Tensor([1, 2]), Tensor([3, 4]), Tensor([5, 6])\n  print(t0.stack(t1, t2, dim=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t0.stack(t1, t2, dim=1).numpy())\n  ```\n  \"\"\"\n  # checks for shapes and number of dimensions delegated to cat\n  return self.unsqueeze(dim).cat(*[t.unsqueeze(dim) for t in args], dim=dim)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.repeat","title":"repeat","text":"<pre><code>repeat(repeats, *args) -&gt; Tensor\n</code></pre> <p>Repeats tensor number of times along each dimension specified by <code>repeats</code>. <code>repeats</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor([1, 2, 3])\nprint(t.repeat(4, 2).numpy())\n</code></pre> <pre><code>[[1 2 3 1 2 3]\n [1 2 3 1 2 3]\n [1 2 3 1 2 3]\n [1 2 3 1 2 3]]\n</code></pre> <pre><code>print(t.repeat(4, 2, 1).shape)\n</code></pre> <pre><code>(4, 2, 3)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def repeat(self, repeats, *args) -&gt; Tensor:\n  \"\"\"\n  Repeats tensor number of times along each dimension specified by `repeats`.\n  `repeats` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.repeat(4, 2).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.repeat(4, 2, 1).shape)\n  ```\n  \"\"\"\n  repeats = argfix(repeats, *args)\n  base_shape = (1,) * (len(repeats) - self.ndim) + self.shape\n  new_shape = [x for b in base_shape for x in [1, b]]\n  expand_shape = [x for rs in zip(repeats, base_shape) for x in rs]\n  final_shape = [r*s for r,s in zip(repeats, base_shape)]\n  return self.reshape(new_shape).expand(expand_shape).reshape(final_shape)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.repeat_interleave","title":"repeat_interleave","text":"<pre><code>repeat_interleave(\n    repeats: int, dim: Optional[int] = None\n) -&gt; Tensor\n</code></pre> <p>Repeat elements of a tensor.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.repeat_interleave(2).numpy())\n</code></pre> <pre><code>[1 1 2 2 3 3]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def repeat_interleave(self, repeats:int, dim:Optional[int]=None) -&gt; Tensor:\n  \"\"\"\n  Repeat elements of a tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.repeat_interleave(2).numpy())\n  ```\n  \"\"\"\n  x, dim = (self.flatten(), 0) if dim is None else (self, dim)\n  shp = x.shape\n  return x.reshape(*shp[:dim+1], 1, *shp[dim+1:]).expand(*shp[:dim+1], repeats, *shp[dim+1:]).reshape(*shp[:dim], shp[dim]*repeats, *shp[dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.split","title":"split","text":"<pre><code>split(\n    sizes: Union[int, List[int]], dim: int = 0\n) -&gt; Tuple[Tensor, ...]\n</code></pre> <p>Splits the tensor into chunks along the dimension specified by <code>dim</code>. If <code>sizes</code> is an integer, it splits into equally sized chunks if possible, otherwise the last chunk will be smaller. If <code>sizes</code> is a list, it splits into <code>len(sizes)</code> chunks with size in <code>dim</code> according to <code>size</code>.</p> <p><pre><code>t = Tensor.arange(10).reshape(5, 2)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1]\n [2 3]\n [4 5]\n [6 7]\n [8 9]]\n</code></pre> <pre><code>split = t.split(2)\nprint(\"\\n\".join([repr(x.numpy()) for x in split]))\n</code></pre> <pre><code>array([[0, 1],\n       [2, 3]], dtype=int32)\narray([[4, 5],\n       [6, 7]], dtype=int32)\narray([[8, 9]], dtype=int32)\n</code></pre> <pre><code>split = t.split([1, 4])\nprint(\"\\n\".join([repr(x.numpy()) for x in split]))\n</code></pre> <pre><code>array([[0, 1]], dtype=int32)\narray([[2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]], dtype=int32)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def split(self, sizes:Union[int, List[int]], dim:int=0) -&gt; Tuple[Tensor, ...]:\n  \"\"\"\n  Splits the tensor into chunks along the dimension specified by `dim`.\n  If `sizes` is an integer, it splits into equally sized chunks if possible, otherwise the last chunk will be smaller.\n  If `sizes` is a list, it splits into `len(sizes)` chunks with size in `dim` according to `size`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(10).reshape(5, 2)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  split = t.split(2)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in split]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  split = t.split([1, 4])\n  print(\"\\\\n\".join([repr(x.numpy()) for x in split]))\n  ```\n  \"\"\"\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  dim = self._resolve_dim(dim)\n  if isinstance(sizes, int): sizes = [min(sizes, self.shape[dim]-i) for i in range(0, max(1, self.shape[dim]), max(1, sizes))]\n  assert sum(sizes) == self.shape[dim], f\"expect sizes to sum exactly to {self.shape[dim]}, but got {sum(sizes)}\"\n  return tuple(self[sl] for sl in [tuple([slice(None)]*dim + [slice(sum(sizes[:i]), sum(sizes[:i + 1]))]) for i in range(len(sizes))])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.chunk","title":"chunk","text":"<pre><code>chunk(chunks: int, dim: int = 0) -&gt; List[Tensor]\n</code></pre> <p>Splits the tensor into <code>chunks</code> number of chunks along the dimension <code>dim</code>. If the tensor size along <code>dim</code> is not divisible by <code>chunks</code>, all returned chunks will be the same size except the last one. The function may return fewer than the specified number of chunks.</p> <p><pre><code>chunked = Tensor.arange(11).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1], dtype=int32)\narray([2, 3], dtype=int32)\narray([4, 5], dtype=int32)\narray([6, 7], dtype=int32)\narray([8, 9], dtype=int32)\narray([10], dtype=int32)\n</code></pre> <pre><code>chunked = Tensor.arange(12).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1], dtype=int32)\narray([2, 3], dtype=int32)\narray([4, 5], dtype=int32)\narray([6, 7], dtype=int32)\narray([8, 9], dtype=int32)\narray([10, 11], dtype=int32)\n</code></pre> <pre><code>chunked = Tensor.arange(13).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1, 2], dtype=int32)\narray([3, 4, 5], dtype=int32)\narray([6, 7, 8], dtype=int32)\narray([ 9, 10, 11], dtype=int32)\narray([12], dtype=int32)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def chunk(self, chunks:int, dim:int=0) -&gt; List[Tensor]:\n  \"\"\"\n  Splits the tensor into `chunks` number of chunks along the dimension `dim`.\n  If the tensor size along `dim` is not divisible by `chunks`, all returned chunks will be the same size except the last one.\n  The function may return fewer than the specified number of chunks.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(11).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(12).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(13).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  \"\"\"\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  assert chunks &gt; 0, f\"expect chunks to be greater than 0, got: {chunks}\"\n  dim = self._resolve_dim(dim)\n  return list(self.split(ceildiv(self.shape[dim], chunks) if self.shape[dim] else [0]*chunks, dim=dim))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim: Optional[int] = None) -&gt; Tensor\n</code></pre> <p>Returns a tensor with specified dimensions of input of size 1 removed. If <code>dim</code> is not specified, all dimensions with size 1 are removed.</p> <p><pre><code>t = Tensor.zeros(2, 1, 2, 1, 2)\nprint(t.squeeze().shape)\n</code></pre> <pre><code>(2, 2, 2)\n</code></pre> <pre><code>print(t.squeeze(0).shape)\n</code></pre> <pre><code>(2, 1, 2, 1, 2)\n</code></pre> <pre><code>print(t.squeeze(1).shape)\n</code></pre> <pre><code>(2, 2, 1, 2)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def squeeze(self, dim:Optional[int]=None) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with specified dimensions of input of size 1 removed.\n  If `dim` is not specified, all dimensions with size 1 are removed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.zeros(2, 1, 2, 1, 2)\n  print(t.squeeze().shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.squeeze(0).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.squeeze(1).shape)\n  ```\n  \"\"\"\n  if dim is None: return self.reshape(tuple(dim for dim in self.shape if dim != 1))\n  dim = self._resolve_dim(dim)\n  return self if not self.ndim or self.shape[dim] != 1 else self.reshape(self.shape[:dim] + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim: int) -&gt; Tensor\n</code></pre> <p>Returns a tensor with a new dimension of size 1 inserted at the specified <code>dim</code>.</p> <p><pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.unsqueeze(0).numpy())\n</code></pre> <pre><code>[[1 2 3 4]]\n</code></pre> <pre><code>print(t.unsqueeze(1).numpy())\n</code></pre> <pre><code>[[1]\n [2]\n [3]\n [4]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unsqueeze(self, dim:int) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with a new dimension of size 1 inserted at the specified `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.unsqueeze(0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.unsqueeze(1).numpy())\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim, outer=True)\n  return self.reshape(self.shape[:dim] + (1,) + self.shape[dim:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.pad2d","title":"pad2d","text":"<pre><code>pad2d(padding: Sequence[int], value: float = 0.0) -&gt; Tensor\n</code></pre> <p>Returns a tensor that pads the last two axes specified by <code>padding</code> (padding_left, padding_right, padding_top, padding_bottom). If <code>value</code> is specified, the tensor is padded with <code>value</code> instead of <code>0.0</code>.</p> <p><pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0 1 2]\n   [3 4 5]\n   [6 7 8]]]]\n</code></pre> <pre><code>print(t.pad2d((1, 1, 2, 0), value=-float(\"inf\")).numpy())\n</code></pre> <pre><code>[[[[-inf -inf -inf -inf -inf]\n   [-inf -inf -inf -inf -inf]\n   [-inf   0.   1.   2. -inf]\n   [-inf   3.   4.   5. -inf]\n   [-inf   6.   7.   8. -inf]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pad2d(self, padding:Sequence[int], value:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that pads the last two axes specified by `padding` (padding_left, padding_right, padding_top, padding_bottom).\n  If `value` is specified, the tensor is padded with `value` instead of `0.0`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad2d((1, 1, 2, 0), value=-float(\"inf\")).numpy())\n  ```\n  \"\"\"\n  pads = tuple((smax(p0, 0), smax(p1, 0)) for p0, p1 in zip(padding[::2], padding[1::2]))[::-1]\n  padded = self.pad((None,) * (self.ndim - len(padding) // 2) + tuple(pads), value=value)\n  shrink = tuple((-smin(p0, 0), smin(p1 + s, s)) for p0, p1, s in zip(padding[::2], padding[1::2], padded.shape[::-1]))[::-1]\n  return padded.shrink((None,) * (self.ndim - len(padding) // 2) + shrink)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.T","title":"T  <code>property</code>","text":"<pre><code>T: Tensor\n</code></pre> <p><code>.T</code> is an alias for <code>.transpose()</code>.</p>"},{"location":"tensor/movement/#tinygrad.Tensor.transpose","title":"transpose","text":"<pre><code>transpose(dim0=1, dim1=0) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a transposed version of the original tensor. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.transpose(0, 1).numpy())\n</code></pre> <pre><code>[[0 3]\n [1 4]\n [2 5]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def transpose(self, dim0=1, dim1=0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a transposed version of the original tensor.\n  The given dimensions `dim0` and `dim1` are swapped.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.transpose(0, 1).numpy())\n  ```\n  \"\"\"\n  order = list(range(self.ndim))\n  order[dim0], order[dim1] = order[dim1], order[dim0]\n  return self.permute(order)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.flatten","title":"flatten","text":"<pre><code>flatten(start_dim=0, end_dim=-1)\n</code></pre> <p>Flattens the tensor by reshaping it into a one-dimensional tensor. If <code>start_dim</code> or <code>end_dim</code> are passed, only dimensions starting with <code>start_dim</code> and ending with <code>end_dim</code> are flattened.</p> <p><pre><code>t = Tensor.arange(8).reshape(2, 2, 2)\nprint(t.flatten().numpy())\n</code></pre> <pre><code>[0 1 2 3 4 5 6 7]\n</code></pre> <pre><code>print(t.flatten(start_dim=1).numpy())\n</code></pre> <pre><code>[[0 1 2 3]\n [4 5 6 7]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flatten(self, start_dim=0, end_dim=-1):\n  \"\"\"\n  Flattens the tensor by reshaping it into a one-dimensional tensor.\n  If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim` and ending with `end_dim` are flattened.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(8).reshape(2, 2, 2)\n  print(t.flatten().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flatten(start_dim=1).numpy())\n  ```\n  \"\"\"\n  start_dim, end_dim = self._resolve_dim(start_dim), self._resolve_dim(end_dim)\n  return self.reshape(self.shape[:start_dim] + (prod(self.shape[start_dim:end_dim+1]), ) + self.shape[end_dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.unflatten","title":"unflatten","text":"<pre><code>unflatten(dim: int, sizes: Tuple[int, ...])\n</code></pre> <p>Unflattens dimension <code>dim</code> of the tensor into multiple dimensions specified by <code>sizes</code>. <code>Tensor.flatten()</code> is the inverse of this function.</p> <p><pre><code>print(Tensor.ones(3, 4, 1).unflatten(1, (2, 2)).shape)\n</code></pre> <pre><code>(3, 2, 2, 1)\n</code></pre> <pre><code>print(Tensor.ones(3, 4, 1).unflatten(1, (-1, 2)).shape)\n</code></pre> <pre><code>(3, 2, 2, 1)\n</code></pre> <pre><code>print(Tensor.ones(5, 12, 3).unflatten(-2, (2, 2, 3, 1, 1)).shape)\n</code></pre> <pre><code>(5, 2, 2, 3, 1, 1, 3)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unflatten(self, dim:int, sizes:Tuple[int,...]):\n  \"\"\"\n  Unflattens dimension `dim` of the tensor into multiple dimensions specified by `sizes`. `Tensor.flatten()` is the inverse of this function.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(3, 4, 1).unflatten(1, (2, 2)).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(3, 4, 1).unflatten(1, (-1, 2)).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(5, 12, 3).unflatten(-2, (2, 2, 3, 1, 1)).shape)\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim)\n  return self.reshape(self.shape[:dim] + sizes + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/ops/","title":"Complex Ops","text":""},{"location":"tensor/ops/#reduce","title":"Reduce","text":""},{"location":"tensor/ops/#tinygrad.Tensor.sum","title":"sum","text":"<pre><code>sum(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n    acc_dtype: Optional[DTypeLike] = None,\n)\n</code></pre> <p>Returns the sum of the elements of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p>You can pass in <code>acc_dtype</code> keyword argument to control the data type of the accumulation. If not specified, the accumulation data type is chosen based on the input tensor's data type.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.sum().numpy())\n</code></pre> <pre><code>15\n</code></pre> <pre><code>print(t.sum(axis=0).numpy())\n</code></pre> <pre><code>[3 5 7]\n</code></pre> <pre><code>print(t.sum(axis=1).numpy())\n</code></pre> <pre><code>[ 3 12]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sum(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False, acc_dtype:Optional[DTypeLike]=None):\n  \"\"\"\n  Returns the sum of the elements of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  You can pass in `acc_dtype` keyword argument to control the data type of the accumulation.\n  If not specified, the accumulation data type is chosen based on the input tensor's data type.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum(axis=1).numpy())\n  ```\n  \"\"\"\n  ret = self.cast(acc_dtype or sum_acc_dtype(self.dtype))._reduce(F.Sum, axis, keepdim)\n  return ret.cast(self.dtype) if acc_dtype is None and self.dtype in (dtypes.float16, dtypes.bfloat16) else ret\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.prod","title":"prod","text":"<pre><code>prod(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n    acc_dtype: Optional[DTypeLike] = None,\n)\n</code></pre> <p>Returns the product of the elements of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p>You can pass in <code>acc_dtype</code> keyword argument to control the data type of the accumulation. If not specified, the accumulation data type is chosen based on the input tensor's data type.</p> <p><pre><code>t = Tensor([-1, -2, -3, 1, 2, 3]).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[-1 -2 -3]\n [ 1  2  3]]\n</code></pre> <pre><code>print(t.prod().numpy())\n</code></pre> <pre><code>-36\n</code></pre> <pre><code>print(t.prod(axis=0).numpy())\n</code></pre> <pre><code>[-1 -4 -9]\n</code></pre> <pre><code>print(t.prod(axis=1).numpy())\n</code></pre> <pre><code>[-6  6]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def prod(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False, acc_dtype:Optional[DTypeLike]=None):\n  \"\"\"\n  Returns the product of the elements of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  You can pass in `acc_dtype` keyword argument to control the data type of the accumulation.\n  If not specified, the accumulation data type is chosen based on the input tensor's data type.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, -2, -3, 1, 2, 3]).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod(axis=1).numpy())\n  ```\n  \"\"\"\n  return self.cast(acc_dtype or self.dtype)._reduce(F.Prod, axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.max","title":"max","text":"<pre><code>max(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n)\n</code></pre> <p>Returns the maximum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.max().numpy())\n</code></pre> <pre><code>5\n</code></pre> <pre><code>print(t.max(axis=0).numpy())\n</code></pre> <pre><code>[5 4 3]\n</code></pre> <pre><code>print(t.max(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[2]\n [5]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False):\n  \"\"\"\n  Returns the maximum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self._reduce(F.Max, axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.min","title":"min","text":"<pre><code>min(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n)\n</code></pre> <p>Returns the minimum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.min().numpy())\n</code></pre> <pre><code>0\n</code></pre> <pre><code>print(t.min(axis=0).numpy())\n</code></pre> <pre><code>[1 0 2]\n</code></pre> <pre><code>print(t.min(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[0]\n [3]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def min(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False):\n  \"\"\"\n  Returns the minimum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return -((-self).max(axis=axis, keepdim=keepdim))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.any","title":"any","text":"<pre><code>any(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n)\n</code></pre> <p>Tests if any element evaluates to <code>True</code> along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the reduce axis and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[True, True], [True, False], [False, False]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ True  True]\n [ True False]\n [False False]]\n</code></pre> <pre><code>print(t.any().numpy())\n</code></pre> <pre><code>True\n</code></pre> <pre><code>print(t.any(axis=0).numpy())\n</code></pre> <pre><code>[ True  True]\n</code></pre> <pre><code>print(t.any(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[ True]\n [ True]\n [False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def any(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False):\n  \"\"\"\n  Tests if any element evaluates to `True` along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the reduce axis and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[True, True], [True, False], [False, False]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self.bool().max(axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.all","title":"all","text":"<pre><code>all(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n)\n</code></pre> <p>Tests if all element evaluates to <code>True</code> along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the reduce axis and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[True, True], [True, False], [False, False]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ True  True]\n [ True False]\n [False False]]\n</code></pre> <pre><code>print(t.all().numpy())\n</code></pre> <pre><code>False\n</code></pre> <pre><code>print(t.all(axis=0).numpy())\n</code></pre> <pre><code>[False False]\n</code></pre> <pre><code>print(t.all(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[ True]\n [False]\n [False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def all(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False):\n  \"\"\"\n  Tests if all element evaluates to `True` along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the reduce axis and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[True, True], [True, False], [False, False]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self.logical_not().any(axis, keepdim).logical_not()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.mean","title":"mean","text":"<pre><code>mean(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n)\n</code></pre> <p>Returns the mean value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the mean is computed and whether the reduced dimensions are retained.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.mean().numpy())\n</code></pre> <pre><code>2.5907674\n</code></pre> <pre><code>print(t.mean(axis=0).numpy())\n</code></pre> <pre><code>[2.6623 2.4031 2.707 ]\n</code></pre> <pre><code>print(t.mean(axis=1).numpy())\n</code></pre> <pre><code>[2.833  2.3485]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mean(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False):\n  \"\"\"\n  Returns the mean value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the mean is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean(axis=1).numpy())\n  ```\n  \"\"\"\n  output_dtype = self.dtype if dtypes.is_float(self.dtype) else dtypes.float32\n  numerator = self.cast(sum_acc_dtype(self.dtype)).sum(axis=axis, keepdim=keepdim)\n  return numerator.div(prod([si for si, so in zip(self.shape, self.sum(axis=axis, keepdim=True).shape) if resolve(si != so)])).cast(output_dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.var","title":"var","text":"<pre><code>var(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n    correction=1,\n)\n</code></pre> <p>Returns the variance of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.var().numpy())\n</code></pre> <pre><code>0.109925404\n</code></pre> <pre><code>print(t.var(axis=0).numpy())\n</code></pre> <pre><code>[0.2134 0.2189 0.0096]\n</code></pre> <pre><code>print(t.var(axis=1).numpy())\n</code></pre> <pre><code>[0.0187 0.08  ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def var(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False, correction=1):\n  \"\"\"\n  Returns the variance of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var(axis=1).numpy())\n  ```\n  \"\"\"\n  squares = (self - self.mean(axis=axis, keepdim=True)).square()\n  n = prod([si for si, so in zip(self.shape, squares.sum(axis=axis, keepdim=True).shape) if resolve(si != so)])\n  return squares.sum(axis=axis, keepdim=keepdim).div(smax([0, n-correction]))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.std","title":"std","text":"<pre><code>std(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n    correction=1,\n)\n</code></pre> <p>Returns the standard deviation of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.std().numpy())\n</code></pre> <pre><code>0.33155\n</code></pre> <pre><code>print(t.std(axis=0).numpy())\n</code></pre> <pre><code>[0.462  0.4679 0.0981]\n</code></pre> <pre><code>print(t.std(axis=1).numpy())\n</code></pre> <pre><code>[0.1367 0.2829]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def std(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False, correction=1):\n  \"\"\"\n  Returns the standard deviation of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std(axis=1).numpy())\n  ```\n  \"\"\"\n  return self.var(axis, keepdim, correction).sqrt()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.std_mean","title":"std_mean","text":"<pre><code>std_mean(\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdim=False,\n    correction=1,\n)\n</code></pre> <p>Calculates the standard deviation and mean over the dimensions specified by dim. Syntactic sugar around <code>Tensor.std</code> and <code>Tensor.mean</code> to match <code>torch.std_mean</code>.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>std, mean = t.std_mean()\nprint(std.numpy(), mean.numpy())\n</code></pre> <pre><code>0.33155 2.5907674\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def std_mean(self, axis:Optional[Union[int, Sequence[int]]]=None, keepdim=False, correction=1):\n  \"\"\"\n  Calculates the standard deviation and mean over the dimensions specified by dim.\n  Syntactic sugar around `Tensor.std` and `Tensor.mean` to match `torch.std_mean`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  std, mean = t.std_mean()\n  print(std.numpy(), mean.numpy())\n  ```\n  \"\"\"\n  return self.std(axis, keepdim, correction), self.mean(axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.softmax","title":"softmax","text":"<pre><code>softmax(axis=-1, dtype: Optional[DTypeLike] = None)\n</code></pre> <p>Applies the softmax function to the tensor along the specified axis.</p> <p>Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the softmax is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.softmax().numpy())\n</code></pre> <pre><code>[[0.4436 0.2664 0.29  ]\n [0.2924 0.1727 0.5349]]\n</code></pre> <pre><code>print(t.softmax(axis=0).numpy())\n</code></pre> <pre><code>[[0.787  0.7897 0.5689]\n [0.213  0.2103 0.4311]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softmax(self, axis=-1, dtype:Optional[DTypeLike]=None):\n  \"\"\"\n  Applies the softmax function to the tensor along the specified axis.\n\n  Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.\n\n  You can pass in the `axis` keyword argument to control the axis along which the softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.softmax().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  _, e, ss = self._softmax(axis, dtype)\n  return e.div(ss)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.log_softmax","title":"log_softmax","text":"<pre><code>log_softmax(axis=-1, dtype: Optional[DTypeLike] = None)\n</code></pre> <p>Applies the log-softmax function to the tensor along the specified axis.</p> <p>The log-softmax function is a numerically stable alternative to the softmax function in log space.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the log-softmax is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.log_softmax().numpy())\n</code></pre> <pre><code>[[-0.8127 -1.3228 -1.238 ]\n [-1.2297 -1.7564 -0.6256]]\n</code></pre> <pre><code>print(t.log_softmax(axis=0).numpy())\n</code></pre> <pre><code>[[-0.2396 -0.2361 -0.564 ]\n [-1.5463 -1.5594 -0.8414]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log_softmax(self, axis=-1, dtype:Optional[DTypeLike]=None):\n  \"\"\"\n  Applies the log-softmax function to the tensor along the specified axis.\n\n  The log-softmax function is a numerically stable alternative to the softmax function in log space.\n\n  You can pass in the `axis` keyword argument to control the axis along which the log-softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.log_softmax().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.log_softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  m, _, ss = self._softmax(axis, dtype)\n  return m - ss.log()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.logsumexp","title":"logsumexp","text":"<pre><code>logsumexp(axis=None, keepdim=False)\n</code></pre> <p>Computes the log-sum-exp of the tensor along the specified axis or axes.</p> <p>The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the log-sum-exp is computed and whether the reduced dimensions are retained.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.logsumexp().numpy())\n</code></pre> <pre><code>2.1347282\n</code></pre> <pre><code>print(t.logsumexp(axis=0).numpy())\n</code></pre> <pre><code>[1.2174 0.7039 1.1167]\n</code></pre> <pre><code>print(t.logsumexp(axis=1).numpy())\n</code></pre> <pre><code>[1.7906 0.9009]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logsumexp(self, axis=None, keepdim=False):\n  \"\"\"\n  Computes the log-sum-exp of the tensor along the specified axis or axes.\n\n  The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the log-sum-exp is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp(axis=1).numpy())\n  ```\n  \"\"\"\n  m = self.max(axis=axis, keepdim=True)\n  return (self - m).exp().sum(axis=axis, keepdim=keepdim).log() + m.squeeze(axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.logcumsumexp","title":"logcumsumexp","text":"<pre><code>logcumsumexp(axis=0)\n</code></pre> <p>Computes the log-cumsum-exp of the tensor along the specified axis or axes.</p> <p>The log-cumsum-exp function is a numerically stable way to compute the logarithm of the cumulative sum of exponentials.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the log-cum-sum-exp is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.logcumsumexp().numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [1.2174 0.7039 1.1167]]\n</code></pre> <pre><code>print(t.logcumsumexp(axis=0).numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [1.2174 0.7039 1.1167]]\n</code></pre> <pre><code>print(t.logcumsumexp(axis=1).numpy())\n</code></pre> <pre><code>[[ 0.9779  1.4481  1.7906]\n [-0.3288  0.1353  0.9009]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logcumsumexp(self, axis=0):\n  \"\"\"\n  Computes the log-cumsum-exp of the tensor along the specified axis or axes.\n\n  The log-cumsum-exp function is a numerically stable way to compute the logarithm of the cumulative sum of exponentials.\n\n  You can pass in the `axis` keyword argument to control the axis along which\n  the log-cum-sum-exp is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp(axis=1).numpy())\n  ```\n  \"\"\"\n  m = self.max(axis=axis, keepdim=True)\n  return (self - m).exp().cumsum(axis=axis).log() + m\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.argmax","title":"argmax","text":"<pre><code>argmax(axis=None, keepdim=False)\n</code></pre> <p>Returns the indices of the maximum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\n</code></pre> <pre><code>3\n</code></pre> <pre><code>print(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\n</code></pre> <pre><code>[1 1 1]\n</code></pre> <pre><code>print(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n</code></pre> <pre><code>[2 0]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmax(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the indices of the maximum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n  ```\n  \"\"\"\n  if axis is None: return self.flatten().argmax(0)\n  axis = self._resolve_dim(axis)\n  m = self == self.max(axis=axis, keepdim=True)\n  idx = m * Tensor.arange(self.shape[axis]-1,-1,-1, requires_grad=False, device=self.device).reshape(self.shape[axis], *[1]*(self.ndim-axis-1))\n  return (self.shape[axis]-idx.max(axis=axis, keepdim=keepdim)-1).cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.argmin","title":"argmin","text":"<pre><code>argmin(axis=None, keepdim=False)\n</code></pre> <p>Returns the indices of the minimum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\n</code></pre> <pre><code>1\n</code></pre> <pre><code>print(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\n</code></pre> <pre><code>[0 0 0]\n</code></pre> <pre><code>print(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n</code></pre> <pre><code>[1 2]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmin(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the indices of the minimum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n  ```\n  \"\"\"\n  return (-self).argmax(axis=axis, keepdim=keepdim)\n</code></pre>"},{"location":"tensor/ops/#processing","title":"Processing","text":""},{"location":"tensor/ops/#tinygrad.Tensor.avg_pool2d","title":"avg_pool2d","text":"<pre><code>avg_pool2d(\n    kernel_size=(2, 2),\n    stride=None,\n    dilation=1,\n    padding=0,\n    count_include_pad=True,\n)\n</code></pre> <p>Applies average pooling over a tensor.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.</p> <p>See: https://paperswithcode.com/method/average-pooling</p> <p><pre><code>t = Tensor.arange(25).reshape(1, 1, 5, 5)\nprint(t.avg_pool2d().numpy())\n</code></pre> <pre><code>[[[[ 3.  5.]\n   [13. 15.]]]]\n</code></pre> <pre><code>print(t.avg_pool2d(padding=1).numpy())\n</code></pre> <pre><code>[[[[ 0.    0.75  1.75]\n   [ 3.75  9.   11.  ]\n   [ 8.75 19.   21.  ]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def avg_pool2d(self, kernel_size=(2,2), stride=None, dilation=1, padding=0, count_include_pad=True):\n  \"\"\"\n  Applies average pooling over a tensor.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.\n\n  See: https://paperswithcode.com/method/average-pooling\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(25).reshape(1, 1, 5, 5)\n  print(t.avg_pool2d().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.avg_pool2d(padding=1).numpy())\n  ```\n  \"\"\"\n  padding_, axis = self._padding2d(padding, len(k_ := make_tuple(kernel_size, 2))), tuple(range(-len(k_), 0))\n  def pool(x:Tensor) -&gt; Tensor: return x.pad2d(padding_)._pool(k_, stride if stride is not None else k_, dilation)\n  return pool(self).mean(axis=axis) if count_include_pad else pool(self).sum(axis=axis) / pool(self.ones_like()).sum(axis=axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.max_pool2d","title":"max_pool2d","text":"<pre><code>max_pool2d(\n    kernel_size=(2, 2), stride=None, dilation=1, padding=0\n)\n</code></pre> <p>Applies max pooling over a tensor.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.</p> <p>See: https://paperswithcode.com/method/max-pooling</p> <p><pre><code>t = Tensor.arange(25).reshape(1, 1, 5, 5)\nprint(t.max_pool2d().numpy())\n</code></pre> <pre><code>[[[[ 6  8]\n   [16 18]]]]\n</code></pre> <pre><code>print(t.max_pool2d(padding=1).numpy())\n</code></pre> <pre><code>[[[[ 0.  2.  4.]\n   [10. 12. 14.]\n   [20. 22. 24.]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max_pool2d(self, kernel_size=(2,2), stride=None, dilation=1, padding=0):\n  \"\"\"\n  Applies max pooling over a tensor.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.\n\n  See: https://paperswithcode.com/method/max-pooling\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(25).reshape(1, 1, 5, 5)\n  print(t.max_pool2d().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max_pool2d(padding=1).numpy())\n  ```\n  \"\"\"\n  padding_ = self._padding2d(padding, len(k_ := make_tuple(kernel_size, 2)))\n  return self.pad2d(padding_, value=float('-inf'))._pool(k_, stride if stride is not None else k_, dilation).max(axis=tuple(range(-len(k_), 0)))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.conv2d","title":"conv2d","text":"<pre><code>conv2d(\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding: int | Tuple[int, ...] = 0,\n    acc_dtype: Optional[DTypeLike] = None,\n) -&gt; Tensor\n</code></pre> <p>Applies a convolution over a tensor with a given <code>weight</code> and optional <code>bias</code>.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html</p> <pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nw = Tensor.ones(1, 1, 2, 2)\nprint(t.conv2d(w).numpy())\n</code></pre> <pre><code>[[[[ 8. 12.]\n   [20. 24.]]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv2d(self, weight:Tensor, bias:Optional[Tensor]=None, groups=1, stride=1, dilation=1, padding:int|Tuple[int, ...]=0,\n           acc_dtype:Optional[DTypeLike]=None) -&gt; Tensor:\n  \"\"\"\n  Applies a convolution over a tensor with a given `weight` and optional `bias`.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  w = Tensor.ones(1, 1, 2, 2)\n  print(t.conv2d(w).numpy())\n  ```\n  \"\"\"\n  if IMAGE: return self.image_conv2d(weight, bias, groups, stride, dilation, padding, acc_dtype)\n  (bs,cin_), (cout,cin), HW = self.shape[:2], weight.shape[:2], weight.shape[2:]\n  assert groups*cin == cin_ and len(self.shape) == len(weight.shape), f\"Input Tensor shape {self.shape} does not match the shape of the weights {weight.shape}. ({groups*cin} vs. {cin_})\"  # noqa: E501\n  if isinstance(padding, (tuple,list)): assert len(padding) == 2*len(HW) or len(padding) == len(HW), f\"Expected padding of length {2*len(HW)} or {len(HW)}, but got {len(padding)} for tensor of shape {self.shape}\"  # noqa: E501\n  padding_ = self._padding2d(padding, len(HW))\n\n  # conv2d is a pooling op (with padding)\n  x = self.pad2d(padding_)._pool(HW, stride, dilation)   # (bs, groups*cin, oy, ox, H, W)\n  rcout, oyx = cout//groups, x.shape[2:-len(HW)]\n  if not all(x == 3 for x in HW) or stride != 1 or dilation != 1 or not WINO:\n    # normal conv\n    x = x.reshape(bs, groups, cin, 1, *oyx, *HW).expand(bs, groups, cin, rcout, *oyx, *HW).permute(0,1,3,*[4+i for i in range(len(oyx))],2,*[4+len(oyx)+i for i in range(len(HW))])  # noqa: E501\n\n    # conv! broadcasted to (bs, groups, rcout, *oyx, cin, *HW)\n    ret = (x * weight.reshape(1, groups, rcout, *[1] * len(oyx), cin, *HW)).sum([-1-i for i in range(1+len(oyx))], keepdim=True, acc_dtype=acc_dtype).reshape(bs, cout, *oyx)  # noqa: E501\n    return ret if bias is None else ret.add(bias.reshape(1, -1, *[1] * len(HW)))\n\n  HWI, HWO = (6,) * len(HW), (4,) * len(HW)  # F(4x4,3x3) winograd tiles\n  winograd_G = [[1/4, 0, 0], [-1/6, -1/6, -1/6], [-1/6, 1/6, -1/6], [1/24, 1/12, 1/6], [1/24, -1/12, 1/6], [0, 0, 1]]\n  winograd_Bt = [[4, 0, -5, 0, 1, 0], [0, -4, -4, 1, 1, 0], [0, 4, -4, -1, 1, 0], [0, -2, -1, 2, 1, 0], [0, 2, -1, -2, 1, 0], [0, 4, 0, -5, 0, 1]]\n  winograd_At = [[1, 1, 1, 1, 1, 0], [0, 1, -1, 2, -2, 0], [0, 1, 1, 4, 4, 0], [0, 1, -1, 8, -8, 1]] # applying At in pre-order doubles compile time\n\n  # todo: stride == dilation\n  # use padding to round up to 4x4 output tiles\n  # (bs, cin_, tyx, HWI)\n  d = self.pad2d(sum([[padding_[i*2], padding_[i*2+1] + (-(dim + sum(padding_[i * 2:(i + 1) * 2]) - 2) % 4)] for i, dim in enumerate(self.shape[-len(HW):])], []))._pool(HWI, HWO)  # noqa: E501\n  # move HW to the front: # (HWI, bs, cin_, tyx)\n  d = d.permute(*range(len(d.shape)-len(HW),len(d.shape)), *range(len(d.shape)-len(HW)))\n  tyx = d.shape[-len(HWI):]  # dim of tiling\n\n  g = weight.permute(*range(len(weight.shape)-len(HW),len(weight.shape)), *range(len(weight.shape)-len(HW)))  # move HW to the front\n\n  # compute 6x6 winograd tiles: GgGt, BtdB\n  # (HWI, groups * rcout, cin) -&gt; (HWI, bs=1, groups, rcout, cin, tyx=(1,1))\n  gfactors = _apply_winograd_matrix(winograd_G, g, len(HW)).reshape(*HWI, 1, groups, rcout, cin, *([1]*len(tyx)))\n  # (HWI, bs, cin_, tyx) -&gt; (HWI, bs, groups, 1 ,cin, *tyx)\n  dfactors = _apply_winograd_matrix(winograd_Bt, d, len(HW)).reshape(*HWI, bs, groups, 1, cin, *tyx)\n\n  # matmul; sum across cin: (HWI, bs, groups, rcout, *tyx); then HWI -&gt; HWO: (HWO, bs, groups, rcout, *tyx)\n  ret = _apply_winograd_matrix(winograd_At, (gfactors * dfactors).sum(axis=-1-len(HW), acc_dtype=acc_dtype), len(HW))\n\n  # interleave tyx and HWO: (bs, groups, rcout, oy, HO, ox, WO)\n  ret = ret.permute([*range(len(HW), len(ret.shape)-len(HW)), *[i+o for i in range(len(HW)) for o in [len(ret.shape)-len(HW),0]]])\n  # merge groups and rcout, tyx and HWO: (bs, groups, cout, *yx), shrink to final\n  ret = ret.reshape(bs, cout, *[c * HWO[i] for i, c in enumerate(tyx)]).shrink(tuple((0, s) for s in [bs, cout, *oyx]))\n\n  return (ret if bias is None else ret.add(bias.reshape(1, -1, *[1 for _ in range(len(HW))]))).contiguous().contiguous_backward()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.conv_transpose2d","title":"conv_transpose2d","text":"<pre><code>conv_transpose2d(\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding=0,\n    output_padding=0,\n) -&gt; Tensor\n</code></pre> <p>Applies a transposed convolution over a tensor with a given <code>weight</code> and optional <code>bias</code>.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d transposed convolutions and instead works for any number of dimensions.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html</p> <pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nw = Tensor.ones(1, 1, 2, 2)\nprint(t.conv_transpose2d(w).numpy())\n</code></pre> <pre><code>[[[[ 0.  1.  3.  2.]\n   [ 3.  8. 12.  7.]\n   [ 9. 20. 24. 13.]\n   [ 6. 13. 15.  8.]]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv_transpose2d(self, weight:Tensor, bias:Optional[Tensor]=None, groups=1, stride=1, dilation=1, padding=0, output_padding=0) -&gt; Tensor:\n  \"\"\"\n  Applies a transposed convolution over a tensor with a given `weight` and optional `bias`.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d transposed convolutions and instead works for any number of dimensions.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  w = Tensor.ones(1, 1, 2, 2)\n  print(t.conv_transpose2d(w).numpy())\n  ```\n  \"\"\"\n  x, w = self, weight.unflatten(0, (groups, -1)).transpose(1, 2).flip(*range(3, len(weight.shape)+1))\n  HW = weight.shape[2:]\n  stride, dilation, padding, output_padding = [make_tuple(x, len(HW)) for x in (stride, dilation, padding, output_padding)]\n  if any(s&gt;1 for s in stride):\n    # handle strides: (k) -&gt; reshape -&gt; (k,1) -&gt; pad -&gt; (k,s) -&gt; reshape -&gt; (k*s) -&gt; shrink (k-(s-1))\n    x = x.reshape(None, None, *flatten((k,1) for k in x.shape[2:]))\n    x = x.pad((None, None, *flatten((None,(0,s-1)) for s in stride)))\n    x = x.reshape(None, None, *[k*s for k,s in zip(x.shape[2::2], stride)])\n    x = x.shrink((None, None, *[(0,k-(s-1)) for k,s in zip(x.shape[2:], stride)]))\n  padding = flatten((((k-1)*d-p,(k-1)*d-p+op) for k,d,p,op in reversed(list(zip(HW, dilation, padding, output_padding)))))\n  return x.conv2d(w.flatten(end_dim=1), groups=groups, bias=bias, dilation=dilation, padding=padding)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.dot","title":"dot","text":"<pre><code>dot(\n    w: Tensor, acc_dtype: Optional[DTypeLike] = None\n) -&gt; Tensor\n</code></pre> <p>Performs dot product between two tensors.</p> <p>You can pass in the optional <code>acc_dtype</code> keyword argument to control the data type of the accumulation.</p> <pre><code>a = Tensor([[1, 2], [3, 4]])\nb = Tensor([[5, 6], [7, 8]])\nprint(a.dot(b).numpy())\n</code></pre> <pre><code>[[19 22]\n [43 50]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dot(self, w:Tensor, acc_dtype:Optional[DTypeLike]=None) -&gt; Tensor:\n  \"\"\"\n  Performs dot product between two tensors.\n\n  You can pass in the optional `acc_dtype` keyword argument to control the data type of the accumulation.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  a = Tensor([[1, 2], [3, 4]])\n  b = Tensor([[5, 6], [7, 8]])\n  print(a.dot(b).numpy())\n  ```\n  \"\"\"\n  if IMAGE: return self.image_dot(w, acc_dtype)\n  n1, n2 = len(self.shape), len(w.shape)\n  assert n1 != 0 and n2 != 0, f\"both arguments to matmul need to be at least 1D, but they are {n1}D and {n2}D\"\n  if (L:=self.shape[-1]) != (R:=w.shape[-min(n2, 2)]): raise AssertionError(f\"shapes {self.shape} and {w.shape} cannot be multiplied ({L} != {R})\")\n  x = self.reshape(*self.shape[0:-1], *[1]*min(n1-1, n2-1, 1), self.shape[-1])\n  w = w.reshape(*w.shape[0:-2], *[1]*min(n1-1, n2-1, 1), *w.shape[-min(n2, 2):]).transpose(-1, -min(n2, 2))\n  return (x*w).sum(-1, acc_dtype=acc_dtype).cast(least_upper_dtype(x.dtype, w.dtype) if acc_dtype is None else acc_dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.matmul","title":"matmul","text":"<pre><code>matmul(\n    x: Tensor,\n    reverse=False,\n    acc_dtype: Optional[DTypeLike] = None,\n) -&gt; Tensor\n</code></pre> <p>Performs matrix multiplication between two tensors.</p> <p>You can pass in the <code>reverse</code> keyword argument to control the order of the matrix multiplication. You can pass in the optional <code>acc_dtype</code> keyword argument to control the data type of the accumulation.</p> <pre><code>a = Tensor([[1, 2], [3, 4]])\nb = Tensor([[5, 6], [7, 8]])\nprint(a.matmul(b).numpy())\n</code></pre> <pre><code>[[19 22]\n [43 50]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def matmul(self, x:Tensor, reverse=False, acc_dtype:Optional[DTypeLike]=None) -&gt; Tensor:\n  \"\"\"\n  Performs matrix multiplication between two tensors.\n\n  You can pass in the `reverse` keyword argument to control the order of the matrix multiplication.\n  You can pass in the optional `acc_dtype` keyword argument to control the data type of the accumulation.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  a = Tensor([[1, 2], [3, 4]])\n  b = Tensor([[5, 6], [7, 8]])\n  print(a.matmul(b).numpy())\n  ```\n  \"\"\"\n  return x.dot(self, acc_dtype=acc_dtype) if reverse else self.dot(x, acc_dtype=acc_dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.einsum","title":"einsum  <code>staticmethod</code>","text":"<pre><code>einsum(\n    formula: str,\n    *raw_xs,\n    acc_dtype: Optional[DTypeLike] = None\n) -&gt; Tensor\n</code></pre> <p>Sums the product of the elements of the input tensors according to a formula based on the Einstein summation convention.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.einsum.html</p> <pre><code>x = Tensor([[1, 2], [3, 4]])\ny = Tensor([[5, 6], [7, 8]])\nprint(Tensor.einsum(\"ij,ij-&gt;\", x, y).numpy())\n</code></pre> <pre><code>70\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef einsum(formula:str, *raw_xs, acc_dtype:Optional[DTypeLike]=None) -&gt; Tensor:\n  \"\"\"\n  Sums the product of the elements of the input tensors according to a formula based on the Einstein summation convention.\n\n  See: https://pytorch.org/docs/stable/generated/torch.einsum.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  x = Tensor([[1, 2], [3, 4]])\n  y = Tensor([[5, 6], [7, 8]])\n  print(Tensor.einsum(\"ij,ij-&gt;\", x, y).numpy())\n  ```\n  \"\"\"\n  def parse_formula(formula: str, *operands: Tensor):\n    if \".\" in formula:\n      ell_chars, ell_longest = \"\".join(set(string.ascii_letters) - set(formula)), 0\n      for i, inp in enumerate(filter(lambda x: \"...\" in x, inputs := formula.split(\"-&gt;\")[0].split(\",\"))):\n        if (ell_count := max(operands[i].ndim, 1) - (len(inp) - 3)) &gt; ell_longest: ell_longest = ell_count\n        inputs[i] = inp.replace(\"...\", \"\" if ell_count == 0 else ell_chars[-ell_count:])\n      inputs_str, out_ellipse = \",\".join(inputs), \"\" if ell_longest == 0 else ell_chars[-ell_longest:]\n      return (inputs_str, formula.split(\"-&gt;\")[1].replace(\"...\", out_ellipse)) if \"-&gt;\" in formula else (inputs_str, \\\n          out_ellipse + ''.join(sorted(c for c in inputs_str if inputs_str.count(c) == 1 and c.isalpha() and c not in out_ellipse)))\n    return formula.split(\"-&gt;\") if \"-&gt;\" in formula else (formula, ''.join(c for c in sorted(formula) if formula.count(c) == 1 and c.isalpha()))\n\n  xs:Tuple[Tensor, ...] = argfix(*raw_xs)\n  inputs_str, output = parse_formula(formula.replace(\" \", \"\"), *xs)\n  inputs = inputs_str.split(\",\")\n  assert len(xs) == len(inputs), f\"number of inputs doesn't match number of operands in formula, expected {len(inputs)}, got {len(xs)}\"\n\n  # map the value of each letter in the formula\n  letter_val = sorted(merge_dicts([dict(zip(letters, tensor.shape)) for letters, tensor in zip(inputs, xs)]).items())\n\n  xs_:List[Tensor] = []\n  lhs = [sorted(enumerate(s), key=lambda e:e[1]) for s in inputs]\n  for x,(order,letters) in zip(xs, [list(zip(*l)) for l in lhs]):\n    # permute to the sorted letter order, then reshape/expand to create dimensions for the missing letters\n    xs_.append(x.permute(order).reshape([val if letter in letters else 1 for letter,val in letter_val]).expand([val for _,val in letter_val]))\n\n  # determine the inverse permutation to revert back to original order\n  rhs_letter_order = argsort(list(output))\n  rhs_order = argsort(rhs_letter_order)\n\n  # sum over all axes that's not in the output, then permute to the output order\n  return functools.reduce(lambda a,b:a*b, xs_) \\\n    .sum(axis=[axis for axis,(letter,_) in enumerate(letter_val) if letter not in output],acc_dtype=acc_dtype).permute(rhs_order)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.cumsum","title":"cumsum","text":"<pre><code>cumsum(axis: int = 0) -&gt; Tensor\n</code></pre> <p>Computes the cumulative sum of the tensor along the specified axis.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the cumulative sum is computed.</p> <p><pre><code>t = Tensor.ones(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <pre><code>print(t.cumsum(1).numpy())\n</code></pre> <pre><code>[[1. 2. 3.]\n [1. 2. 3.]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cumsum(self, axis:int=0) -&gt; Tensor:\n  \"\"\"\n  Computes the cumulative sum of the tensor along the specified axis.\n\n  You can pass in the `axis` keyword argument to control the axis along which the cumulative sum is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.cumsum(1).numpy())\n  ```\n  \"\"\"\n  axis = self._resolve_dim(axis)\n  if self.ndim == 0 or 0 in self.shape: return self\n  # TODO: someday the optimizer will find this on it's own\n  # for now this is a two stage cumsum\n  SPLIT = 256\n  if not isinstance(s:=self.shape[axis], int) or s &lt;= SPLIT*2: return self._cumsum(axis)\n  ret = self.transpose(axis,-1).pad2d((round_up(s, SPLIT)-s, 0)).unflatten(-1, (-1, SPLIT))._cumsum(-1)\n  base_add = ret[..., -1]._cumsum(-1, _first_zero=True)\n  base_add = base_add.unsqueeze(-1).expand(*base_add.shape, ret.shape[-1])\n  def fix(x:Tensor): return x.flatten(start_dim=-2)[..., -s:].transpose(axis,-1)\n  return fix(ret) + fix(base_add)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.triu","title":"triu","text":"<pre><code>triu(diagonal: int = 0) -&gt; Tensor\n</code></pre> <p>Returns the upper triangular part of the tensor, the other elements are set to 0.</p> <p>The argument <code>diagonal</code> determines which diagonal is on the boundary. <code>diagonal = 0</code> means the main diagonal. Positive <code>diagonal</code> means above the main diagonal, and negative <code>diagonal</code> means below the main diagonal.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=0).numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 0  6  7  8]\n [ 0  0 11 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=1).numpy())\n</code></pre> <pre><code>[[ 0  2  3  4]\n [ 0  0  7  8]\n [ 0  0  0 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=-1).numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 0 10 11 12]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def triu(self, diagonal:int=0) -&gt; Tensor:\n  \"\"\"\n  Returns the upper triangular part of the tensor, the other elements are set to 0.\n\n  The argument `diagonal` determines which diagonal is on the boundary. `diagonal = 0` means the main diagonal.\n  Positive `diagonal` means above the main diagonal, and negative `diagonal` means below the main diagonal.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=-1).numpy())\n  ```\n  \"\"\"\n  return Tensor._tri(self.shape[-2], self.shape[-1], diagonal=diagonal, device=self.device, dtype=dtypes.bool).where(self, 0).cast(self.dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.tril","title":"tril","text":"<pre><code>tril(diagonal: int = 0) -&gt; Tensor\n</code></pre> <p>Returns the lower triangular part of the tensor, the other elements are set to 0.</p> <p>The argument <code>diagonal</code> determines which diagonal is on the boundary. <code>diagonal = 0</code> means the main diagonal. Positive <code>diagonal</code> means above the main diagonal, and negative <code>diagonal</code> means below the main diagonal.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.tril(diagonal=0).numpy())\n</code></pre> <pre><code>[[ 1  0  0  0]\n [ 5  6  0  0]\n [ 9 10 11  0]]\n</code></pre> <pre><code>print(t.tril(diagonal=1).numpy())\n</code></pre> <pre><code>[[ 1  2  0  0]\n [ 5  6  7  0]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.tril(diagonal=-1).numpy())\n</code></pre> <pre><code>[[ 0  0  0  0]\n [ 5  0  0  0]\n [ 9 10  0  0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tril(self, diagonal:int=0) -&gt; Tensor:\n  \"\"\"\n  Returns the lower triangular part of the tensor, the other elements are set to 0.\n\n  The argument `diagonal` determines which diagonal is on the boundary. `diagonal = 0` means the main diagonal.\n  Positive `diagonal` means above the main diagonal, and negative `diagonal` means below the main diagonal.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=-1).numpy())\n  ```\n  \"\"\"\n  return Tensor._tri(self.shape[-2], self.shape[-1], diagonal=diagonal+1, device=self.device, dtype=dtypes.bool).where(0, self).cast(self.dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.interpolate","title":"interpolate","text":"<pre><code>interpolate(\n    size: Tuple[int, ...],\n    mode: str = \"linear\",\n    align_corners: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Downsamples or Upsamples to the input <code>size</code>, accepts 0 to N batch dimensions.</p> <p>The interpolation algorithm is selected with <code>mode</code> which currently only supports <code>linear</code>, <code>nearest</code> and <code>nearest-exact</code>. To run <code>bilinear</code> or <code>trilinear</code>, pass in a 2D or 3D size.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [21, 22, 23, 24], [41, 42, 43, 44]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [21 22 23 24]\n [41 42 43 44]]\n</code></pre> <pre><code>print(t.interpolate(size=(2,3), mode=\"linear\").numpy())\n</code></pre> <pre><code>[[ 6  7  8]\n [36 37 38]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def interpolate(self, size:Tuple[int, ...], mode:str=\"linear\", align_corners:bool=False) -&gt; Tensor:\n  \"\"\"\n  Downsamples or Upsamples to the input `size`, accepts 0 to N batch dimensions.\n\n  The interpolation algorithm is selected with `mode` which currently only supports `linear`, `nearest` and `nearest-exact`.\n  To run `bilinear` or `trilinear`, pass in a 2D or 3D size.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [21, 22, 23, 24], [41, 42, 43, 44]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.interpolate(size=(2,3), mode=\"linear\").numpy())\n  ```\n  \"\"\"\n  assert isinstance(size, (tuple,list)) and all_int(size) and 0 &lt; len(size) &lt;= self.ndim, f\"invalid {size=}\"\n  assert mode in (\"linear\", \"nearest\", \"nearest-exact\"), \"only supports linear, nearest or nearest-exact interpolate\"\n  assert not (align_corners and mode != \"linear\"), \"align_corners option can only be set with the interpolating mode linear\"\n  x, expand = self, list(self.shape)\n  for i in range(-1,-len(size)-1,-1):\n    scale = (self.shape[i] - int(align_corners)) / (size[i] - int(align_corners))\n    arr, reshape = Tensor.arange(size[i], dtype=dtypes.float32, device=self.device), [1] * self.ndim\n    reshape[i] = expand[i] = size[i]\n    if mode == \"linear\":\n      index = (scale*arr if align_corners else (scale*(arr+0.5))-0.5).clip(0, self.shape[i]-1)\n      low, high, perc = [y.reshape(reshape).expand(expand) for y in (index.floor(), index.ceil(), index - index.floor())]\n      x = x.gather(i, low).lerp(x.gather(i, high), perc)\n    else:\n      index = (scale*(arr+0.5) if mode==\"nearest-exact\" else scale*arr).cast(dtypes.int32).reshape(reshape).expand(expand)\n      x = x.gather(i, index)\n  return x.cast(self.dtype)\n</code></pre>"},{"location":"tensor/ops/#neural-network-functional","title":"Neural Network (functional)","text":""},{"location":"tensor/ops/#tinygrad.Tensor.linear","title":"linear","text":"<pre><code>linear(weight: Tensor, bias: Optional[Tensor] = None)\n</code></pre> <p>Applies a linear transformation to <code>self</code> using <code>weight</code> and <code>bias</code>.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</p> <pre><code>t = Tensor([[1, 2], [3, 4]])\nweight = Tensor([[1, 2], [3, 4]])\nbias = Tensor([1, 2])\nprint(t.linear(weight, bias).numpy())\n</code></pre> <pre><code>[[ 8 12]\n [16 24]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def linear(self, weight:Tensor, bias:Optional[Tensor]=None):\n  \"\"\"\n  Applies a linear transformation to `self` using `weight` and `bias`.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2], [3, 4]])\n  weight = Tensor([[1, 2], [3, 4]])\n  bias = Tensor([1, 2])\n  print(t.linear(weight, bias).numpy())\n  ```\n  \"\"\"\n  x = self.mul(weight) if len(weight.shape) == 1 else self.dot(weight)\n  return x.add(bias) if bias is not None else x\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.sequential","title":"sequential","text":"<pre><code>sequential(ll: List[Callable[[Tensor], Tensor]])\n</code></pre> <p>Applies a sequence of functions to <code>self</code> chaining the output of each function to the input of the next.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.sequential([lambda x: x * 2, lambda x: x + 1]).numpy())\n</code></pre> <pre><code>[3 5 7]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sequential(self, ll:List[Callable[[Tensor], Tensor]]):\n  \"\"\"\n  Applies a sequence of functions to `self` chaining the output of each function to the input of the next.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.sequential([lambda x: x * 2, lambda x: x + 1]).numpy())\n  ```\n  \"\"\"\n  return functools.reduce(lambda x,f: f(x), ll, self)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.layernorm","title":"layernorm","text":"<pre><code>layernorm(\n    axis: Union[int, Tuple[int, ...]] = -1,\n    eps: float = 1e-05,\n) -&gt; Tensor\n</code></pre> <p>Applies Layer Normalization over a mini-batch of inputs.</p> <ul> <li>Described: https://paperswithcode.com/method/layer-normalization</li> <li>Paper: https://arxiv.org/abs/1607.06450v1</li> </ul> <p><pre><code>t = Tensor.randn(8, 10, 16) * 2 + 8\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>7.923057556152344 2.0072731971740723\n</code></pre> <pre><code>t = t.layernorm()\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-2.184478153921532e-09 1.0003893375396729\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def layernorm(self, axis:Union[int,Tuple[int,...]]=-1, eps:float=1e-5) -&gt; Tensor:\n  \"\"\"\n  Applies Layer Normalization over a mini-batch of inputs.\n\n  - Described: https://paperswithcode.com/method/layer-normalization\n  - Paper: https://arxiv.org/abs/1607.06450v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.randn(8, 10, 16) * 2 + 8\n  print(t.mean().item(), t.std().item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.layernorm()\n  print(t.mean().item(), t.std().item())\n  ```\n  \"\"\"\n  y = (self - self.mean(axis, keepdim=True))\n  return y.mul((y*y).mean(axis, keepdim=True).add(eps).rsqrt())\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.batchnorm","title":"batchnorm","text":"<pre><code>batchnorm(\n    weight: Optional[Tensor],\n    bias: Optional[Tensor],\n    mean: Tensor,\n    invstd: Tensor,\n    axis: Union[int, Tuple[int, ...]] = 1,\n) -&gt; Tensor\n</code></pre> <p>Applies Batch Normalization over a mini-batch of inputs.</p> <ul> <li>Described: https://paperswithcode.com/method/batch-normalization</li> <li>Paper: https://arxiv.org/abs/1502.03167</li> </ul> <p><pre><code>t = Tensor.randn(8, 4, 16, 16) * 2 + 8\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>8.030435562133789 1.9699469804763794\n</code></pre> <pre><code>t = t.batchnorm(None, None, t.mean(axis=(0,2,3)), t.var(axis=(0,2,3)).add(1e-5).rsqrt())\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.7121278688136954e-06 0.9998164176940918\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def batchnorm(self, weight:Optional[Tensor], bias:Optional[Tensor], mean:Tensor, invstd:Tensor, axis:Union[int,Tuple[int,...]]=1) -&gt; Tensor:\n  \"\"\"\n  Applies Batch Normalization over a mini-batch of inputs.\n\n  - Described: https://paperswithcode.com/method/batch-normalization\n  - Paper: https://arxiv.org/abs/1502.03167\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.randn(8, 4, 16, 16) * 2 + 8\n  print(t.mean().item(), t.std().item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.batchnorm(None, None, t.mean(axis=(0,2,3)), t.var(axis=(0,2,3)).add(1e-5).rsqrt())\n  print(t.mean().item(), t.std().item())\n  ```\n  \"\"\"\n  axis_ = argfix(axis)\n  shape = tuple(s if ax in axis_ else 1 for ax, s in enumerate(self.shape))\n  x = self - mean.reshape(shape)\n  if weight is not None: x = x * weight.reshape(shape)\n  ret = x.mul(invstd.reshape(shape) if len(invstd.shape) == len(axis_) else invstd)\n  return (ret + bias.reshape(shape)) if bias is not None else ret\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.dropout","title":"dropout","text":"<pre><code>dropout(p=0.5) -&gt; Tensor\n</code></pre> <p>Applies dropout to <code>self</code>.</p> <p>Note</p> <p>dropout is only applied when <code>Tensor.training</code> is <code>True</code>.</p> <ul> <li>Described: https://paperswithcode.com/method/dropout</li> <li>Paper: https://jmlr.org/papers/v15/srivastava14a.html</li> </ul> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 2)\nwith Tensor.train():\n  print(t.dropout().numpy())\n</code></pre> <pre><code>[[ 0.      2.17  ]\n [ 0.     -0.1682]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dropout(self, p=0.5) -&gt; Tensor:\n  \"\"\"\n  Applies dropout to `self`.\n\n  NOTE: dropout is only applied when `Tensor.training` is `True`.\n\n  - Described: https://paperswithcode.com/method/dropout\n  - Paper: https://jmlr.org/papers/v15/srivastava14a.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 2)\n  with Tensor.train():\n    print(t.dropout().numpy())\n  ```\n  \"\"\"\n  if not Tensor.training or p == 0: return self\n  return (Tensor.rand_like(self, requires_grad=False, dtype=dtypes.default_float, contiguous=False) &gt;= p).contiguous().where(self, 0) / (1.0 - p)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.one_hot","title":"one_hot","text":"<pre><code>one_hot(num_classes: int = -1) -&gt; Tensor\n</code></pre> <p>Converts <code>self</code> to a one-hot tensor.</p> <p><code>num_classes</code> defaults to -1, which means num_classes will be inferred as max(self) + 1.</p> <pre><code>t = Tensor([0, 1, 3, 3, 4])\nprint(t.one_hot(5).numpy())\n</code></pre> <pre><code>[[1 0 0 0 0]\n [0 1 0 0 0]\n [0 0 0 1 0]\n [0 0 0 1 0]\n [0 0 0 0 1]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def one_hot(self, num_classes:int=-1) -&gt; Tensor:\n  \"\"\"\n  Converts `self` to a one-hot tensor.\n\n  `num_classes` defaults to -1, which means num_classes will be inferred as max(self) + 1.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([0, 1, 3, 3, 4])\n  print(t.one_hot(5).numpy())\n  ```\n  \"\"\"\n  if num_classes == -1: num_classes = (self.max()+1).item()\n  return (self[..., None] == Tensor.arange(num_classes, requires_grad=False, device=self.device)).where(1, 0)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.scaled_dot_product_attention","title":"scaled_dot_product_attention","text":"<pre><code>scaled_dot_product_attention(\n    key: Tensor,\n    value: Tensor,\n    attn_mask: Optional[Tensor] = None,\n    dropout_p: float = 0.0,\n    is_causal: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Computes scaled dot-product attention. <code>self</code> is the query tensor, <code>key</code> is the key tensor, and <code>value</code> is the value tensor.</p> <ul> <li>Described: https://paperswithcode.com/method/scaled</li> <li>Paper: https://arxiv.org/abs/1706.03762v7</li> </ul> <pre><code>q = Tensor.randn(2, 4, 8)\nk = Tensor.randn(2, 4, 8)\nv = Tensor.randn(2, 4, 8)\nprint(q.scaled_dot_product_attention(k, v).numpy())\n</code></pre> <pre><code>[[[-0.1425 -0.1433 -0.3625  0.8853 -0.3129  1.0271 -0.0019  0.2445]\n  [-0.7137  0.2617  1.1393  0.692   0.0461  0.1132  0.391  -0.3563]\n  [ 0.4718  0.6791  0.8956  0.9387 -0.7198  0.753   0.5702  0.2661]\n  [-1.0183  0.005   0.9208  0.6447  0.2658  0.0411  0.2314 -0.4636]]\n\n [[ 0.2928 -0.3364 -0.1937 -0.0755 -0.6196 -0.7339  0.8431 -0.3794]\n  [ 0.5915  0.3565 -0.6987  0.241   0.2624 -0.1074 -0.3026 -0.3574]\n  [ 0.3176 -0.4436 -0.3136 -0.5334 -0.5756 -0.851   0.9595 -0.4201]\n  [ 0.4378  0.0234 -0.0984  0.4847 -0.3579 -0.3998  0.3781 -0.2338]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def scaled_dot_product_attention(self, key:Tensor, value:Tensor, attn_mask:Optional[Tensor]=None,\n                                 dropout_p:float=0.0, is_causal:bool=False) -&gt; Tensor:\n  \"\"\"\n  Computes scaled dot-product attention.\n  `self` is the query tensor, `key` is the key tensor, and `value` is the value tensor.\n\n  - Described: https://paperswithcode.com/method/scaled\n  - Paper: https://arxiv.org/abs/1706.03762v7\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  q = Tensor.randn(2, 4, 8)\n  k = Tensor.randn(2, 4, 8)\n  v = Tensor.randn(2, 4, 8)\n  print(q.scaled_dot_product_attention(k, v).numpy())\n  ```\n  \"\"\"\n  # NOTE: it also works when `key` and `value` have symbolic shape.\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  if is_causal: attn_mask = Tensor.ones(self.shape[-2], key.shape[-2], requires_grad=False, device=self.device).tril(0).cast(dtypes.bool)\n  if attn_mask is not None and attn_mask.dtype == dtypes.bool: attn_mask = (attn_mask == 0).where(-float(\"inf\"), 0)\n  qk = self.matmul(key.transpose(-2,-1), acc_dtype=least_upper_dtype(self.dtype, key.dtype, dtypes.float32)) / math.sqrt(self.shape[-1])\n  return ((qk+attn_mask) if attn_mask is not None else qk).softmax(-1).cast(self.dtype).dropout(dropout_p) @ value\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.binary_crossentropy","title":"binary_crossentropy","text":"<pre><code>binary_crossentropy(\n    Y: Tensor, reduction: ReductionStr = \"mean\"\n) -&gt; Tensor\n</code></pre> <p>Computes the binary cross-entropy loss between <code>self</code> and <code>Y</code>.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html</p> <pre><code>t = Tensor([0.1, 0.9, 0.2])\nY = Tensor([0, 1, 0])\nprint(t.binary_crossentropy(Y).item())\n</code></pre> <pre><code>0.14462155103683472\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy(self, Y:Tensor, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the binary cross-entropy loss between `self` and `Y`.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([0.1, 0.9, 0.2])\n  Y = Tensor([0, 1, 0])\n  print(t.binary_crossentropy(Y).item())\n  ```\n  \"\"\"\n  return (-Y*self.log() - (1-Y)*(1-self).log())._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.binary_crossentropy_logits","title":"binary_crossentropy_logits","text":"<pre><code>binary_crossentropy_logits(\n    Y: Tensor, reduction: ReductionStr = \"mean\"\n) -&gt; Tensor\n</code></pre> <p>Computes the binary cross-entropy loss between <code>self</code> and <code>Y</code> where <code>self</code> is logits.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html</p> <pre><code>t = Tensor([-1, 2, -3])\nY = Tensor([0, 1, 0])\nprint(t.binary_crossentropy_logits(Y).item())\n</code></pre> <pre><code>0.16292567551136017\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy_logits(self, Y:Tensor, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the binary cross-entropy loss between `self` and `Y` where `self` is logits.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, -3])\n  Y = Tensor([0, 1, 0])\n  print(t.binary_crossentropy_logits(Y).item())\n  ```\n  \"\"\"\n  return (self.maximum(0) - Y * self + (1 + self.abs().neg().exp()).log())._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.sparse_categorical_crossentropy","title":"sparse_categorical_crossentropy","text":"<pre><code>sparse_categorical_crossentropy(\n    Y: Tensor,\n    ignore_index: int = -1,\n    label_smoothing=0.0,\n    reduction: ReductionStr = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Computes the sparse categorical cross-entropy loss between <code>self</code> and <code>Y</code>.</p> <p>Note</p> <p><code>self</code> is logits and <code>Y</code> is the target labels. NOTE: unlike PyTorch, this function expects the class axis to be -1</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</p> <pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.sparse_categorical_crossentropy(Y).item())\n</code></pre> <pre><code>0.09391524642705917\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sparse_categorical_crossentropy(self, Y:Tensor, ignore_index:int=-1, label_smoothing=0.0, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the sparse categorical cross-entropy loss between `self` and `Y`.\n\n  NOTE: `self` is logits and `Y` is the target labels.\n  NOTE: unlike PyTorch, this function expects the class axis to be -1\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.sparse_categorical_crossentropy(Y).item())\n  ```\n  \"\"\"\n  assert 0.0 &lt;= label_smoothing &lt;= 1.0, \"label_smoothing must be in [0.0, 1.0]\"\n  assert reduction in (\"mean\", \"sum\", \"none\"), \"reduction must be one of ['mean', 'sum', 'none']\"\n  log_probs, loss_mask = self.log_softmax(), (Y != ignore_index) if ignore_index != -1 else Y.ones_like(dtype=dtypes.bool)\n  y_counter = Tensor.arange(self.shape[-1], requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n  y = ((y_counter == Y.flatten().reshape(-1, 1)) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n  smoothing = label_smoothing * (log_probs.mean(-1) * loss_mask)\n  unreduced = ((1 - label_smoothing) * (log_probs * y).sum(-1) + smoothing)\n  # NOTE: because of ignore_index, we can't use Tensor.mean (so can't use `_do_reduction` here)\n  return -(unreduced.sum() / loss_mask.sum() if reduction == \"mean\" else (unreduced.sum() if reduction == \"sum\" else unreduced))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(\n    Y: Tensor,\n    reduction: ReductionStr = \"mean\",\n    label_smoothing: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the cross entropy loss between input logits and target.</p> <p>Note</p> <p><code>self</code> are logits and <code>Y</code> are the target labels or class probabilities.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html</p> <p><pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.cross_entropy(Y).item())\n</code></pre> <pre><code>0.09391524642705917\n</code></pre> <pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.cross_entropy(Y, reduction='none').numpy())\n</code></pre> <pre><code>[0.055  0.1328]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cross_entropy(self, Y:Tensor, reduction:ReductionStr=\"mean\", label_smoothing:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Compute the cross entropy loss between input logits and target.\n\n  NOTE: `self` are logits and `Y` are the target labels or class probabilities.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.cross_entropy(Y).item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.cross_entropy(Y, reduction='none').numpy())\n  ```\n  \"\"\"\n  assert 0.0 &lt;= label_smoothing &lt;= 1.0, \"label_smoothing must be in [0.0, 1.0]\"\n  Y = Y.one_hot(num_classes=cast(int, self.shape[1])) if Y.ndim &lt; 2 else Y\n  Y = (1 - label_smoothing)*Y + label_smoothing / cast(int, Y.shape[1])\n  ret = -self.log_softmax(axis=1).mul(Y).sum(axis=1)\n  return ret._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/properties/","title":"Properties","text":""},{"location":"tensor/properties/#basic","title":"Basic","text":""},{"location":"tensor/properties/#tinygrad.Tensor.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: Tuple[sint, ...]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: DType\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.device","title":"device  <code>property</code>","text":"<pre><code>device: Union[str, Tuple[str, ...]]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Returns the number of dimensions in the tensor.</p> <pre><code>t = Tensor([[1, 2], [3, 4]])\nprint(t.ndim)\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.numel","title":"numel","text":"<pre><code>numel() -&gt; sint\n</code></pre> <p>Returns the total number of elements in the tensor.</p> <pre><code>t = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint(t.numel())\n</code></pre> <pre><code>8\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def numel(self) -&gt; sint:\n  \"\"\"\n  Returns the total number of elements in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n  print(t.numel())\n  ```\n  \"\"\"\n  return prod(self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.element_size","title":"element_size","text":"<pre><code>element_size() -&gt; int\n</code></pre> <p>Returns the size in bytes of an individual element in the tensor.</p> <pre><code>t = Tensor([5], dtype=dtypes.int16)\nprint(t.element_size())\n</code></pre> <pre><code>2\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def element_size(self) -&gt; int:\n  \"\"\"\n  Returns the size in bytes of an individual element in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([5], dtype=dtypes.int16)\n  print(t.element_size())\n  ```\n  \"\"\"\n  return self.dtype.itemsize\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.nbytes","title":"nbytes","text":"<pre><code>nbytes() -&gt; int\n</code></pre> <p>Returns the total number of bytes of all elements in the tensor.</p> <pre><code>t = Tensor([8, 9], dtype=dtypes.float)\nprint(t.nbytes())\n</code></pre> <pre><code>8\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def nbytes(self) -&gt; int:\n  \"\"\"\n  Returns the total number of bytes of all elements in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([8, 9], dtype=dtypes.float)\n  print(t.nbytes())\n  ```\n  \"\"\"\n  return self.numel() * self.element_size()\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.is_floating_point","title":"is_floating_point","text":"<pre><code>is_floating_point() -&gt; bool\n</code></pre> <p>Returns <code>True</code> if the tensor contains floating point types, i.e. is one of <code>dtype.float64</code>, <code>dtype.float32</code>, <code>dtype.float16</code>, <code>dtype.bfloat16</code>.</p> <pre><code>t = Tensor([8, 9], dtype=dtypes.float32)\nprint(t.is_floating_point())\n</code></pre> <pre><code>True\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def is_floating_point(self) -&gt; bool:\n  \"\"\"\n  Returns `True` if the tensor contains floating point types, i.e. is one of `dtype.float64`, `dtype.float32`,\n  `dtype.float16`, `dtype.bfloat16`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([8, 9], dtype=dtypes.float32)\n  print(t.is_floating_point())\n  ```\n  \"\"\"\n  return dtypes.is_float(self.dtype)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.size","title":"size","text":"<pre><code>size(\n    dim: Optional[int] = None,\n) -&gt; Union[sint, Tuple[sint, ...]]\n</code></pre> <p>Return the size of the tensor. If <code>dim</code> is specified, return the length along dimension <code>dim</code>. Otherwise return the shape of the tensor.</p> <p><pre><code>t = Tensor([[4, 5, 6], [7, 8, 9]])\nprint(t.size())\n</code></pre> <pre><code>(2, 3)\n</code></pre> <pre><code>print(t.size(dim=1))\n</code></pre> <pre><code>3\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def size(self, dim:Optional[int]=None) -&gt; Union[sint, Tuple[sint, ...]]:\n  \"\"\"\n  Return the size of the tensor. If `dim` is specified, return the length along dimension `dim`. Otherwise return the shape of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[4, 5, 6], [7, 8, 9]])\n  print(t.size())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.size(dim=1))\n  ```\n  \"\"\"\n  return self.shape if dim is None else self.shape[dim]\n</code></pre>"},{"location":"tensor/properties/#data-access","title":"Data Access","text":""},{"location":"tensor/properties/#tinygrad.Tensor.data","title":"data","text":"<pre><code>data() -&gt; memoryview\n</code></pre> <p>Returns the data of this tensor as a memoryview.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(np.frombuffer(t.data(), dtype=np.int32))\n</code></pre> <pre><code>[1 2 3 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def data(self) -&gt; memoryview:\n  \"\"\"\n  Returns the data of this tensor as a memoryview.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(np.frombuffer(t.data(), dtype=np.int32))\n  ```\n  \"\"\"\n  assert self.dtype.fmt is not None, f\"no fmt dtype for {self.dtype}\"\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  return self._data().cast(self.dtype.fmt, self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.item","title":"item","text":"<pre><code>item() -&gt; ConstType\n</code></pre> <p>Returns the value of this tensor as a standard Python number.</p> <pre><code>t = Tensor(42)\nprint(t.item())\n</code></pre> <pre><code>42\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def item(self) -&gt; ConstType:\n  \"\"\"\n  Returns the value of this tensor as a standard Python number.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor(42)\n  print(t.item())\n  ```\n  \"\"\"\n  assert self.dtype.fmt is not None, f\"no fmt dtype for {self.dtype}\"\n  assert self.numel() == 1, \"must have one element for item\"\n  return self._data().cast(self.dtype.fmt)[0]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.tolist","title":"tolist","text":"<pre><code>tolist() -&gt; Union[Sequence[ConstType], ConstType]\n</code></pre> <p>Returns the value of this tensor as a nested list.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.tolist())\n</code></pre> <pre><code>[1, 2, 3, 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tolist(self) -&gt; Union[Sequence[ConstType], ConstType]:\n  \"\"\"\n  Returns the value of this tensor as a nested list.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.tolist())\n  ```\n  \"\"\"\n  return self.data().tolist()\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.numpy","title":"numpy","text":"<pre><code>numpy() -&gt; 'np.ndarray'\n</code></pre> <p>Returns the value of this tensor as a <code>numpy.ndarray</code>.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(repr(t.numpy()))\n</code></pre> <pre><code>array([1, 2, 3, 4], dtype=int32)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def numpy(self) -&gt; 'np.ndarray':  # type: ignore [name-defined] # noqa: F821\n  \"\"\"\n  Returns the value of this tensor as a `numpy.ndarray`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(repr(t.numpy()))\n  ```\n  \"\"\"\n  import numpy as np\n  if self.dtype == dtypes.bfloat16: return self.float().numpy()\n  assert _to_np_dtype(self.dtype) is not None, f\"no np dtype for {self.dtype}\"\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  return np.frombuffer(self._data(), dtype=_to_np_dtype(self.dtype)).reshape(self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad-ops","title":"tinygrad ops","text":""},{"location":"tensor/properties/#tinygrad.Tensor.schedule_with_vars","title":"schedule_with_vars","text":"<pre><code>schedule_with_vars(\n    *lst: Tensor,\n) -&gt; Tuple[List[ScheduleItem], Dict[Variable, int]]\n</code></pre> <p>Creates the schedule needed to realize these Tensor(s), with Variables.</p> <p>Note</p> <p>A Tensor can only be scheduled once.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule_with_vars(self, *lst:Tensor) -&gt; Tuple[List[ScheduleItem], Dict[Variable, int]]:\n  \"\"\"\n  Creates the schedule needed to realize these Tensor(s), with Variables.\n\n  NOTE: A Tensor can only be scheduled once.\n  \"\"\"\n  schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]))\n  return memory_planner(schedule), var_vals\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.schedule","title":"schedule","text":"<pre><code>schedule(*lst: Tensor) -&gt; List[ScheduleItem]\n</code></pre> <p>Creates the schedule needed to realize these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule(self, *lst:Tensor) -&gt; List[ScheduleItem]:\n  \"\"\"Creates the schedule needed to realize these Tensor(s).\"\"\"\n  schedule, var_vals = self.schedule_with_vars(*lst)\n  assert len(var_vals) == 0\n  return schedule\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.realize","title":"realize","text":"<pre><code>realize(*lst: Tensor, do_update_stats=True) -&gt; Tensor\n</code></pre> <p>Triggers the computation needed to create these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def realize(self, *lst:Tensor, do_update_stats=True) -&gt; Tensor:\n  \"\"\"Triggers the computation needed to create these Tensor(s).\"\"\"\n  run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.replace","title":"replace","text":"<pre><code>replace(x: Tensor) -&gt; Tensor\n</code></pre> <p>Replaces the data of this tensor with the data of another tensor. Only the shape of the tensors must match.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def replace(self, x:Tensor) -&gt; Tensor:\n  \"\"\"\n  Replaces the data of this tensor with the data of another tensor. Only the shape of the tensors must match.\n  \"\"\"\n  # used for replacing a Tensor with a new version of it (potentially with a different device and dtype)\n  assert not x.requires_grad and getattr(self, '_ctx', None) is None\n  assert self.shape == x.shape, f\"replace shape mismatch {self.shape} != {x.shape}\"\n  self.lazydata = x.lazydata\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.assign","title":"assign","text":"<pre><code>assign(x) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def assign(self, x) -&gt; Tensor:\n  # TODO: this is a hack for writing to DISK. remove with working assign\n  if isinstance(self.device, str) and self.device.startswith(\"DISK\"):\n    if x.__class__ is not Tensor: x = Tensor(x, device=\"NPY\", dtype=self.dtype)\n    self.contiguous().realize().lazydata.base.realized.copyin(x.numpy().data)\n    return self\n  if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n  if DEBUG &gt;= 4: print(f\"assign {self.lazydata} &lt;- {x.lazydata}\")\n  if self.lazydata is x.lazydata: return self  # a self assign is a NOOP\n  # NOTE: we allow cross device assign\n  assert self.shape == x.shape, f\"assign shape mismatch {self.shape} != {x.shape}\"\n  assert self.device == x.device, f\"assign device mismatch {self.device} != {x.device}\"\n  assert self.dtype == x.dtype, f\"assign dtype mismatch {self.dtype} != {x.dtype}\"\n  assert not isinstance(self.lazydata, MultiLazyBuffer) or self.lazydata.axis == x.lazydata.axis, \"axis must match on MultiLazyBuffer\"\n  assert not x.requires_grad  # self requires_grad is okay?\n  if not self.lazydata.is_realized(): return self.replace(x)\n  self.lazydata = self.lazydata.assign(x.lazydata)\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.detach","title":"detach","text":"<pre><code>detach() -&gt; Tensor\n</code></pre> <p>Returns a new tensor with the same data as this tensor, but detached from the autograd graph.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def detach(self) -&gt; Tensor:\n  \"\"\"\n  Returns a new tensor with the same data as this tensor, but detached from the autograd graph.\n  \"\"\"\n  return Tensor(self.lazydata, device=self.device, requires_grad=False)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.to","title":"to","text":"<pre><code>to(device: Optional[Union[str, Tuple[str, ...]]]) -&gt; Tensor\n</code></pre> <p>Moves the tensor to the given device.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to(self, device:Optional[Union[str, Tuple[str, ...]]]) -&gt; Tensor:\n  \"\"\"\n  Moves the tensor to the given device.\n  \"\"\"\n  device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)\n  if device == self.device: return self\n  if not isinstance(device, str): return self.shard(device)\n  ret = Tensor(self.lazydata, device, requires_grad=self.requires_grad)\n  if self.grad is not None: ret.grad = self.grad.to(device)\n  if hasattr(self, '_ctx'): ret._ctx = self._ctx\n  return ret\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.to_","title":"to_","text":"<pre><code>to_(device: Optional[Union[str, Tuple[str, ...]]])\n</code></pre> <p>Moves the tensor to the given device in place.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to_(self, device:Optional[Union[str, Tuple[str, ...]]]):\n  \"\"\"\n  Moves the tensor to the given device in place.\n  \"\"\"\n  real = self.to(device)\n  # TODO: is this assign?\n  if self.grad is not None and real.grad is not None: self.grad.lazydata = real.grad.lazydata\n  self.lazydata = real.lazydata\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.shard","title":"shard","text":"<pre><code>shard(\n    devices: Tuple[str, ...],\n    axis: Optional[int] = None,\n    splits: Optional[Tuple[int, ...]] = None,\n) -&gt; Tensor\n</code></pre> <p>Shards the tensor across the given devices. Optionally specify which axis to shard on, and how to split it across devices.</p> <pre><code>t = Tensor.empty(2, 3)\nprint(t.shard((t.device, t.device), axis=1, splits=(2, 1)).lazydata)\n</code></pre> <pre><code>&lt;MLB self.axis=1 self.real=[True, True] \nCLANG ShapeTracker(views=(View(shape=(2, 2), strides=(2, 1), offset=0, mask=None, contiguous=True),))\nCLANG ShapeTracker(views=(View(shape=(2, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))&gt;\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard(self, devices:Tuple[str, ...], axis:Optional[int]=None, splits:Optional[Tuple[int, ...]]=None) -&gt; Tensor:\n  \"\"\"\n  Shards the tensor across the given devices. Optionally specify which axis to shard on, and how to split it across devices.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 3)\n  print(t.shard((t.device, t.device), axis=1, splits=(2, 1)).lazydata)\n  ```\n\n  \"\"\"\n  assert isinstance(self.lazydata, LazyBuffer), \"can't shard a MultiLazyBuffer\"\n  devices, bounds = tuple(Device.canonicalize(x) for x in devices), None\n  if axis is not None:\n    if axis &lt; 0: axis += len(self.shape)\n    if splits is None:\n      if not isinstance(total:=self.shape[axis], int): raise RuntimeError(f\"cannot shard symbolic shape {self.shape=}, {axis=}\")\n      sz = round_up(total, len(devices)) // len(devices)\n      splits = tuple([max(0, min(sz, total - sz*i)) for i in range(len(devices))])\n    assert sum(splits) == self.shape[axis], \"specified splits do not sum up to axis shape\"\n    boundaries = tuple(itertools.accumulate(splits))\n    bounds = tuple(zip((0,) + boundaries, boundaries))\n  return Tensor(MultiLazyBuffer.from_sharded(self.lazydata, devices, axis, bounds), device=devices, requires_grad=self.requires_grad)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.shard_","title":"shard_","text":"<pre><code>shard_(\n    devices: Tuple[str, ...],\n    axis: Optional[int] = None,\n    splits: Optional[Tuple[int, ...]] = None,\n)\n</code></pre> <p>Shards the tensor across the given devices in place.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard_(self, devices:Tuple[str, ...], axis:Optional[int]=None, splits:Optional[Tuple[int, ...]]=None):\n  \"\"\"\n  Shards the tensor across the given devices in place.\n  \"\"\"\n  self.lazydata = self.shard(devices, axis, splits).lazydata\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.contiguous","title":"contiguous","text":"<pre><code>contiguous()\n</code></pre> <p>Returns a contiguous tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous(self):\n  \"\"\"\n  Returns a contiguous tensor.\n  \"\"\"\n  return F.Contiguous.apply(self)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.contiguous_backward","title":"contiguous_backward","text":"<pre><code>contiguous_backward()\n</code></pre> <p>Inserts a contiguous operation in the backward pass.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous_backward(self):\n  \"\"\"\n  Inserts a contiguous operation in the backward pass.\n  \"\"\"\n  return F.ContiguousBackward.apply(self)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.backward","title":"backward","text":"<pre><code>backward(\n    gradient: Optional[Tensor] = None,\n    retain_graph: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Propagates the gradient of a tensor backwards through the computation graph. If the 'gradient' argument is not provided, the tensor must be a scalar, and the gradient is implicitly set to 1.0. If 'retain_graph' is false, the graph used to compute the grads will be freed. Otherwise, it will be kept. Keeping it can increase memory usage. <pre><code>t = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\nt.sum().backward()\nprint(t.grad.numpy())\n</code></pre> <pre><code>[1. 1. 1. 1.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def backward(self, gradient:Optional[Tensor]=None, retain_graph:bool=False) -&gt; Tensor:\n  \"\"\"\n  Propagates the gradient of a tensor backwards through the computation graph.\n  If the 'gradient' argument is not provided, the tensor must be a scalar, and the gradient is implicitly set to 1.0.\n  If 'retain_graph' is false, the graph used to compute the grads will be freed. Otherwise, it will be kept. Keeping it can increase memory usage.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n  t.sum().backward()\n  print(t.grad.numpy())\n  ```\n  \"\"\"\n  toposorted = self._deepwalk()\n  if gradient is None:\n    assert self.shape == tuple(), \"when no gradient is provided, backward must be called on a scalar tensor\"\n    # fill in the first grad with one. don't use Tensor.ones because we don't need contiguous\n    # this is \"implicit gradient creation\"\n    gradient = Tensor(1.0, dtype=self.dtype, device=self.device, requires_grad=False)\n\n  assert self.shape == gradient.shape, f\"grad shape must match tensor shape, {gradient.shape!r} != {self.shape!r}\"\n  self.grad = gradient\n  for t0 in reversed(toposorted):\n    if t0.grad is None: raise RuntimeError(f\"tensor {t0} has no grad\")\n    token = _METADATA.set(dataclasses.replace(md, backward=True) if (md := t0._ctx.metadata) is not None else None)\n    grads = t0._ctx.backward(t0.grad.lazydata)\n    _METADATA.reset(token)\n    grads = [Tensor(g, device=self.device, requires_grad=False) if g is not None else None\n      for g in ([grads] if len(t0._ctx.parents) == 1 else grads)]\n    for t, g in zip(t0._ctx.parents, grads):\n      if g is not None and t.requires_grad:\n        assert g.shape == t.shape, f\"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}\"\n        t.grad = g if t.grad is None else (t.grad + g)\n    if not retain_graph: del t0._ctx\n  return self\n</code></pre>"}]}